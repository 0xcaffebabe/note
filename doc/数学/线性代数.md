# 线性代数

## 向量

> 向量（vector）：也可以叫做矢量。它代表一组数字，并且这些数字是有序排列的

C++中的数组就叫做vector

向量可以用来表示某个物体的特征。其中，向量的每个元素就代表一维特征，而元素的值代表了相应特征的值，我们称这类向量为特征向量

### 向量空间

x1​，x2​，……，xn​∈F，就有 Fn

假设 V 是 Fn​ 的非零子集，如果对任意的向量 x、向量 y∈V，都有 (x+y)∈V，我们称为 V 对向量的加法封闭；对任意的标量 k∈V，向量 x∈V，都有 kx 属于 V，我们称 V 对标量与向量的乘法封闭

如果 V 满足向量的加法和乘法封闭性，我们就称 V 是 F 上的向量空间

#### 向量子空间

已知 V:=(V,+,⋅) 是一个向量空间，如果 U⊆V,U !=0 ,那么 U:=(U,+,⋅) 就是 V 的向量子空间

#### 距离

可以把一个向量想象为 n 维空间中的一个点。而向量空间中两个向量的距离，就是这两个向量所对应的点之间的距离

- 曼哈顿距离 

$$MD(x,y) = \sum_{i=1}^n |x_i - y_i|$$

- 欧氏距离

$$ED(x,y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^{2}}$$

- 切比雪夫距离

$$CD(x,y) = \argmax_1^n |x_i - y_i|$$

#### 长度

通常使用欧氏距离来表示向量的长度

#### 夹角

空间中两个向量所形成夹角的余弦值

$$Cosine(X,Y) = \frac{\sum_{i=1}^n (x_i * y_i)}{ \sqrt{\sum_{i=1}^n x^2 *  \sum_{i=1}^n y^2}}$$

#### 向量空间模型

向量空间模型假设所有的对象都可以转化为向量，然后使用向量间的距离，或者是向量间的夹角余弦来表示两个对象之间的相似程度

向量空间可以很形象地表示数据点之间的相似程度，不仅能运用在信息检索，也可以运用在基于相似度的一些机器学习算法中，例如 K 近邻（KNN）分类、K 均值（K-Means）聚类

信息检索：

1. 将文档转为特征向量，分词后将词转换为向量，最简单的方法是用“1”表示这个词条出现在文档中，“0”表示没有出现，也可以对不同的词赋予不同的权重来丰富特征
2. 将查询和文档进行匹配，查询需要先转为向量，因为查询与文档的维度不一样（出现的词），对于在文档出现而在查询没有出现的词，可以采取简单的置0，或者把文档的这个维度剔除掉
3. 进行上面两个步骤后，就能得到每篇文档与查询的相似度了

### 运算

- 向量相加

$$
x=\begin{bmatrix}x_1\\x_2\\x_3\\\varvdots\\x_n\end{bmatrix}\quad y=\begin{bmatrix}y_1\\y_2\\y_3\\\varvdots\\y_n\end{bmatrix}\quad x+y=\begin{bmatrix}x_1+y_1\\x_2+y_2\\x_3+y_3\\\varvdots\\x_n+y_n\end{bmatrix}
$$

- 向量相乘

$$
xy=\begin{bmatrix}x_1\\x_2\\x_3\\\vdots\\x_n\end{bmatrix}\begin{bmatrix}y_1\\y_2\\y_3\\\vdots\\y_n\end{bmatrix}=x_1y_1+x_2y_2+x_3y_3+...+x_ny_n
$$

![向量的加法实际上就是把几何问题转化成了代数问题](/assets/2022119203634.webp)

## 矩阵

> 矩阵由多个长度相等的向量组成，其中的每列或者每行就是一个向量

### 使用矩阵表示方程

$$
\left.\left\{\begin{array}{c}a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n=b_1\\a_{21}x_1+a_{22}x_2+\cdots+a_{2n}x_n=b_2\\\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\\a_{m1}x_1+a_{m2}x_2+\cdots+a_{mn}x_n=b_m\end{array}\right.\right.
$$

=

$$
\left.\tilde{A}=\left[\begin{array}{ccccc}a_{11}&a_{12}&\ldots&a_{1n}&b_1\\a_{21}&a_{22}&\ldots&a_{2n}&b_2\\\ldots&\ldots&\ldots&\ldots&\ldots\\a_{m1}&a_{m2}&\ldots&a_{mn}&b_m\end{array}\right.\right]
$$

### 矩阵运算

#### 矩阵相加

$$
A\in\mathbb{R}^{m\times n}, B\in\mathbb{R}^{m\times n}
$$

$$
\left.A+B=\left[\begin{array}{ccc}a_{11}+b_{11}&\ldots&a_{1n}+b_{1n}\\\cdot&&\cdot\\\cdot&&\cdot\\\cdot&&\cdot\\a_{m1}+b_{m1}&\ldots&a_{mn}+b_{mn}\end{array}\right.\right]\in R^{m\times n}
$$

#### 矩阵乘

- 普通矩阵乘

![普通矩阵乘](/assets/2022119203926.webp)

只有相邻阶数匹配的矩阵才能相乘，例如，一个 n×k 矩阵 A 和一个 k×m 矩阵 B 相乘，最后得出 n×m 矩阵 C，而这里的 k 就是相邻阶数

- 哈达玛积，就是矩阵各对应元素的乘积

$$
\left.C=A^*B=\left[\begin{array}{cc}1&2\\4&5\end{array}\right.\right]\left[\begin{array}{cc}1&4\\2&5\end{array}\right]=\left[\begin{array}{cc}1*1&2*4\\4*2&5*5\end{array}\right]=\left[\begin{array}{cc}1&8\\8&25\end{array}\right]
$$

- 克罗内克积，是两个任意大小矩阵间的运算

A×B，如果 A 是一个 m×n 的矩阵，而 B 是一个 p×q 的矩阵，克罗内克积则是一个 mp×nq 的矩阵

### 单位矩阵

主对角线上的元素均为 1，除此以外全都为 0

$$
\left.I_1=[1],I_2=\left[\begin{array}{ccc}1&0\\0&1\end{array}\right.\right],I_3=\left[\begin{array}{ccc}1&0&0\\0&1&0\\0&0&1\end{array}\right],\ldots,I_n=\left[\begin{array}{cccc}1&0&\ldots&0\\0&1&\ldots&0\\.&.&\ldots&.\\.&.&.&.\\0&0&\ldots&1\end{array}\right]
$$

### 矩阵的性质

1. 结合律：任意实数 m×n 矩阵 A，n×p 矩阵 B，p×q 矩阵 C 之间相乘，满足结合律 (AB)C=A(BC)
2. 分配律：任意实数 m×n 矩阵 A 和 B，n×p 矩阵 C 和 D 之间相乘满足分配律 (A+B)C=AC+BC，A(C+D)=AC+AD
3. 单位矩阵相乘：任意实数 m×n 矩阵 A 和单位矩阵之间的乘，等于它本身 A

### 逆矩阵

A 乘以它的逆矩阵 $A^{-1}$ 就等于单位矩阵.如果一个矩阵是可逆的，那这个矩阵我们叫做非奇异矩阵，如果一个矩阵是不可逆的，那这个矩阵我们就叫做奇异矩阵

### 矩阵转置

$$
\left.x=\left[\begin{array}{cccc}x_{1,1}&x_{1,2}&x_{1,3}&x_{1,4}\\x_{2,1}&x_{2,2}&x_{2,3}&x_{2,4}\\x_{3,1}&x_{3,2}&x_{3,3}&x_{3,4}\\\end{array}\right.\right]
$$

$$
\left.x'=\left[\begin{array}{ccc}x_{1,1}&x_{2,1}&x_{3,1}\\x_{1,2}&x_{2,2}&x_{3,2}\\x_{1,3}&x_{2,3}&x_{3,3}\\x_{1,4}&x_{2,4}&x_{3,4}\end{array}\right.\right]
$$

## 线性回归

- 当自变量 x 的个数大于 1 时就是多元回归
- 当因变量 y 个数大于 1 时就是多重回归

如果因变量和自变量为线性关系时，就是线性回归模型；如果因变量和自变量为非线性关系时，就是非线性回归分析模型

### 高斯消元

主要分为两步，消元（Forward Elimination）和回代（Back Substitution）

设如下方程矩阵表达：

$$
\left.\left[\begin{array}{cccccc}1&-2&1&-1&1&|&0\\4&-8&3&-3&1&|&2\\-2&4&-2&-1&4&|&-3\\1&-2&0&-3&4&|&a\end{array}\right.\right]
$$

以第一行为基础，开始执行乘和加变换，将第一行乘以 -4 的结果和第二行相加，再将第一行乘以 2 的结果，再和第三行相加，不断消元

$$
\left.\left[\begin{array}{cccccc|c}1&-2&1&-1&1&|&0\\0&0&1&-1&3&|&-2\\0&0&0&1&-2&|&1\\0&0&0&0&0&|&a+1\end{array}\right.\right]
$$

得出 a 为 -1时，方程有解

通过程式之间的运算，消除未知的x

### 最小二乘法

解未知参数，使得理论值与观测值之差（即误差，或者说残差）的平方和达到最小

## PCA主成分分析

一种针对数值型特征、较为通用的降维方法

![二维数据降到一维](/assets/20231027202730.png)

1. 标准化样本矩阵中的原始数据；
2. 获取标准化数据的协方差矩阵；
3. 计算协方差矩阵的特征值和特征向量；
4. 依照特征值的大小，挑选主要的特征向量；
5. 生成新的特征

主成分每个轴之间都会成九十度

## SVD 奇异值分解

通过样本矩阵本身的分解，找到一些“潜在的因素”，然后通过把原始的特征维度映射到较少的潜在因素之上，达到降维的目的
