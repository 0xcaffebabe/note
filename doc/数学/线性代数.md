---
tags: ['线性代数', '向量空间', '线性变换', '矩阵理论', '降维']
---

# 线性代数

> **核心问题**：如何用**线性结构**来表示、变换与压缩信息？
>
> 线性代数并不是关于"算矩阵"，而是关于：
>
> * 如何表示世界（Representation）
> * 如何改变视角（Transformation）
> * 如何在信息不丢失或最优丢失的前提下进行简化（Optimization & Compression）

---

## 一、线性代数的整体认知地图

```
线性代数的三大核心问题
├── 表示（Representation）
│   ├── 向量
│   ├── 向量空间 / 子空间
│   ├── 基、维度、秩
│
├── 变换（Transformation）
│   ├── 线性映射
│   ├── 矩阵（映射的坐标表示）
│   ├── 核空间 / 像空间
│
└── 优化与压缩（Optimization & Compression）
    ├── 投影
    ├── 最小二乘
    ├── PCA
    ├── SVD
```

> **稳定认知**：所有后续方法，都是在这三类问题中反复出现的不同形式。

---

## 二、表示世界：向量与向量空间

### 2.1 向量的本质

> 向量不是“数组”，而是**可以进行线性组合的抽象对象**。

向量的核心价值在于：

* 可以被加权
* 可以被叠加
* 可以被投影到其他方向

这使得向量成为：

* 状态的表示
* 特征的载体
* 信息的最小表达单元

数学上，一个向量是一组**有序标量**：
[
\mathbf{x} = (x_1, x_2, \dots, x_n)
]

---

### 2.2 向量空间：为什么不是任意集合

> 向量空间 = **允许线性运算的世界**。

设标量域为 (F)，若集合 (V\subseteq F^n) 满足：

* 对加法封闭
* 对标量乘法封闭

则称 (V) 是 (F) 上的向量空间。

**第一性原理解释**：

* 封闭性保证了“组合后仍然合法”
* 向量空间是“线性推理”得以成立的最小结构

---

### 2.3 子空间：结构中的结构

> 子空间不是子集，而是**保留线性结构的子集**。

若 (U \subseteq V)，且 (U) 本身仍是向量空间，则 (U) 为 (V) 的子空间。

子空间的意义在于：

* 描述约束
* 表示低维结构
* 是“降维”的理论基础

---

### 2.4 度量、长度与角度：几何结构的引入

向量本身只是代数对象，
**一旦引入距离或内积，就获得了几何意义**。

#### 距离（相似度的不同假设）

* 曼哈顿距离：轴对齐世界
* 欧氏距离：连续几何世界
* 切比雪夫距离：最大偏差控制

> 距离的选择，本质是对“世界几何形态”的假设。

#### 内积与夹角

内积定义了：

* 长度
* 角度
* 正交性

[
\cos(\theta) = \frac{\langle x, y \rangle}{|x||y|}
]

这是后续**投影、最优化、PCA**的基础。

---

## 三、表示能力的极限：线性无关、基与维度

### 3.1 线性无关：冗余的判别标准

一组向量 ({v_1, \dots, v_n}) 若满足：
[
a_1v_1 + \cdots + a_nv_n = 0 \Rightarrow a_1=\cdots=a_n=0
]

则称其线性无关。

> 本质：没有向量是“多余的”。

---

### 3.2 基：最小且完备的表达系统

> 基 = **最小生成集 + 唯一坐标表示**。

任意向量空间都存在基，且：

* 任意向量都能唯一表示为基的线性组合
* 所有基的向量个数相同

这个数量称为：**维度**。

---

### 3.3 秩：矩阵的表达能力

矩阵的秩 =

* 行空间维度 = 列空间维度

> 秩衡量的是：
> **一个线性系统最多能表达多少独立信息**。

---

## 四、改变视角：线性映射与矩阵

### 4.1 线性映射的本质

> 线性映射描述的是：
> **结构保持的变化**。

映射 (f: V \to W) 若满足：

* (f(u+v)=f(u)+f(v))
* (f(cv)=cf(v))

则称为线性映射。

---

### 4.2 矩阵：线性映射的坐标表示

> **矩阵不是数表，而是映射在某组基下的表示**。

一旦选定基：

* 抽象映射 → 数值矩阵
* 映射复合 → 矩阵乘法

---

### 4.3 核空间与像空间

* 核空间：被压缩为零的方向
* 像空间：实际可达的输出空间

它们揭示了：

* 哪些信息被丢失
* 哪些信息被保留

---

## 五、仿射空间：引入“位置”的线性世界

> 向量空间关心方向，仿射空间关心位置。

仿射空间 = 向量空间 + 平移

[
L = x_0 + U
]

这是：

* 直线
* 平面
* 超平面

的统一抽象。

---

## 六、求解与优化：线性系统与最小二乘

### 6.1 高斯消元：解的结构分析

高斯消元的意义不在“算解”，而在：

* 判断是否有解
* 判断解的自由度

---

### 6.2 最小二乘：投影思想

> 最小二乘的本质：
> **把观测投影到可解释的子空间上**。

这引出了：

* 正交投影
* 误差最小化

---

## 七、压缩与表示最优性：PCA 与 SVD

### 7.1 PCA 的第一性原理

> PCA 的问题不是“怎么降维”，而是：
> **在哪个坐标系下，信息最集中？**

PCA 做的事情：

* 寻找一组正交基
* 使投影后的方差最大

---

### 7.2 SVD：最一般的线性分解

> SVD 揭示了任意矩阵的本质结构。

[
A = U \Sigma V^T
]

它统一了：

* PCA
* 低秩逼近
* 噪声过滤

---

### 7.3 PCA 与 SVD 的统一视角

| 维度  | PCA  | SVD    |
| --- | ---- | ------ |
| 出发点 | 协方差  | 原矩阵    |
| 本质  | 正交投影 | 最优低秩分解 |
| 目标  | 信息集中 | 结构分离   |

> 二者本质上都是：
> **在所有子空间中寻找最优表示**。

---

## 八、线性代数的长期价值

线性代数不是工具箱，而是：

* 表示学习的骨架
* 深度学习的底层结构
* 现代科学建模的语言

> 一旦理解其抽象结构，
> 具体算法只是自然推论。

---

**结束语**：

> 学线性代数，不是为了记住公式，
> 而是为了获得一种**线性看世界的能力**。

## 关联内容（自动生成）

- [/数据技术/机器学习.md](/数据技术/机器学习.md) 线性代数是机器学习的数学基础，特别是矩阵与向量运算在数据建模中的应用
- [/数据技术/推荐系统.md](/数据技术/推荐系统.md) 推荐系统中的矩阵分解和SVD算法直接应用了线性代数中的降维技术
- [/数据技术/深度学习.md](/数据技术/深度学习.md) 深度学习中的卷积神经网络使用矩阵表示图像数据并进行特征提取和降维
- [/算法与数据结构/图.md](/算法与数据结构/图.md) 图的邻接矩阵表示法是线性代数在图论中的重要应用
- [/软件工程/架构/Web前端/可视化.md](/软件工程/架构/Web前端/可视化.md) 计算机图形学中的线性变换、旋转、缩放等操作都基于线性代数原理
- [/数据技术/非监督学习.md](/数据技术/非监督学习.md) 降维技术如PCA是线性代数在机器学习中的重要应用之一
- [/数据技术/特征工程.md](/数据技术/特征工程.md) 特征向量和非负矩阵因式分解等概念直接来源于线性代数
- [/操作系统/死锁.md](/操作系统/死锁.md) 死锁检测中的资源分配矩阵体现了线性代数在系统设计中的应用
- [/计算机网络/网络层.md](/计算机网络/网络层.md) 网络路由中的距离矩阵算法与线性代数相关
- [/软件工程/架构/Web前端/可视化.md](/软件工程/架构/Web前端/可视化.md) 3D图形渲染中的模型矩阵、视图矩阵等都是线性代数的应用
