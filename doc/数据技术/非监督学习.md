---
tags: ['非监督学习', '机器学习', '数据科学', '聚类', '关联规则挖掘']
---

# 非监督学习

> **从无标签数据中恢复世界结构的学习范式**

---

## 一、第一性原理：非监督学习到底在学什么？

**非监督学习的本质不是“没有标签的监督学习”**，而是：

> **在缺乏外部语义标注的前提下，假设数据本身蕴含某种内在结构，并试图恢复这种结构。**

这些“结构”不是算法决定的，而是**建模假设**决定的。

从稳定、跨算法的视角，可以将非监督学习统一抽象为三类**结构学习问题**：

| 结构类型       | 学习对象       | 核心问题        |
| ---------- | ---------- | ----------- |
| 共现 / 频率结构  | 事件是否经常同时出现 | 世界中哪些因素彼此相关 |
| 几何 / 拓扑结构  | 数据在空间中如何聚集 | 世界是如何被自然分组的 |
| 概率 / 支持域结构 | 什么是“正常范围”  | 世界的常态边界在哪里  |

对应形成三大方法族：

* **关联规则挖掘** → 频率结构
* **聚类** → 几何 / 分布结构
* **异常检测** → 概率支持域结构

> 后续所有算法，都是对这些结构假设的不同工程实现。

---

## 二、关联规则挖掘：学习“共现结构”

### 1. 建模哲学

关联规则挖掘假设：

> **如果两个事件在数据中经常同时出现，那么它们之间可能存在某种潜在关系。**

它不关心因果，只关心**统计共现**。

---

### 2. 核心度量：频率与条件关系

#### 支持度（Support）

> 描述某个事件组合在整体中的**出现频率**

[
Support(X,Y)=P(X,Y)=\frac{count(X,Y)}{N}
]

* 是**全局频率指标**
* 决定“是否值得关注”

---

#### 置信度（Confidence）

> 描述在 Y 已发生的条件下，X 发生的可能性

[
Confidence(X\Leftarrow Y)=P(X|Y)=\frac{P(X,Y)}{P(Y)}
]

* 是**条件概率**
* 容易被高频项误导

---

#### 提升度（Lift）

> 衡量条件关系是否“超出随机独立假设”

[
Lift(X\rightarrow Y)=\frac{P(Y|X)}{P(Y)}
]

* Lift > 1：正相关
* Lift = 1：独立
* Lift < 1：负相关

👉 **提升度才是真正的“相关性校正指标”**

---

### 3. 频繁项集挖掘的核心不变量

#### Apriori 原理（反单调性）

* 若项集是频繁的，其所有子集必然频繁
* 若项集非频繁，其所有超集必然非频繁

这是：

> **搜索空间剪枝的数学基础**

---

### 4. 算法演进逻辑

| 算法        | 核心思想         | 演进动因    |
| --------- | ------------ | ------- |
| Apriori   | 广度优先 + 候选剪枝  | 频繁扫描数据集 |
| FP-Growth | 压缩表示 + 条件模式基 | 减少扫描次数  |

FP-Growth 的本质不是“更聪明”，而是：

> **用结构换时间，用内存换扫描成本**

---

## 三、聚类：学习“空间与分布结构”

### 1. 聚类的统一建模假设

所有聚类算法都隐含一个前提：

> **相似的样本在某种空间或分布中应当彼此接近。**

不同算法的差异在于：

* “相似”如何定义？
* “簇”被假设成什么形态？

---

### 2. 稳定的聚类分类维度（而非算法记忆）

| 维度       | 关键问题      |
| -------- | --------- |
| 距离 vs 分布 | 是否显式建模概率？ |
| 参数依赖     | 是否需要预设 K？ |
| 簇形状假设    | 是否只支持凸簇？  |
| 噪声鲁棒性    | 是否能识别离群点？ |

---

### 3. 聚类方法族

#### （1）层次聚类 —— 结构优先

* 通过逐步合并 / 分裂构建树状结构
* 不需要预设簇数
* 适合探索性分析

👉 输出的是**层次关系本身**

---

#### （2）原型聚类（K-means）—— 几何均值假设

**建模假设**：

* 簇是凸的
* 簇中心可用均值表示

**目标函数**：
[
J=\frac1m\sum_{i=1}^{m}|x^{(i)}-\mu_{c^{(i)}}|^2
]

**算法不变量**：

* 每次迭代目标函数**单调不增**
* 若上升 → 实现或数值错误

**工程代价**：

* 依赖初始点
* 易陷入局部最优

---

#### （3）分布聚类（EM / GMM）—— 概率生成模型

**核心思想**：

> 数据来自多个隐含分布的混合

EM 是一个**优化框架**：

* E-step：估计隐变量
* M-step：最大化参数

👉 聚类结果是**概率归属，而非硬划分**

---

#### （4）密度聚类（DBSCAN）—— 支持域假设

* 高密度区域是簇
* 低密度区域是分隔或噪声

优点：

* 无需 K
* 可识别任意形状簇

代价：

* 对参数和尺度敏感

---

## 四、异常检测：学习“正常性的边界”

### 1. 异常检测的核心哲学

异常检测的关键假设是：

> **正常数据有稳定结构，异常是对该结构的偏离。**

因此：

* 不直接学习异常
* 而是**建模正常性**

---

### 2. 密度估计范式

#### 判定规则

[
p(x)\begin{cases}
<\varepsilon & anomaly\
\ge\varepsilon & normal
\end{cases}
]

这是一个**支持域判定问题**。

---

### 3. 高斯异常检测模型

假设各特征独立且服从高斯分布：

[
\mu_j=\frac{1}{m}\sum x_j^{(i)},\quad
\sigma_j^2=\frac{1}{m}\sum(x_j^{(i)}-\mu_j)^2
]

联合概率：
[
p(\vec{x})=\prod_{j=1}^np(x_j;\mu_j,\sigma_j^2)
]

---

### 4. 阈值选择与数据现实

* 阈值 ε 通过验证集选择
* 异常样本稀少、分布多样
* 本质上是**开放世界问题**

---

### 5. 异常检测 vs 监督学习（本质对比）

| 维度    | 异常检测    | 监督分类  |
| ----- | ------- | ----- |
| 建模对象  | 正常分布    | 类边界   |
| 对异常假设 | 未知、多样   | 稳定、可见 |
| 适用场景  | 欺诈、设备监控 | 分类、识别 |

---

## 五、方法演进与稳定认知

非监督学习的演进不是“算法更新”，而是：

> **数据规模、维度与分布复杂度的变化**

* 频率结构 → 从扫描到压缩
* 几何结构 → 从均值到密度
* 正常性模型 → 从参数分布到表征学习

---

## 六、最终总结：稳定知识视角

* 非监督学习 ≠ 算法集合
* 非监督学习 = **结构学习范式**
* 算法只是：

  > 假设 × 数据 × 计算资源 的交点

## 关联内容（自动生成）

- [/数据技术/机器学习.md](/数据技术/机器学习.md) 机器学习是监督学习和非监督学习的上层概念，非监督学习是机器学习的重要范式之一，两者在模型优化、泛化能力等方面有密切关系
- [/数据技术/监督学习.md](/数据技术/监督学习.md) 监督学习与非监督学习是机器学习的两大范式，对比理解有助于深入掌握非监督学习的特点
- [/数据技术/深度学习.md](/数据技术/深度学习.md) 深度学习在非监督学习领域有重要应用，如自编码器和生成模型
- [/数据技术/特征工程.md](/数据技术/特征工程.md) 非监督学习中的聚类、降维等方法常用于特征工程中的特征变换和降维处理，提升特征质量
- [/数学/线性代数.md](/数学/线性代数.md) 线性代数是机器学习的数学基础，特别是矩阵与向量运算在数据建模中的应用
- [/数学/概率论与数理统计.md](/数学/概率论与数理统计.md) 概率论为非监督学习中的概率图模型、贝叶斯方法等提供理论基础
