# 反爬虫

- 反爬不仅单单是技术上的，还是法律上的、商业上的

## 接口定制化反爬

对于目的明确的爬虫，只访问特定的接口，这种特征意味着反爬方可以通过镇对特定IP进行接口限流来防御

但爬方可以通过访问其他接口来伪装，同时可以上代理服务器突破单个IP阈值

最后，由于NAT的存在，这种方式可能会误杀正常用户

### 数据投毒

明确对方爬数据的意图，返回造成其无法达成目的、利于自身的假数据

### 随机来回

对于数据下毒混淆，对于识别出爬虫的客户端使用随机的策略来避免爬虫方通过数据的整体统计出正确值

爬虫方此时若想跳出与反爬方一来一回的结局识别出假数据，只能通过更深的伪装，将伪装成真正的用户请求的数据与假数据比对来发现出假数据

但一来一回，随着反爬措施升级，势必会误伤真实用户

## 字体反爬

- 使用自制字体将一个加密的结果映射为真实的结果

这种方式对于网络有较高要求，在弱网环境下不仅会误伤真实用户，反爬效率也不高，当误杀真实用户时，甚至有陷入舆论风波的危险

## 浏览器模拟反爬

当爬虫直接使用模拟浏览器爬数据，通过canvas指纹、dom指纹，本质都是识别请求特征，对于这种特征的请求统统封杀

更疯狂的可以故意拖慢浏览器性能，降低爬虫爬的量，只是苦了用户

## 验证码反爬

简单的字符验证码防不住OCR，复杂的验证码可以接入打码平台，而且愈复杂的验证码愈影响用户体验

## 网络攻防反爬

有些使用前端加密的反爬都有一段核心加密代码，这段代码在浏览器可以直接获取到，如果爬虫想模拟请求，可能会尝试执行摘下来这段代码在本地去执行从而去模拟加密发起请求，如果这段代码包含恶意分支，那么可以直接对爬虫的本地环境发起攻击，无论是格盘、还是什么恐怖的操作

进行到这一步，就是从反爬转守为攻，可以是社工、可以是悄悄潜伏

## 爬虫行为

### 协议

- HTTP 应用层的协议 基础平台都提供了支持
- TCP 除了解决HTTP类库的细节封装问题 对于移动端软件 需要分析自由协议
- FTP

### 抓取

- 定时 
- 实时 对于有热点的数据 实时爬取这些 很容易造成本系统的核心数据外泄给反爬方 

### 解析验证

反爬方可以返回不标准的标签但浏览器可以识别的标签来干扰爬虫的解析

此时直接使用浏览器的DOM解析虽然可以规避，但是效率很低

而为了验证是不是假数据，通过机器进行交叉验证，也就说对比其多个端的接口看是否一致，也可以进行人工抽检，当发现假数据，应将本批数据全部作废

## 爬虫检测

### 网络层检测

这一层一般就是对检测出属于爬虫的IP或者IP段进行封锁

需要配合白名单机制防止误伤

通过WHOIS识别出对方是不是公有云，或者进行端口扫描其有没有开放的端口，总之不属于正常用户的特征统统封杀

### 应用层检测

学习并聚合非爬虫的HTTP请求特征，爬虫很难一直模仿正常的浏览器

像是请求头

### 浏览器特征检测

以DOM指纹为代表的技术就是通过发现爬虫渲染的DOM特征与正常用户的DOM特征不一样来检测

具体的操作过程，其实就是从 Window 开始往下拉一棵树，循环引用跳掉，最终得到的 DOM 结构

### 业务特征检测

爬虫跟正常用户总是拥有不一样的行为，利用业务流程的特性，发现爬虫

## 分布式爬虫

分布式不仅可以使用更多的IP资源进行伪装 同时进行异构分布式也能使爬虫很好伪装自己 提升可用性

## 爬虫的法律问题

- 侵权
- 破坏计算机系统
- 侵犯个人隐私

## 爬虫到道德问题

- 降低对方站点压力
- 不得牺牲用户体验

## 后端反爬

### 特征检测模块

- 在线检测 针对近期的流量做样本，合并做一个集合。在集合中取特征平均值，最后拿当前流量做比较就可以了。如果后续单个流量过来后，特征不符合统计分布，那么这个流量就是有问题的
- 离线检测 流量放到线下，抽时间慢慢算，算好了给线上用就可以了
- 混合检测 在线检测的问题是对于生产的压力有点大，对于实时性高的规则，可以使用在线检测，对于低的则可以在业务低谷利用空闲服务器来计算

### key生成

某些操作总是需要前端向后端传递一个特殊看似随机的key的 为了防爬虫 这个key要如何生成？

一个很重要的点就是需要引入随机跟混淆 随机又不能真随机 否则后端无法校验

这点跟一些接口使用签名的方式是一样的

```js
var left = md5(sku+current_hour+fingerprint(http-header))// 32位
var right = md5(random())// 取一个随机值，并md5掉，用于混淆key
var result = xor(left, right)+right// 两个key异或作为新key的left，right不变。
```

为了更加变态的反爬，可以通过后端下发预定义的n个加密策略，随机组合来进行加密

但是这些加密策略最终也是要在前端变成js代码执行的，如果保护代码：

- 变量名混淆 主要是为了降低可读性
- eval
- js实现解释虚拟机

### 随机

对于后端反爬，不做百分百封杀 而是通过返回小部分假数据的方式 只要存在一定假数据 对方爬取的这批数据就无法真正地敢用在生产环境

### 效果检测

- 误伤率：对于检测出爬虫的客户端 使用Cookie标记出来 在只有用户才可能访问的页面看能不能拿到 来计算 在这里Cookie存放的信息也需要加密
- 爬虫熔断：当发现误伤过高 赶紧关掉爬虫检测

## 前端反爬

### 信息收集

为了收集足够多的信息来对抗爬虫，需要注意的有：

- 这些信息最终应该计算成指纹 避免明文被反攻
- 指纹计算应该加入盐
