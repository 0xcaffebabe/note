# 爬虫

## 运行方式

1. 起始爬行页面集合：通过手动指定或者自动化的方式，是爬虫的种子列表
2. 从起始爬行页面提取链接，对链接规范化，处理掉链接的别名（如url里的hash），并将相对链接转为绝对链接，加入爬行列表
3. 减少爬取环路，使用哈希表、布隆过滤器等过滤爬取过的页面，并且要将这些数据持久化
  - 由于诸如CGI以及文件系统的符号链接等的存在，环路是无法完全避免的

### 环路减少策略

- 规范化URL 避免别名
- 广度优先爬行，避免无限深入某一URL链
- 对每个站点进行流量控制，避免爬取的太多
- 限制URL的长度
- 人工监视，维护URL环路黑名单
- 对URL进行模式检测，发现可能是回环的URL
- 内容指纹，对内容进行取数字摘要

## robots.txt

版本  | 说明
--- | -----------------
0.0 | 支持Disallow指令 禁止爬行
1.0 | 支持Allow指令 允许爬行
2.0 | 新增正则表达式及定时功能

### 格式

```text
# 注释: 禁止百度蜘蛛和谷歌蜘蛛爬取/admin
User-Agent: baidu-spider
User-Agent: google-spider
Disallow: /admin

# 禁止所有蜘蛛爬取/super-admin
User-Agent: *
Disallow: /super-admin
```

url的匹配是前缀匹配

## 爬虫META指令

```html
<meta name='robots' content='noindex'>
```

content可取值如下：

- noindex 禁止索引本页面
- nofollow 禁止爬取本页面的外链
- index 可以对本页面索引
- follow 可以爬取本页面的所有外链
- all 等价于 index+follow
- none 等价于 noindex+nofollow

给搜索引擎的META标签

```html
<meta name='description' content="页面描述">
<meta name='keywords' content="关键词1, 关键词2">
<!-- 10天后再来爬一次 -->
<meta name='revisit-after' content="10 days">
```
