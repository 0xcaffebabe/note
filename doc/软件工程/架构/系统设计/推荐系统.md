
# 推荐系统

> 一种信息过滤系统，手段是预测用户（User）对物品（Item）的评分和偏好

- 能做什么：把那些最终会在用户（User）和物品（Item）之间产生的连接提前找出来
- 需要什么：根据存在的连接，从已有的连接去预测未来的连接
- 怎么做：机器推荐与人工推荐

## 问题

### 模型

1. 评分预测 假如用户消费完一个物品之后会给出一个打分，能不能提前预测一个用户对每一个物品会打多少分，找出那些他可能会打高分的物品，推荐给他
2. 行为预测 利用隐式反馈数据预测隐式反馈的发生概率

评分预测的问题在于评分比较难收集，并且评分更偏主观，这种主动告知评分称之为显式反馈，与之相对的还有隐式反馈，通常根据各类用户行为对物品进行打分

隐式反馈能收集的数据更多，并且更代表用户的真实想法，常常和模型的目标函数关联更密切

### 顽疾

1. 冷启动 缺乏相关数据的用户和物品，很难直接加入到推荐系统
2. 探索与利用 EE 问题，已知用户喜好，如何给他推荐
3. 安全问题 推荐系统攻击问题，不靠谱的推荐、难以消除的脏数据、损害商业利益

## 特性

- 不确定性
- 追求的是目标的增长

## 内容推荐

![内容推荐框架](/assets/2022125203834.webp)

- 内容源：推荐系统必须有源源不断的新鲜数据
- 内容分析：主要是为了结构化内容库，以及产出内容分析模型
- 内容推荐算法：最基本的就是基于向量的相似度算法，更复杂的则是基于机器学习的算法

### 用户画像

- 对用户信息的向量化表示，是构建推荐系统的过程中产生的一个关键环节的副产品

#### 关键因素

1. 维度
2. 维度的量化

这两个关键因素很主观，取决于设计者

#### 构建方法

1. 直接使用原始数据作为用户画像的内容，对于用户冷启动等场景非常有用
2. 通过统计手段从历史数据挖掘出标签
3. 机器学习，提炼出人类无法理解也无法解释的稠密向量

#### 文本构建用户画像

1. 把所有非结构化的文本结构化，去粗取精，保留关键信息
2. 根据用户行为数据把物品的结构化结果传递给用户，与用户自己的结构化信息合并

结构化文本的方法：

1. [TF-IDF](/中间件/检索技术.md#TF-IDF%2d算法)
2. TextRank
3. 内容分类 SVM算法
4. 命名实体识别 进行NLP，分词后识别为定义的命名实体集合之一，可以用提前准备好的实体字典来识别
5. 文本[聚类](/数据技术/数据挖掘.md#群组)
6. 词嵌入 除了 LDA，其他都是得到一些标签，而这些标签无一例外都是稀疏的，而词嵌入则能够为每一个词学习得到一个稠密的向量
   1. Word2Vec

选择标签：

- [卡方检验和信息增益](/通识/数学/概率论与数理统计.md#特征选择)

## 近邻推荐

### 协同过滤

- 基于记忆的协同过滤（Memory-Based）
- 基于模型的协同过滤（Model-Based）

#### 基于用户

一些工程上的问题：

1. 相似度计算，如果物品很多，即向量很长，为了降低复杂度，有两种方法
   1. 对向量采样计算 随机选取n个维度来计算
   2. 向量运算
2. 如果用户量很大，两两之间计算代价就很大，这个时候就需要引入Map Reduce之类的并行计算来加快速度
3. 推荐计算：为每一个用户计算每一个物品的推荐分数，优化方式是只有相似用户喜欢过的物品需要计算，另外一个就是并行计算
4. 权重，一般来说，热门、过期的物品，权重值越低

#### 基于物品

首先计算相似物品，然后再根据用户消费过、或者正在消费的物品为其推荐相似的

相似度算法改进：

1. 物品中心化。把矩阵中的分数，减去的是物品分数的均值，可以去掉评分中的非理性因素
2. 用户中心化。把矩阵中的分数，减去对应用户分数的均值，一定程度上仅仅保留了偏好，去掉了主观成分

推荐结果：

1. 计算用户对所有物品的喜欢度，进行TOPK推荐
2. 相关推荐

**Slope One 算法**

#### 相似度算法

- [向量距离](/通识/数学/线性代数.md#距离)

欧式距离度量的是空间中两个点的绝对差异，适用于分析用户能力模型之间的差异，比如消费能力、贡献内容的能力

余弦相似度则是对两个向量进行归一化处理，对绝对值不敏感

皮尔逊相关度度量的是两个变量的变化趋势是否一致，所以不适合用作计算布尔值向量之间相关度

杰卡德（Jaccard）相似度：两个集合的交集元素个数在并集中所占的比例，适合用于隐式反馈数据

## 矩阵分解

近邻模型的问题：

1. 物品之间存在相关性，信息量并不随着向量维度增加而线性增加
2. 矩阵元素稀疏，计算结果不稳定

矩阵分解，直观上说来简单，就是把原来的大矩阵，近似分解成两个小矩阵的乘积，在实际推荐计算时不再使用大矩阵，而是使用分解得到的两个小矩阵，也就是降维，[SVD](/通识/数学/线性代数.md#SVD%20奇异值分解)是其中的一种算法

![屏幕截图 2022-12-07 203725](/assets/屏幕截图%202022-12-07%20203725.png)

整个 SVD 的学习过程就是：

1. 准备好用户物品的评分矩阵，每一条评分数据看做一条训练样本
2. 给分解后的 U 矩阵和 V 矩阵随机初始化元素值
3. 用 U 和 V 计算预测后的分数
4. 计算预测的分数和实际的分数误差
5. 按照梯度下降的方向更新 U 和 V 中的元素值
6. 重复步骤 3 到 5，直到达到停止条件

偏置信息：一个用户给一个物品的评分会由全局平均分、物品的评分偏置、用户评分的偏置、用户和物品之间的兴趣偏好四部分相加

历史行为：用户有过行为的物品集合也都有一个隐因子向量，维度是一样的。把用户操作过的物品隐因子向量加起来，用来表达用户的兴趣偏好。另外一个是用户属性，全都转换成 0-1 型的特征后，对每一个特征也假设都存在一个同样维度的隐因子向量，一个用户的所有属性对应的隐因子向量相加，也代表了他的一些偏好

时间因素：让久远的评分更趋近平均值、不同的时间区间内分别学习出隐因子向量

### 交替最小二乘

要把一个矩阵分解为两个矩阵的相似解：

1. 初始化随机矩阵 Q 里面的元素值
2. 把 Q 矩阵当做已知的，直接用线性代数的方法求得矩阵 P
3. 得到了矩阵 P 后，把 P 当做已知的，故技重施，回去求解矩阵 Q
4. 上面两个过程交替进行，一直到误差可以接受为止

加权交替最小二乘对待隐式反馈：

- 对物品无隐式反馈则认为评分是 0
- 用户对物品有至少一次隐式反馈则认为评分是 1，次数作为该评分的置信度

没有反馈的缺失值，就是在我们的设定下，取值为 0 的评分就非常多，就需要进行负样本采样

因此按照物品热门程度采样的思想就是：一个越热门的物品，用户越可能知道它的存在。那这种情况下，用户还没对它有反馈就表明：这很可能就是真正的负样本

得到了分解后的矩阵后，相当于每个用户得到了隐因子向量，这是一个稠密向量，用于代表他的兴趣，让用户和物品的隐因子向量两两相乘，计算点积就可以得到所有的推荐结果

### 贝叶斯个性化排序BPR

- 排序的评价指标：AUC，全称是 Area Under Curve，意思是曲线下的面积，这里的曲线就是 ROC 曲线

BPR做了三件事：

1. 一个样本构造方法
2. 一个模型目标函数
3. 一个模型学习框架

## 模型融合

推荐系统技术实现的三个阶段：

1. 挖掘：对用户和物品做非常深入的结构化分析
2. 召回：每次给一个用户计算推荐结果时，用一些手段从全量的物品中筛选出一部分
3. 排序

### 逻辑回归和梯度提升决策树组合

逻辑回归输出值范围就是 0 到 1 之间，是广义线性模型

树模型天然就可以肩负起特征组合的任务，最原始的是决策树，简称 DT

![2022127213446](/assets/2022127213446.webp)
