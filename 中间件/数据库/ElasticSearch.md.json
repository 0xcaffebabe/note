{"name":"ElasticSearch","id":"中间件-数据库-ElasticSearch","content":"# ElasticSearch\n\n> ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口\n\n- Near Realtime\n  - 从写入数据到数据可以被搜索到有一个小延迟，大概是 1s\n  - 基于 es 执行搜索和分析可以达到秒级\n\n## 优势\n\n- 横向可扩展\n- 分片机制提供更好的分布性\n- 高可用\n\n## 安装\n\n> 使用 docker\n\n```shell\ndocker run elasticsearch:7.3.1\n```\n\n```shell\ndocker network create somenetwork;\ndocker run -d --name elasticsearch --net somenetwork -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:7.3.1\n```\n\n9300端口： ES节点之间通讯使用\n\n9200端口： ES节点 和 外部 通讯使用\n\n## 概念\n\n- 集群(cluster)\n  - 多个节点组成\n- 节点(node)\n  - 服务器实例\n- 索引（index）\n  - Databases 数据库\n- ​文档（Document）\n  - Row 行\n- ​字段（Field）\n  - Columns 列\n- primary shard\n  - es 可以将一个索引中的数据切分为多个 primary shard，分布在多台服务器上存储，以此解决水平扩展问题\n- replica shard\n  - 任何一个服务器随时可能故障或宕机，此时 primary shard 可能就会丢失，因此可以为每个 primary shard 创建多个 replica shard。replica 可以在 primary shard 故障时提供备用服务，所以同一片 primary shard 跟 replica primary 不能存放在同一节点\n- 映射 mapping：类似于 schema\n\n```mermaid\nstateDiagram\n    state \"es cluster\" as es_cluster {\n        state Node1 {\n            state Shard1\n            state Replica1\n            Shard1 --> Replica1\n        }\n        state Node2 {\n            state Shard2\n            state Replica2\n            Shard2 --> Replica2\n        }\n        state Node3 {\n            state Shard3\n            state Replica3\n            Shard3 --> Replica3\n        }\n        state Node4 {\n            state Shard4\n            state Replica4\n            Shard4 --> Replica4\n        }\n    }\n\n```\n\n## 数据类型\n\n- text：该类型被用来索引长文本，在创建索引前会将这些文本进行分词，转化为词的组合，建立索引；允许es来检索这些词，text类型不能用来排序和聚合。\n- keyword：该类型不需要进行分词，可以被用来检索过滤、排序和聚合，keyword类型自读那只能用本身来进行检索（不可用text分词后的模糊检索）\n- 数值型：long、integer、short、byte、double、float\n- 日期型：date\n- 布尔型：boolean\n- 二进制型：binary\n\n## CRUD\n\n- create 操作：如果 ID 不存在，则新增，如果 ID 存在，则更新\n- index 操作：如果 ID 不存在，则新增，如果 ID 存在，则删除后新增，版本号会增加\n- update 操作：文档必须已存在，更新只会对相应的字段做增量修改\n\n## 批量API\n\n- bulk 操作：批量操作，一次请求可以执行多个操作，包括索引、更新、删除等操作\n- mget\n- msearch\n\n## 查询\n\n### 指定查询索引\n\n语法|范围\n-|-\n/_search|集群上所有的索引\n/index1/_search|index1\n/index1,index-2/_search|index1和index2\n/index*/_search|以index开头的索引\n\n### 基本查询\n\n```json\nGET /索引库名/_search\n{\n    \"query\":{\n        \"查询类型\":{\n            \"查询条件\":\"查询条件值\"\n        }\n    }\n}\n```\n\n- 根据ID查询\n\n`GET http://my-pc:9200/blog/hello/1`\n\n- 根据字段查询\n\n>Term Query为精确查询，在搜索时会整体匹配关键字，不再将关键字分词。 \n\n```json\nGET /shop/_search\n{\n  \"_source\": [\"title\",\"price\"],\n  \"query\": {\n    \"term\": {\n      \"price\": 2699\n    }\n  }\n}\n```\n\n- queryString查询\n\n```json\n{\n    \"query\":{\n        \"query_string\":{\n            \"default_field\":\"content\",\n            \"query\":\"内容\"\n        }\n    }\n}\n```\n\n**过滤**\n\n- includes：来指定想要显示的字段\n- excludes：来指定不想要显示的字段\n\n```json\nGET /shop/_search\n{\n  \"_source\": {\n    \"includes\":[\"title\",\"price\"]\n  },\n  \"query\": {\n    \"term\": {\n      \"price\": 2699\n    }\n  }\n}\n```\n\n**排序**\n\n```json\nGET /shop/_search\n{\n  ...\n  \"sort\": [\n    {\n      \"price\": {\n        \"order\": \"desc\"\n      }\n    }\n  ]\n}\n```\n\n**模糊查询**\n\n```json\nGET /heima/_search\n{\n  \"query\": {\n    \"fuzzy\": {\n        \"title\": {\n            \"value\":\"appla\",\n            \"fuzziness\":1\n        }\n    }\n  }\n}\n```\n\n### term查询与全文查询\n\n- term查询包括 Term Query / Range Query / Exists Query / Prefix Query /Wildcard Query。 是精确查询，用于查找字段中完全匹配指定值的文档。它不进行分词，也就是说，它不会将搜索的词拆分成更小的部分，而是将整个词作为一个单元进行匹配\n- 全文查询包括 Match Query / Match Phrase Query / Query String Query / Simple Query String Query。 索引跟查询的时候都会进行分词，查询是根据分词的词项进行底层查询并进行合并，为每个文档生成一个分数\n\n### Function Score Query\n\n可以在查询结束后，对每一个匹配的文档进行一系列的重新算分，根据新生成的分数进行排序。\n\n- Weight：为每一个文档设置一个简单而不被规范化的权重\n- Field Value Factor:使用该数值来修改_score\n- Random Score:为每一个用户使用一个不同的，随机算分结果\n- 衰减函数：以某个字段的值为标准，距离某个值越近，得分越高\n- Script Score：自定义脚本完全控制所需逻辑\n\n### Suggester\n\n### 分页\n\n> 深度分页问题：对于大分页或者一次查询大量文档，ES 需要计算所有之前页的数据，这会导致大量的内存和CPU使用\n\n为了解决这个问题，可以使用 search_after ，其可以通过指定 id 参数，实现类似于 where id > ? limit xx 的功能\n\n另外还有一个 scroll api，其可以创建一个数据的快照，实现类似于 resultset.next() 的功能。适用于需要遍历大量文档的场景\n\n## 分词\n\n分词器是专门处理分词的组件，Analyzer由三部分组成：\n\n- Character Filters(针对原始文本处理，例如去除html)\n- Tokenizer(按照规则切分为单词)\n- Token Filter(将切分的的单词进行加工，小写，删除stopwords,增加同义词)\n\n### 内置的分词器\n\n- Standard Analyzer 默认分词器，按词切分，小写处理\n- Simple Analyzer 按照非字母切分（符号被过滤），小写处理\n- Stop Analyzer 小写处理，停用词过滤(the,a,is)\n- Whitespace Analyzer 按照空格切分，不转小写\n- Keyword Analyzer 不分词，直接将输入当作输出\n- Patter Analyzer 正则表达式，默认\\W+(非字符分隔)\n- Language Analyzers 内置 30 种语言的分词器\n- Fingerprint Analyzer\n\n### 测试分词\n\n`GET /_analyze`\n\n```json\n{\n  \"analyzer\": \"standard\",\n  \"text\": \"中文测试分词\"\n}\n```\n\n### 中文分词器\n\n[下载](https://github.com/medcl/elasticsearch-analysis-ik)\n\n```shell\ndocker run --name elasticsearch --net somenetwork -v /root/plugin:/usr/share/elasticsearch/plugins -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:7.3.1\n```\n\n`GET http://my-pc:9200/_analyze`\n\n```json\n{\n  \"analyzer\": \"ik_max_word\",\n  \"text\": \"中文测试分词\"\n}\n```\n\nik 的两种模式：\n\n- max：会将文本做最细粒度的拆分 会穷尽所有的可能\n- smart：最最粗粒度的划分\n\n### 自定义分词器\n\n```json\n{\n  \"settings\": {\n    \"analysis\": {\n      \"char_filter\": {\n        \"my_char_filter\": {\n          \"type\": \"html_strip\"\n        }\n      },\n      \"tokenizer\": {\n        \"my_tokenizer\": {\n          \"type\": \"standard\"\n        }\n      },\n      \"filter\": {\n        \"my_lowercase_filter\": {\n          \"type\": \"lowercase\"\n        },\n        \"my_stop_filter\": {\n          \"type\": \"stop\",\n          \"stopwords\": \"_english_\"\n        }\n      },\n      \"analyzer\": {\n        \"my_custom_analyzer\": {\n          \"type\": \"custom\",\n          \"char_filter\": [\n            \"my_char_filter\"\n          ],\n          \"tokenizer\": \"my_tokenizer\",\n          \"filter\": [\n            \"my_lowercase_filter\",\n            \"my_stop_filter\"\n          ]\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"content\": {\n        \"type\": \"text\",\n        \"analyzer\": \"my_custom_analyzer\"\n      }\n    }\n  }\n}\n```\n\n## Mapping\n\nMapping 类似数据库中的schema的定义，作用：\n\n1. 定义索引中的字段的名称\n2. 定义字段的数据类型\n3. 进行字段，倒排索引的相关配置，(Analyzed or Not Analyzed,Analyzer)\n\n### Dynamic Mapping\n\n当向 Elasticsearch 插入一个新文档时，且文档包含的字段在索引中尚未定义时，Elasticsearch 会自动推断这些字段的数据类型，并在索引中创建相应的映射\n\n新增加字段：\n\n- Dynamic设为true：当有新增字段的文档写入时，Elasticsearch 会自动更新映射（Mapping）。\n- Dynamic设为false：映射不会更新，新增加的字段数据无法被索引，但信息会出现在 _source 字段中。\n- Dynamic设置为strict：文档写入会失败。\n\n已有字段：\n\n- 一旦已有字段的数据写入后，不再支持修改字段定义。Lucene实现的倒排索引一旦生成后，就不允许修改。\n- 如果希望改变字段类型，必须使用 Reindex API 重建索\n\n### Mapping 配置\n\n- index：是否索引，默认为true。如果设置为 false，该字段不可被搜索\n- index_options：控制倒排索引记录的内容\n  - docs：只记录文档的编号\n  - freqs：记录文档编号和词频\n  - positions：记录文档编号、词频和词的位置\n  - offsets：记录文档编号、词频、词的位置和词的偏移量\n- null_value：只有 keyword 类型才支持\n- copy_to：将字段值复制到其他字段中\n- fielddata：将倒排索引转换为顺序结构，主要用于 text 类型字段，且数据载入内存\n- doc_values：以列存储形式保存在磁盘上，主要用于数值、日期和 keyword 类型字段(2.x 之后默认启用)\n\n## template\n\n### Index Template\n\nIndex Templates 可以帮助设定 Mappings 和 Settings,并按照一定的规则，自动匹配到新创建的索引之上，可以通过设定多个 template，这些设置会被合并在一起，通过指定 order 的数值，来控制合并的过程\n\n### Dynamic Template\n\n用于在索引新文档时，自动根据字段名或字段类型来应用特定的映射规则。它可以在动态映射的基础上提供更细粒度的控制，使得可以指定某些字段的映射类型和其他属性，而不需要事先知道这些字段的名字或类型\n\n```json\n{\n  \"mappings\": {\n    \"dynamic_templates\": [\n      {\n        \"date_fields\": {\n          \"match\": \"date_*\",\n          \"mapping\": {\n            \"type\": \"date\",\n            \"format\": \"yyyy-MM-dd\"\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\n### Search Template\n\n允许预定义查询，并在执行时注入参数。这可以帮助简化复杂查询的重用，并提高性能和可维护性\n\n```json\nPOST _scripts/my_search_template\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"match\": {\n                \"title\": \"{{title}}\"\n              }\n            },\n            {\n              \"range\": {\n                \"release_date\": {\n                  \"gte\": \"{{start_date}}\",\n                  \"lte\": \"{{end_date}}\"\n                }\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n\n```\n\n```json\nPOST _search/template\n{\n  \"id\": \"my_search_template\",\n  \"params\": {\n    \"title\": \"Inception\",\n    \"start_date\": \"2010-01-01\",\n    \"end_date\": \"2010-12-31\"\n  }\n}\n```\n\n### Index Alias\n\n索引别名可以通过别名实现不停机的索引切换与重建、管理索引的视图与读写权限等\n\n```json\nPOST /_alias\n{\n  \"actions\": [\n    {\n      \"add\": {\n        \"index\": \"`screen_recorder_log_202406`\",\n        \"alias\": \"screen_log_now\"\n      }\n    }\n  ]\n}\n```\n\n## 聚合\n\n- Matrix Aggregration 支持对多个字段的操作并提供一个结果矩阵\n\n### Bucket Aggregation\n\n一些列满足特定条件的文档的集合(group by)\n\n```json\n{\n  \"size\": 0, \n  \"aggs\": {\n      \"group\": {\n          \"terms\": {\n            \"field\": \"sales_person.keyword\"\n          }\n      }\n  }\n}\n```\n\n### Metric Aggregation\n\n一些数学运算，可以对文档字段进行统计分析(count、avg、sum)\n\n```json\n{\n  \"size\": 0, \n  \"aggs\": {\n      \"avg_amount\": {\n          \"avg\": {\n            \"field\": \"amount\"\n          }\n      }\n  }\n}\n```\n\n### Pipeline Aggregation\n\n对其他的聚合结果进行二次聚合\n\n```json\nPOST /sales/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"monthly_sales\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"calendar_interval\": \"month\"\n      },\n      \"aggs\": {\n        \"total_sales\": {\n          \"sum\": {\n            \"field\": \"sales_amount\"\n          }\n        }\n      }\n    },\n    \"avg_monthly_sales\": {\n      \"avg_bucket\": {\n        \"buckets_path\": \"monthly_sales>total_sales\" # 对 monthly_sales 聚合结果再进行平均\n      }\n    }\n  }\n}\n```\n\n### 作用范围\n\n- query和filter，是先选定数据范围，再聚合桶\n- post_filter对聚合桶没影响，桶是全部返回，只对查询结果进行过滤返回，类似于 having\n- global的作用是覆盖掉query的查询作用\n\n### 精准度问题\n\nElasticsearch 是一个分布式搜索引擎，其数据分布在多个分片上。当执行聚合操作时，Elasticsearch 会在每个分片上单独计算聚合结果，然后将这些结果合并。在某些情况下（特别是对于 terms 聚合），为了减少内存消耗和提高查询性能，Elasticsearch 不会从每个分片返回所有的桶，而是仅返回前 N 个桶。这可能导致最终结果中的文档计数存在一些误差\n\ndoc_count_error_upper_bound 表示返回的结果中每个桶的文档计数可能偏差的最大值。具体来说，它是一个上限值，表明在分布式聚合过程中，由于只返回部分桶可能导致的最大文档计数误差\n\n可以通过调整 shard_size 的方式减少误差\n\n## ES集群\n\n采用ES集群，将单个索引的分片到多个不同分布式物理机器上存储，从而可以实现高可用、容错性\n\n- Master-eligible Node 是指具有成为主节点资格的节点，Master Node 是当前已经被选举为主节点并执行主节点职责的节点，默认情况下，所有节点都是 Master-eligible node\n- Data Node 负责存储数据并执行数据相关操作，通过增加 Data Node 可以解决数据水平扩展和数据单点问题\n- Coordinating Node 处理来自客户端的请求，分发请求到相关的数据节点，并在所有数据节点上汇总结果后返回给客户端，默认情况下，所有节点都是协调节点\n\n### 架构\n\nes 集群多个节点，会自动选举一个节点为 master 节点。master 节点宕机了，那么会重新选举一个节点为 master 节点\n\n非 master 节点宕机了，那么会由 master 节点，让那个宕机节点上的 primary shard 的身份转移到其他机器上的 replica shard\n\n```mermaid\ngraph LR\n    A[es客户端] -->|将数据写入 primary shard| B[机器1 es进程01 shard01 primary shard 03 replica]\n    A -->|读| C[机器2 es进程02 shard 02 primary shard 01 replica]\n    A -->|读| D[机器3 es进程03 shard 03 primary shard 02 replica]\n    \n    B -->|primary shard 将数据同步到 replica shard 上| C\n    B -->|primary shard 将数据同步到 replica shard 上| D\n\n```\n\n可以使用三个节点，将索引分成三份，每个节点存放一份primary shard，两份replica，这样就算只剩下一台节点，也能保证服务可用\n\n#### 分片路由\n\n读写文档时会根据 hash 计算出文档所属的分片所在的机器，由接入节点将请求转发到分片所在的机器\n\n#### 索引结构\n\n一个 Lucene 索引文件内部存放的内容：\n\n```mermaid\ngraph TD\n    subgraph Term Index in Memory\n        A0\n        A1\n        A2\n        A3\n        A4\n        A5\n        A6\n        A7\n        A8\n        A0 --> A1\n        A1 --> A2\n        A1 --> A3\n        A2 --> A4\n        A3 --> A5\n        A5 --> A6\n        A5 --> A7\n        A7 --> A8\n    end\n\n    subgraph Term Dictionary\n        B0[block]\n        B1[Abc]\n        B2[Abd]\n        B3[Abdf]\n        B4[block]\n        B5[staff]\n        B6[straw]\n        B7[stick]\n        B8[stop]\n        B0 --> B1\n        B0 --> B2\n        B0 --> B3\n        B4 --> B5\n        B4 --> B6\n        B4 --> B7\n        B4 --> B8\n    end\n\n    subgraph Posting List\n        C1[[12,234,5,6,7,78,23]]\n        C2[[122,2364,5,26,722,78,23]]\n        C3[[12,234,23]]\n        C4[[12,23]]\n        C5[[178,23]]\n        C6[[12,234,23]]\n        C7[[77,34,63]]\n    end\n\n    A2 --> B1\n    A3 --> B2\n    A4 --> B3\n    A5 --> B5\n    A6 --> B6\n    A7 --> B7\n    A8 --> B8\n\n    B1 --> C1\n    B2 --> C2\n    B3 --> C3\n    B5 --> C4\n    B6 --> C5\n    B7 --> C6\n    B8 --> C7\n\n```\n\n```mermaid\ngraph TD\n    subgraph Lucene Index\n        Segment1[Segment 1]\n        Segment2[Segment 2]\n        Segment3[Segment 3]\n        Segment4[Segment 4]\n        CommitPoint[Commit Point]\n        DelFile[.del]\n    end\n\n    Segment1 --> CommitPoint\n    Segment2 --> CommitPoint\n    Segment3 --> CommitPoint\n    Segment4 --> CommitPoint\n    CommitPoint --> DelFile\n\n    classDef segment fill:#f9f,stroke:#333,stroke-width:2px;\n    class Segment1,Segment2,Segment3,Segment4 segment;\n\n    classDef commit fill:#ff9,stroke:#333,stroke-width:2px;\n    class CommitPoint commit;\n\n    classDef delete fill:#9ff,stroke:#333,stroke-width:2px;\n    class DelFile delete;\n\n    note[An ES Shard = A Lucene Index]\n```\n\n单个倒排索引文件被称为 Segment Segment是自包含的，不可变更的。多个Segments汇总在一起，称为Lucene的\nIndex,其对应的就是ES中的Shard\n\n当有新文档写入时，会生成新Segment,查询时会同时查询所有Segments,并且对结果汇总。Lucene中有一个文件，用来记录所有Segments信息，叫做Commit Point\n\n删除的文档信息，保存在“.del”文件中\n\n### 部署架构\n\n不同节点类型所需资源：\n\n- master eligible nodes: 负责集群状态(cluster state)的管理，使用低配置的CPU,RAM和磁盘\n- data nodes:负责数据存储及处理客户端请求，使用高配置的CPU,RAM和磁盘\n- ingest nodes:负责数据处理，使用高配置CPU;中等配置的RAM;低配置的磁盘\n- coordination only node：这种节点是专门负责搜索结果的 gather/reduce，对于大集群，可以配置专门的这种节点，防止大量占用内存的聚合操作导致 OOM 影响集群稳定\n\n从高可用的角度，要避免 master 与其他的节点混部\n\n#### 水平扩展\n\n```mermaid\nstateDiagram-v2\n  应用 --> LB\n  LB --> 协调节点1: 读写\n  LB --> 协调节点2: 读写\n  LB --> 协调节点3: 读写\n```\n\n#### 读写分离\n\n```mermaid\nstateDiagram-v2\n  应用 --> 读LB\n  应用 --> 写LB\n  读LB --> 协调节点1: 读\n  读LB --> 协调节点2: 读\n  写LB --> 协调节点3: 写\n  写LB --> 协调节点4: 写\n```\n\n#### 冷热分离\n\n存放热数据的节点一般会有持续的文档写入更新，通常使用 SSD 加上较高性能的 CPU。冷数据节点使用大容量的 HDD 。\n\nES 可以通过 nodeattrs 标记节点类型，通过设置不同的 nodeattrs，将节点分为不同的角色，比如：\n\n- warm\n- hot\n\n在创建或更新索引时，可以通过 settings 指定索引创建在什么节点类型上，来实现冷热分离\n\n使用这个 nodeattr 还可以实现强制要求 shard 分布在不同的节点类型上，类似于 k8s 的节点污点\n\n### 分片设计与管理\n\n7.0 之后，默认只有一个主分片，1 个主分片可以解决算法、聚合不准的问题，但单个主分片无法实现水平扩展\n\n理想情况，主分片数应该大于节点数，当集群增加新节点后，ES 会自动进行 rebalancing 操作。多分片如果分布在不同的节点，读写可以并行扩展，但过多的分片会带来额外的性能开销，除了每次读写要扩散到每个分片上进行读写，还有大量分片元数据给 master 带来的开销。\n\n### 容量规划\n\n核心问题：一个集群总共需要多少个节点？一个索引需要设置几个分片？规划上需要保持一定的余量，当负载出现波动，节点出现丢失时，还能正常运行\n\n做容量规划时，一些需要考虑的因素：\n- 机器的软硬件配置\n- 单条文档的尺寸/文档的总数据量/索引的总数据量(Time base数据保留的时间)/副本分片数\n- 文档是如何写入的(Bulk的尺寸)\n- 文档的复杂度，文档是如何进行读取的（怎么样的查询和聚合）\n\n数据吞吐及性能需求：\n\n- 写入吞吐量\n- 查询吞吐量\n- 单条查询可接受的最大返回时间\n\n了解数据：\n\n- 数据格式与数据的 mapping\n- 实际的查询与聚合是怎么样的\n\n硬件配置：\n\n- 数据节点尽可能使用 SSD\n- 搜索性能要求高的场景，建议使用 SSD，内存与硬盘的比例 1: 10\n- 日志类和查询并发低的可以使用 HDD，内存与硬盘比例 1:50\n- 单节点数据控制在 2TB 内\n- JVM 内存配置为物理机器一半，不超过 32 G\n\n索引拆分：\n\n- 非日志类型的日数据，特点是总量很大，但增长相对较慢，更关心搜索与聚合的性能，关注搜索相关度，可以通过业务维度对索引进行分区，启用 routing 降低查询时相关的 shard\n- 日志类型的数据，特点是每天的增长量多，按时间维度进行分区，对写入性能要求高，可以利用日期对索引分区，做冷热分离，同时每个分区较小，备份及删除的效率高\n\n### 跨集群搜索\n\n### 分布式查询\n\nElasticsearch 将索引的数据分成多个分片（shard），每个分片可以分布在不同的节点上。查询时，这些分片会独立计算相关性评分（score），然后再合并结果。这会导致评分计算时使用的信息不全，从而导致算分不准\n\n解决这个问题的方式是将主分片设置为 1，或者使用 dfs_query_then_fetch 模式\n\n## 并发控制\n\n在更新文档时，可以通过 if_seq_no 与 if_primary_term 选项实现预期版本号与主分片的判断，实现乐观锁\n\n## 数据建模\n\n类型选择：\n\n- 优先考虑反范式化压平字段\n- 当数据包含多数值对象，同时有查询需求，使用nested\n- 关联文档更新非常频繁时，使用父子文档\n\n避免过多字段：过多的字段数不容易维护，mapping 信息保存在Cluster State中，数据量过大，对集群性能会有影响(Cluster State信息需要和所有的节点同步),删除或者修改数据需要reindex，默认最大字段数是1000，可以设置index.mapping.total_fields.limt限定最大字段数。\n\n避免正则查询\n\n避免字段控制导致聚合查询不准\n\n对 mapping 进行管理，禁止更新删除字段，可以对 mapping 写入 meta 信息方便管理\n\n### 字段设置\n\n如不需要检索，排序和聚合分析，source的enable设置成false\n\n如不需要检索 index设置成false\n\n对需要检索的字段，可以通过如下配置，设定存诸粒度\n  - index_options / norms：不需要归一化数据时，可以关闭\n\n如不需要排序或者聚合分析：docvalues / fielddata 设置为false\n\n更新频繁、聚合查询频繁的 keyword 字段，尅将 eager_global_ordinals 设置为 true\n\n### 嵌套对象\n\n嵌套对象类型用于存储嵌套文档。与标准对象类型不同，嵌套对象类型的每个嵌套文档在内部都会被独立索引，从而允许进行更精确的查询，通过在创建 mapping 时指定 type 为 nested 来定义嵌套对象\n\n嵌套查询：\n\n```json\nGET /my_index/_search\n{\n  \"query\": {\n    \"nested\": {\n      \"path\": \"user\",\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            { \"match\": { \"user.first\": \"Alice\" }},\n            { \"match\": { \"user.last\": \"Smith\" }}\n          ]\n        }\n      }\n    }\n  }\n}\n```\n\n### 父子文档\n\n### reindex\n\n支持把文档从一个索引复制到另一个索引，使用场景：\n\n1. 修改主分片数\n2. 改变 mapping 的字段类型\n3. 集群内数据迁移 / 跨集群数据迁移\n\n## 数据处理\n\n### ingest pipeline\n\n可以添加一些简单的 processor 来处理数据\n\n### painless\n\n可以通过脚本对文档字段进行加工处理、在pipeline 中执行脚本、在reindex、update by query 中对数据进行处理\n\n## 监控\n\nES 暴露了诸如 /_cluster/stats /_nodes/stats /{index}/_stats 等接口获取集群、索引内部的状态信息\n\n### 诊断项\n\n```mermaid\nmindmap\n  root((诊断项))\n    集群异常类\n      集群颜色诊断\n      节点失联诊断\n    集群资源类\n      存储资源诊断\n      计算资源诊断\n      master节点高负载诊断\n    日常运维类\n      min master node合理性诊断\n      单节点shard个数诊断\n      shard合理性诊断\n      cluster meta变更频率诊断\n      data node负载高高诊断\n      segment合理性诊断\n      bulk reject 诊断\n      recover速度诊断\n      xpack 类索引过多诊断\n    使用规范类\n      mapping dynamic禁用诊断\n      index副本合理性诊断\n      正则删除禁用诊断\n      别名使用诊断\n      type合理性诊断\n    日志类\n      异常日志诊断\n      GC日志诊断\n      slow log诊断\n      冷数据诊断\n      mapping合理性诊断\n```\n\n### 健康度\n\n- 红：至少有一个主分片没有分配\n- 黄：至少有一个副本没有分配\n- 绿：主副本分片全部正常分配\n\n索引健康：最差的分片的状态\n\n集群健康：最差的索引的状态\n\n分片没有被分配的原因：\n\n1. INDEX CREATE:创建索引导致。在索引的全部分片分配完成之前，会有短暂的red,不一定代表有问题\n2. CLUSTER RECOVER:集群重启阶段，会有这个问题\n3. INDEX_REOPEN:Open一个之前Close的索引，资源不足导致无法分配成功\n4. DANGLING INDEX IMPORTED:一个节点离开集群期间，有索引被删除。这个节点重新返回时，会导致Dangling的问题\n\n解决无法分配的整体思路就是让无法分配的分片能被分配，如看看是不是有节点离线，或者设置了错误的副本数、以及磁盘资源不足等问题\n\n## 缓存\n\nNode Query Cache:\n\n该节点所有 shard 共享，只缓存 filter context 的数据，使用 lru 算法，在 segement 被合并时失效\n\nShard Query Cache:\n\n使用整个 JSON 查询串 作为 key，缓存每个分片的查询结果，只会缓存设置了 size = 0 的聚合查询与 suggestions 的查询结果，分片 refresh 时失效\n\nFielddata Cache:\n\n缓存的是字段数据，特别是用于排序、聚合和脚本的字段数据。它通常用于 text 字段在执行这些操作时的内存中数据表示，在 segement 被合并时失效\n\n## 索引管理\n\n- open/close：关闭索引后，除了磁盘外，集群的相关开销基本为0，无法被搜索和读取，需要时可以重新 open\n- shrink：会使用和源索引的相同配置创建一个新索引，仅仅降低主分片数\n- split：可以扩大主分片个数\n- rollover：该机制允许在满足特定条件时，自动创建新的索引，并将新数据写入新索引中，从而使得单个索引不会变得过大或难以管理\n\n### 生命周期管理\n\nhot -> warm -> cold -> delete\n\n- hot:索引还存在着大量的读写操作\n- warm:索引不存在写操作，还有被查询的需要\n- cold:数据不存在写操作，读操作也不多\n- delete:索引不再需要，可以被安全删除\n\nindex lifecycle management：\n\n```json\nPUT _ilm/policy/my_policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\": {\n            \"max_size\": \"50gb\",\n            \"max_age\": \"7d\"\n          }\n        }\n      },\n      \"warm\": {\n        \"min_age\": \"7d\",\n        \"actions\": {\n          \"shrink\": {\n            \"number_of_shards\": 1\n          },\n          \"allocate\": {\n            \"number_of_replicas\": 1\n          }\n        }\n      },\n      \"cold\": {\n        \"min_age\": \"30d\",\n        \"actions\": {\n          \"freeze\": {}\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"90d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n\n```\n\n## es操作过程\n\n### 写过程\n\n客户端选择一个协调节点（coordinating node）发送请求，协调节点将请求转发给对应的node\n对应的node在primary shard上处理请求，并同步到replica shard上\n\n![批注 2020-03-19 082208](/assets/批注%202020-03-19%20082208.png)\n\n#### 写过程原理\n\n![批注 2020-03-19 083304](/assets/批注%202020-03-19%20083304.png)\n\n1. 写入缓存：数据首先被写入内存缓冲区（buffer）和 translog 日志文件(page cache)，这时索引尚未生成 Segment，且缓存中的数据不可被查询\n2. refresh 操作：将数据从内存缓冲区写入操作系统缓存（os cache），生成一个Segment文件并清空缓冲区。同时建立倒排索引，使数据可以被客户端访问。默认每秒执行一次refresh操作。当缓冲区占满时，也会触发refresh操作\n3. translog 刷盘：在写缓存的时候，为了避免数据丢失，还会写入到translog中，translog会定时刷盘\n4. flush 操作：每隔一段时间或数据量达到一定值会将os cache中的数据以Segment文件形式持久化到磁盘\n5. merge 操作：随着时间推移，磁盘上的Segment数量增加，需要定期进行合并\n\n### 读过程\n\n客户端选择一个协调节点（coordinating node）发送根据ID查询请求，协调节点会根据id进行哈希，得到doc所在的分片，将请求转发到对应的node\n\n这个node然后会在primary shard与replica中使用随机轮询，进行负载均衡，返回document给协调节点\n\n协调节点再把document返回给客户端\n\n### 搜索过程\n\n客户端发送搜索请求给协调节点，协调节点将这个请求发送给所有的shard\n\n每个shard将自己的搜索结果返回给协调节点\n\n由协调节点进行数据的合并、排序、分页等操作，产出最终结果\n\n接着协调节点根据id再去查询对应的document的数据，返回给客户端\n\n### 删除/更新过程\n\n删除操作，会生成一个对应document id的.del文件，标识这个document被删除\n\n如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据\n\n每refresh一次，会生成一个segment file，系统会定期合并这些文件，合并这些文件的时候，会物理删除标记.del的document\n\n## 性能优化\n\n### 写入性能\n\n增大写吞吐量：\n\n- 客户端：多线程、批量写\n- 服务端：减少 IO、降低 CPU 与存储开销、分片分布均衡、减少线程调度开销\n\n关闭无关的功能：\n\n如不需要算分，norms 可以设置为 false，关闭 dynamic mapping、关闭 _source 操作减少 IO 操作，不需要搜索 index 设置成 false 等\n\n牺牲可靠性：\n\n减少副本数量为 0，只写主分片，可以减少 IO\n\n牺牲搜索实时性：\n\n增加写入时的 refresh 间隔，避免每次写入都刷缓存到 os cache\n\n牺牲可靠性：\n\n避免同步地、每次写请求去写 translog\n\n```json\nPUT myindex\n{\n  \"settings\": {\n    \"index\": {\n      \"refresh_interval\": \"30s\",\n      \"number_of_shards\": \"2\"\n    },\n    \"routing\": {\n      \"allocation\": {\n        \"total_shards_per_node\": \"3\"\n      }\n    },\n    \"translog\": {\n      \"sync_interval\": \"30s\",\n      \"durability\": \"async\"\n    },\n    \"number_of_replicas\": 0\n  },\n  \"mappings\": {\n    \"dynamic\": false,\n    \"properties\": {}\n  }\n}\n```\n\n### 读取性能\n\n#### 杀手锏：filesystem cache\n\n![批注 2020-03-19 085001](/assets/批注%202020-03-19%20085001.png)\n\n在es中，doc的字段尽量只存储要被搜索的字段，这样可以节省内存，存放更多数据，做缓存效果更好\n\n#### 数据预热\n\n对于一些热点数据，也要通过一些方式让它在缓存中\n\n#### 冷热分离\n\n保证热点数据都在缓存里，提高系统性能\n\n#### 数据建模\n\n对于一些复杂的关联，最好在应用层面就做好，对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的\n\n避免 script、避免通配符查询，使用 filter，减少不必要的算分\n\n#### 聚合性能优化\n\n利用 filter 减少聚合的数据量\n\n#### 分片优化\n\n一个查询需要访问所有分片，过多的分片会增加开销\n\n#### 分页性能优化\n\n由于分页操作是由协调节点来完成的，所以翻页越深，性能越差\n解决：\n\n- 不允许深度翻页\n- 将翻页设计成不允许跳页，只能一页一页翻\n\n### 合并性能优化\n\n1. 通过提高 refresh 来减少产生的分段数量\n2. 通过 segements_per_tier 、max_merged_segement 降低最大分段的大小，提升合并性能\n3. 使用 fore merge 降低分段数量，这个操作很耗费性能，应在没有写入、资源低峰期操作，理想情况下合成 1 个分段最好\n\n## kibana\n\nKibana是一个基于Node.js的Elasticsearch索引库数据统计工具，可以利用Elasticsearch的聚合功能，生成各种图表，如柱形图，线状图，饼图等。\n\n而且还提供了操作Elasticsearch索引数据的控制台，并且提供了一定的API提示，非常有利于我们学习Elasticsearch的语法。\n\n- docker\n\n```shell\ndocker pull kibana:5.6.8 # 拉取镜像\ndocker run -d --name kibana --net somenetwork -p 5601:5601 kibana:5.6.8 # 启动\n```\n","metadata":"tags: ['中间件', 'es']","hasMoreCommit":true,"totalCommits":24,"commitList":[{"date":"2024-12-12T15:26:38+08:00","author":"MY","message":"📦ES","hash":"28b974130d8b4a8ae778d39e061492a9ceeaf871"},{"date":"2024-11-20T19:43:10+08:00","author":"MY","message":"✏ES","hash":"270b0046fd04c188311d1ec73be1cd3b76a9c403"},{"date":"2024-07-17T15:54:07+08:00","author":"MY","message":"✏ES","hash":"06cfd00bffe6d7d4c91c3aad727c982645d3aeea"},{"date":"2024-07-10T20:07:07+08:00","author":"MY","message":"✏ES","hash":"530693ddb6a170ef95e83e016a78b99c6a3c2afb"},{"date":"2024-07-09T20:10:40+08:00","author":"MY","message":"✏ES","hash":"5ca1fa41444725bf69b3a526fb578f57a2110978"},{"date":"2024-07-08T20:12:06+08:00","author":"MY","message":"✏ES","hash":"1d37eaff55d1d757c29cddf43495f567cfeaa82e"},{"date":"2024-07-08T17:04:51+08:00","author":"MY","message":"✏ES","hash":"073af9b7436f29f48782ac072e87c0cd99887823"},{"date":"2024-07-04T16:10:57+08:00","author":"MY","message":"✏ES","hash":"e954c3ec638854e03ec500320021f9e573e998eb"},{"date":"2024-07-03T19:45:04+08:00","author":"MY","message":"✏ES","hash":"cc13bf605acd71bf859a8f43bbd0b468fd38ac73"},{"date":"2024-07-02T19:02:55+08:00","author":"MY","message":"✏ES","hash":"7afbf581c1077070bdef293d73cccbe11550dfe0"}],"createTime":"2019-09-03T22:29:51+08:00"}