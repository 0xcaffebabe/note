{"name":"哨兵","id":"中间件-数据库-redis-哨兵","content":"# 哨兵\n\n- 集群监控：负责监控 redis master 和 slave 进程是否正常工作。\n- 消息通知：Sentinel节点会将故障转移的结果通知给应用方。\n- 故障转移：如果 master node 挂掉了，sentinel会自动选出一个新的redis master node\n- 配置中心：客户端在初始化的时候连接的是Sentinel节点集合，从中获取主节点 信息如果故障转移发生了，通知 client 客户端新的 master 地址\n\n哨兵至少需要 3 个实例，来保证自己的健壮性\n\n当大多数Sentinel节点都认为主节点不可达时，它们会选举出一个Sentinel节点来完成自动故障转移的工作，同时会将这个变化实时通知给Redis应用方\n\n哨兵(sentinel) 是一个分布式系统,你可以在一个架构中运行多个哨兵(sentinel) 进程,这些进程使用流言协议(gossip protocols)来接收关于Master是否下线的信息,并使用投票协议(agreement protocols)来决定是否执行自动故障迁移,以及选择哪个Slave作为新的Master\n\n```mermaid\nstateDiagram-v2\n  state SentinelCluster {\n    sentinel1 --> sentinel2: 监控\n    sentinel2 --> sentinel1: 监控\n    sentinel2 --> sentinel3: 监控\n    sentinel3 --> sentinel2: 监控\n    sentinel3 --> sentinel1: 监控\n    sentinel1 --> sentinel3: 监控\n  }\n  master --> slave1: 复制\n  master --> slave2: 复制\n  SentinelCluster --> master: 监控\n  SentinelCluster --> slave1: 监控\n  SentinelCluster --> slave2: 监控\n```\n\n高可用读写分离：\n\n```mermaid\nstateDiagram-v2\n  state SentinelCluster {\n    sentinel1\n    sentinel2\n    sentinel3\n  }\n  state SlaveCluster {\n    slave1\n    slave2\n  }\n  master --> slave1: 复制\n  master --> slave2: 复制\n  SentinelCluster --> master: 监控\n  SentinelCluster --> slave1: 监控\n  SentinelCluster --> slave2: 监控\n  client1 --> SlaveCluster\n  client2 --> SlaveCluster\n  client3 --> SlaveCluster\n```\n\n## 原理\n\n三个定时任务：\n\n- 每隔10秒，每个Sentinel节点会向主节点和从节点发送info命令获取最新的拓扑结构\n- 每隔2秒，每个Sentinel节点会向Redis数据节点的__sentinel__：hello频道上发送该Sentinel节点对于主节点的判断以及当前Sentinel节点的信息 每个Sentinel节点也会订阅该频道\n- 每隔1秒，每个Sentinel节点会向主节点、从节点、其余Sentinel节点\n发送一条ping命令做一次心跳检测\n\n如果在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上，就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主库\n\nsdown与odown：\n\n- sdown：主观宕机，某一哨兵发现无法连接master\n- odown，一定数量的哨兵发现无法连接master，这个数量由管理员配置\n\nsentinel领导节点选举：通过[raft算法](/软件工程/架构/系统设计/分布式/分布式共识算法.md#Raft)选举出领导节点， 由sentienl领导节点进行故障转移的操作\n\n### 故障转移\n\n选择一个新主节点：\n\n```mermaid\ngraph TB\n  从节点列表 --> 过滤掉不在线或者网络状况不好的\n  过滤掉不在线或者网络状况不好的 --> A{slave-priority最大的}\n  A --> |yes| 选择完毕\n  A --> |no| B{继续选择}\n  B --> 复制偏移量最大的节点\n  复制偏移量最大的节点 --> |yes| 选择完毕\n  复制偏移量最大的节点 --> |no| runId最大的节点\n```\n\n- Sentinel领导者节点会对第一步选出来的从节点执行slaveof no one命\n令让其成为主节点\n- Sentinel领导者节点会向剩余的从节点发送命令，让它们成为新主节点的从节点\n- Sentinel节点集合会将原来的主节点更新为从节点，并保持着对其关注，当其恢复后命令它去复制新的主节点\n\n### 消息发送\n\n哨兵集群的自动发现通过 pub/sub 系统实现的，每个哨兵都会往 `__sentinel__`:hello 这个 channel 里发送一个消息，内容是自己的 host、ip 和 runid 还有对这个 master 的监控配置,这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在\n\n在哨兵内部，sentinel 内部不同的事件会创建不同的 pub/sub 频道来进行消息的同步\n\n```c\n// 通过该方法发送消息\nvoid sentinelEvent(int level, char *type, sentinelRedisInstance *ri, const char *fmt, ...);\n\nif (level != LL_DEBUG) {\n    channel = createStringObject(type,strlen(type));\n    payload = createStringObject(msg,strlen(msg));\n    pubsubPublishMessage(channel,payload,0);\n    decrRefCount(channel);\n    decrRefCount(payload);\n}\n\n// 调用\nsentinelEvent(LL_WARNING,\"+monitor\",ri,\"%@ quorum %d\",ri->quorum);\n```\n\n频道名称|含义\n-|-\n+sdown|哨兵判断主节点主观下线\n+odown|哨兵判断主节点客观下线\n+new-epoch|当前的纪元被更新，进入新的纪元\n+try-failover|达到故障切换的条件，开始故障切换\n+failover-state-select-slave|开始要选一个从节点作为主节点\n+selected-slave|找到一个合适的从节点作为新的主节点\n+failover-end|故障切换成功完成\n+switch-master|主节点发生切换，主节点的信息发生替换\n\n## 数据丢失\n\n- 主备切换时，master异步向salve同步的命令丢失导致数据丢失\n- 网络异常，导致master暂时失联，当master重新连接上网络时，变成了slave，数据丢失\n\n解决：拒绝客户端的写请求\n\n## 哨兵配置\n\n```config\n# sentinel.conf\nport 26379\nsentinel monitor mymaster 172.17.0.5 6379 2\n```\n\n这个配置代表需要监控127.0.0.1：6379这个主节点，2代表判断主节点失败至少需要2个Sentinel节点同意，mymaster是主节点的别名\n\n```sh\n# 启动哨兵\nredis-sentinel sentinel.conf\n```\n\n哨兵的一些配置项：\n\n```properties\n# 如果超过了down-after-milliseconds配置的时间且没有有效的回复，则判定节点不可达\nsentinel down-after-milliseconds <master-name> <times>\n# 用来限制在一次故障转移之后，每次向新的主节点发起复制操作的从节点个数\nsentinel parallel-syncs <master-name> <nums>\n# slaveof no one一直失败（例如该从节点此时出现故障），当此过程超过failover-timeout时，则故障转移失败\nsentinel failover-timeout <master-name> <times>\n# 主节点通信密码\nsentinel auth-pass <master-name> <password>\n# 当一些警告级别的Sentinel事件发生（指重要事件，例如-sdown：客观下线、-odown：主观下线）时，会触发对应路径的脚本\nsentinel notification-script <master-name> <script-path>\n# 故障转移结束后，会触发对应路径的脚本\nsentinel client-reconfig-script <master-name> <script-path>\n```\n\n动态调整配置：`sentinel set` 命令\n\n## 监控多个主节点\n\n```properties\nsentinel monitor master-business-1 10.10.xx.1 6379 2\nsentinel monitor master-business-2 10.16.xx.2 6380 2\n```\n\n```mermaid\nstateDiagram-v2\n  state SentinelCluster {\n    sentinel1\n    sentinel2\n    sentinel3\n  }\n  state RedisCluster1 {\n    master1 --> slave1\n    master1 --> slave2\n  }\n  state RedisCluster2 {\n    master2 --> slave3\n    master2 --> slave4\n  }\n  SentinelCluster --> RedisCluster1: 监控\n  SentinelCluster --> RedisCluster2: 监控\n```\n\n## 部署\n\n- Sentinel节点不应该部署在一台物理机器上\n- 部署至少三个且奇数个的Sentinel节点\n- 一套sentinel还是多套sentinel\n  - 一套Sentinel，很明显这种方案在一定程度上降低了维护成本 但如果这套Sentinel节点集合出现异常，可能会对多个Redis数据节点造成影响\n  - 多套Sentinel会造成资源浪费。但是优点也很明显，每套Redis Sentinel都是彼此隔离的\n\n## 运维\n\n节点下线：\n\n- 主节点下线 使用sentienl failover命令选出一个新主节点 将原来的主节点下线\n- 从节点或者sentienl节点下线 保证客户端能感受到从节点的变化 避免发送无效请求\n\n节点上线：\n\n- 添加从节点 添加slaveof配置启动即可\n- 添加sentienl节点 添加sentienl monitor配置启动即可\n\n## API\n\n```sh\n# 查看所有被监控的主节点状态以及相关的统计信息\nsentinel masters\n# 查看指定主节点\nsentinel master <master name>\n# 查看指定主节点的从节点\nsentinel slaves <master name>\n# 列出指定的主从集群sentinel节点（不包含本节点）\nsentinel sentinels <master name>\n# 返回指定的主节点地址和端口\nsentinel get-master-addr-by-name <master name>\n# 对符合<pattern>（通配符风格）主节点的配置进行重置\nsentinel reset <pattern>\n# 强制对集群进行故障转移\nsentinel failover <master name>\n# 检测当前可达的Sentinel节点总数是否达到<quorum>的个数\nsentinel ckquorum <master name>\n# 将Sentinel节点的配置强制刷到磁盘上\nsentinel flushconfig\n# 取消当前sentinel节点对指定master的监控那个\nsentinel remove <master name>\n# 监控指定master\nsentinel monitor <master name> <ip> <port> <quorum>\n# Sentinel节点之间用来交换对主节点是否下线的判断\nsentinel is-master-down-by-addr\n```\n\n## 客户端连接\n\n1）遍历Sentinel节点集合获取一个可用的Sentinel节点\n\n2）通过sentinel get-master-addr-by-name master-name这个API来获取对应主节点的相关信息\n\n3）验证当前获取的“主节点”是真正的主节点，这样做的是为了防止获取之后主节点又发生了变化\n\n4）保持和Sentinel节点集合的“联系”，时刻获取关于主节点的相关“信息”\n\nJava 客户端：\n\n```java\nJedisSentinelPool pool =\n    new JedisSentinelPool(\"mymaster\", Set.of(\"192.168.1.101:26379\",\"192.168.1.101:26380\",\"192.168.1.101:26381\"));\nJedis resource = pool.getResource();\nSystem.out.println(resource.ping());\nresource.close();\n```\n","metadata":"","hasMoreCommit":false,"totalCommits":3,"commitList":[{"date":"2023-12-11T19:59:23+08:00","author":"MY","message":"✏Redis","hash":"2f7e29a08817db6f3ce12f5fc78d44c23e8b1412"},{"date":"2023-11-16T18:57:34+08:00","author":"MY","message":"✏Redis","hash":"7955ae3a5874322931281a113be629782a29c57a"},{"date":"2020-11-05T16:53:44+08:00","author":"MY","message":"📦重构 Redis","hash":"436d256214f45aea37f8659d4758b82fe1709560"}],"createTime":"2020-11-05T16:53:44+08:00"}