{"name":"消费者","id":"中间件-消息队列-Kafka-消费者","content":"# 消费者\n\n```mermaid\nstateDiagram-v2\n  state 主题T1 {\n    分区0 分区1 分区2 分区3\n  }\n  state 消费者群组1 {\n    消费者1 消费者2 消费者3 消费者4\n  }\n  state 消费者群去2 {\n    消费者5 消费者6\n  }\n  分区0 --> 消费者1\n  分区1 --> 消费者2\n  分区2 --> 消费者3\n  分区3 --> 消费者4\n\n  分区0 --> 消费者5\n  分区1 --> 消费者5\n  分区2 --> 消费者6\n  分区3 --> 消费者6\n```\n\n分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为再均衡\n\n消费者通过向被指派为群组协调器的 broker（不同的群组可以有不同的协调器）发送心跳来维持它们和群组的从属关系以及它们对分区的所有权关系\n\n```java\nProperties props = new Properties();\n//kafka 集群，broker-list\nprops.put(\"bootstrap.servers\", \"172.24.211.140:9092\");\nprops.put(\"group.id\", \"consumer1\");\nprops.put(\"key.deserializer\",\n        \"org.apache.kafka.common.serialization.StringDeserializer\");\nprops.put(\"value.deserializer\",\n        \"org.apache.kafka.common.serialization.StringDeserializer\");\nKafkaConsumer<String, String> consumer = new KafkaConsumer<String,\n                        String>(props);\nconsumer.subscribe(List.of(\"test\"));\nwhile(true){\n    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));\n    for (ConsumerRecord<String, String> record : records) {\n        System.err.println(record);\n    }\n}\n```\n\n消费方式:\n\n采用 pull（拉）模式从 broker 中读取数据\n\n相较于服务器 push 模式，pull模式使消费者可以根据自己的情况来决定消费的速率，另一方面，pull模式每次也可以批量拉取数据，提高服务器吞吐量，而 push 模式一旦有消息产生，就会立马推送，这意味着网络开销会比pull模式高一些。\n\n但pull模式也有一些缺陷，如果没有消息的话，客户端就会在那里空转等待，所以Kafka为消费者的poll方法指定了一个时间参数，避免空转浪费CPU资源\n\n## 配置\n\n- fetch.min.bytes\n  - 指定了消费者从服务器获取记录的最小字节数\n- fetch.max.wait.ms\n  - 指定 broker 的等待时间\n- max.partition.fetch.bytes\n   - 指定了服务器从每个分区里返回给消费者的最大字节数\n- session.timeout.ms\n  - 指定了消费者在被认为死亡之前可以与服务器断开连接的时间\n- heartbeat.interval.ms 控制发送心跳请求频率的参数\n- auto.offset.reset\n  - 指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下（因消费者长时间失效，包含偏移量的记录已经过时并被删除）该作何处理\n  - earliest 当分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费\n  - latest 当分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据\n  - none 当该topic下所有分区中存在未提交的offset时，抛出异常\n- enable.auto.commit\n  - 指定了消费者是否自动提交偏移量\n- partition.assignment.strategy 决定哪些分区应该被分配给哪个消费者\n  - range:该策略会把主题的若干个连续的分区分配给消费者\n  - 轮询：该策略把主题的所有分区逐个分配给消费者\n- client.id\n- max.poll.records 控制单次调用 poll() 方法能够返回的记录数量\n- max.poll.interval.ms 用于指定consumer两次poll的最大时间间隔（默认5分钟），如果超过了该间隔consumer client会主动向coordinator发起LeaveGroup请求，触发rebalance\n-  receive.buffer.bytes 和 send.buffer.bytes\n   -  读写数据时用到的 TCP 缓冲区也可以设置大小\n\n## 偏移量\n\n每一个消费者组都会对有进行消费的主题分区的偏移量进行记录，这也就意味着消费者可以手动设置这个偏移量从头消费数据。\n\n更新分区当前偏移量的操作叫作**提交**\n\nKafka 0.9 版本之前，consumer 默认将 offset 保存在 Zookeeper 中，从 0.9 版本开始，consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中，该 topic为__consumer_offsets。\n\n__consumer_offsets的key为（主题、分区号、消费组），value 为（版本号、偏移量、元数据、提交时间戳、leader epoch、过期时间戳），随着不断提交偏移量，这个主题的消息会越来越多，所以会有一个线程定时对这个主题 compact，因为只需要保留一个最新的偏移量即可\n\n__consumer_offsets 位移主题中的每个消费者组只能被一个 Coordinator 所管理，这个 Coordinator 通过使用 grouId hashCode 取余决定由哪台 broker 来进行管理\n\n消费者获取偏移量不是直接消费 __consumer_offsets 位移主题，而是 Coordinator 会异步地去消费这个位移主题的数据，填充到元数据缓存中，对外提供查询的是元数据缓存\n\n**自动提交：**\n\n- 如果 enable.auto.commit 被设为 true ，那么每过 5s，消费者会自动把从 poll() 方法接收到的最大偏移量提交上去\n\n**手动提交：**\n\n把 auto.commit.offset 设为 false ，使用 commitSync()\n\n异步提交 commitAsync() , 但该方法在发生错误时不会进行重试\n\n消费者提交偏移量，都是向 coordinator 所在的 broker 提交，coordinator 专门为 消费组服务，负责执行再平衡以及提供位移管理和组成员管理等\n\n要确定 coordinator，首先确定由位移主题的哪个分区来保存该消费组的数据：partitionId=Math.abs(groupId.hashCode() % offsetsTopicPartitionCount)。接下来找出该分区 leader 副本所在的 broker，该 broker 即为对应的 coordinator\n\n**再均衡监听：**\n\n订阅的时候传入 ConsumerRebalanceListener 实现相关接口\n\n**从特定偏移量开始处理：**`seekToBeginning(..)`\n\n**读取特定偏移量：**`seek(..)`\n\n### Kafka 消费异常没有手动提交导致的问题\n\n假设本次偏移量消费到28 抛出异常 没有手动提交偏移量\n\n该消费者下次迭代也会从29开始消费 为解决这个问题 所以只能新起消费者来替代此消费者\n\n### CommitFailedException\n\n- 当消费时长超过 max.poll.interval.ms 时，会抛出这个异常\n- 设置相同 group.id 值的消费者组程序和独立消费者程序，也会出现这个异常\n\n### 重设偏移量\n\n- Earliest:把位移调整到当前最早位移处。\n- Latest:把位移调整到当前最新位移处。\n- Current:把位移调整到当前最新提交位移处。\n- Specified-Offset:把位移调整成指定位移。\n- Shift-By-N：把位移调整到当前位移+N处。\n- DateTime:把位移调整到大于给定时间的最小位移处。\n- Duration:把位移调整到距离当前时间指定间隔的位移处。\n\n## 再平衡\n\n如果未能及时发送心跳或者消费时间过长，消费者都会被认为已经死了，此时消费者数量会发生变化\n\n当消费者数量、订阅的主题数或者分区数发生变更时，就会触发消费组的再平衡操作，即重新分配消费者与分区的映射关系，再平衡期间，所有消费都会停止，消费者越多这个过程越慢，停止消费的时间就越长\n\n0.11 之后的版本推出了 StickyAssignor，即有粘性的分区分配策略，使用这个策略会尽可能地保留之前的分配方案，尽量实现分区分配的最小变动\n\n消费者组状态机：\n\n- Empty：组内没有任何成员，但消费者组可能存在已提交的位移数据，而且这些位移尚未过期。\n- Dead：同样是组内没有任何成员，但组的元数据信息已经在协调者端被移除。协调者组件保存着当前向它注册过的所有组信息，所谓的元数据信息就类似于这个注册信息\n- PreparingRebalance：消费者组准备开启重平衡，此时所有成员都要重新请求加入消费者组。\n- CompletingRebalance：消费者组下所有成员已经加入，各个成员正在等待分配方案。该状态在老一点的版本中被称为AwaitingSync,它和CompletingRebalance.是等价的。\n- Stable：消费者组的稳定状态。该状态表明重平衡已经完成，组内各成员能够正常消费数据了。\n\n```mermaid\nstateDiagram-v2\n  Empty --> Dead: 组信息过期被删除\n  Empty --> PreparingRebalance: 准备开启rebalance\n  PreparingRebalance --> Empty: 组内所有成员离组\n  PreparingRebalance --> Dead: 位移主题分区leader发生变更\n  PreparingRebalance --> CompletingRebalance: 有成员入组\n  CompletingRebalance --> PreparingRebalance: 有成员加入或离开\n  CompletingRebalance --> Stable: leader完成分配\n  Stable --> PreparingRebalance: 心跳过期/成员离组/新成员加入\n  Stable --> Dead: 位移主题分区leader发生变更\n```\n\n```mermaid\nsequenceDiagram\n  消费领导者 ->> coordinator: joingroup(订阅主题)\n  消费者1 ->> coordinator: joingroup(订阅主题)\n  消费者2 ->> coordinator: joingroup(订阅主题)\n  coordinator -->> 消费领导者: 当前组成员(1,2)\n  coordinator -->> 消费者1: 消费领导者为0\n  coordinator -->> 消费者2: 消费领导者为0\n  消费领导者 ->> 消费领导者: 制订消费方案\n  消费领导者 ->> coordinator: 提交消费方案\n  coordinator -->> 消费者1: 下发消费方案\n  coordinator -->> 消费者2: 下发消费方案\n```\n\n### 分区分配策略\n\n1. Range：根据消费者的数量将分区的范围分配给消费者。具体来说，Range 会将分区划分成若干块，每个消费者负责一个或多个连续的分区。\n2. RoundRobin：将分区轮流分配给每个消费者，使得分配更加均衡。它并不考虑分区的顺序或连续性\n3. Sticky：2.4 版本新增的分区分配策略。该策略尽量保持消费者原有的分区分配不变，以避免分区在消费者之间频繁地重新分配\n4. Custom：客户端可以实现 ConsumerPartitionAssignor 来更灵活地设计分配策略，这也是为什么消费分配策略设计在客户端的原因\n\n## 退出\n\n其他线程调用`consumer.wakeup()` 会使consumer在poll抛出异常 然后进行close即可\n\n## 没有群组的消费者\n\n调用assign为其设置消费的分区\n\n## 消费进度监控\n\n```\n(start)----lead-----(consume offset)---------lag--------(end)--->\n```\n\n- lag：最新一条数据与当前消费到的数据的偏移量差值，越大代表积压越严重\n- lead：指消费者最新消费消息的位移与分区当前第一条消息位移的差值，越小代表越有丢数据的风险\n\n","metadata":"","hasMoreCommit":true,"totalCommits":12,"commitList":[{"date":"2024-12-31T15:36:59+08:00","author":"MY","message":"📦Kafka","hash":"ea9109746bc43e320441a94e7a4866b66d6cb339"},{"date":"2024-12-13T15:26:31+08:00","author":"MY","message":"📦Kafka","hash":"669ee24fcbc7d1f943da5f42d773e29c2990942d"},{"date":"2024-11-20T16:12:20+08:00","author":"MY","message":"📦Kafka","hash":"955524f9775b97e250675e61652f914ad8337055"},{"date":"2023-12-25T19:31:55+08:00","author":"MY","message":"✏Kafka","hash":"4e13dddf9d95c59d8a08a051a769024b32e32c34"},{"date":"2023-12-01T17:20:45+08:00","author":"MY","message":"✏Kafka","hash":"330072b3e64e9c310102137594ce75ba60bf27f8"},{"date":"2023-11-30T19:20:43+08:00","author":"MY","message":"✏Kafka","hash":"d70587ba4f48e6abec74bc770cced58464e1f599"},{"date":"2023-11-29T19:21:03+08:00","author":"MY","message":"✏Kafka","hash":"44c479880fa9eb76f4dd26a33df88f7ae230a50f"},{"date":"2023-11-28T19:43:58+08:00","author":"MY","message":"✏Kafka","hash":"1659408db251662415e754150f16c570cec4ef7d"},{"date":"2022-04-19T11:31:47+08:00","author":"cjiping","message":"📦整理 问题记录","hash":"68080b22aa70385e3941d8bd0502d83fb67312c5"},{"date":"2021-05-13T10:44:15+08:00","author":"cjiping","message":"✏更新 Kafka消费者 auto.offset.reset","hash":"5abd0542bbd9ece398a6ee7ff73badad23560727"}],"createTime":"2021-03-23T16:48:37+08:00"}