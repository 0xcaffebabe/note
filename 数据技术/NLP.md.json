{"name":"NLP","id":"数据技术-NLP","content":"# NLP\n\n## AI基础\n\n### 预测性建模\n\n在给定数据的基础上提升预测模型的准确率，只关注模型的预测准确性\n\n而统计学模型主要关注模型的可解释性\n\n### 优化问题\n\n指如何根据已有的数学模型求出最优解的过程\n\n### 增强学习\n\n如何在动态环境下做出正确的决定，游戏AI、推荐、排名\n\n### 其他分类\n\n- 结构化数据\n- 非结构化数据：文本数据、语音数据、图像数据\n\nAI集中在对非结构化数据的处理上，因为非结构化数据很难处理\n\n### 项目流程\n\n一般流程：调研方案 -> 数据标注与开发 -> 调优 -> 部署\n\n判断是否要做一个 AI 项目：\n\n1. 技术成熟度\n   1. 人工无法解决的问题，机器更不可能解决\n   2. 论文中的技术能否复现，这项技术在第一梯队厂商的成熟度如何\n   3. 初期通过小 Demo 验证\n   4. 团队的资源与能力\n   5. 项目部署问题：硬件、环境\n   6. 交付时间够不够\n2. 需求的可控程度\n   1. 技术导向还是销售导向意味着需求是否可控\n   2. 客户的管理能力意味着需求的质量\n   3. 团队的管理能力\n3. 项目的周期与成本\n   1. 被低估\n   2. 需求变更\n   3. 其他问题\n      1. 标注不可控\n      2. 调优所需的成本\n      3. 部署所需的成本\n      4. 模型的算力成本\n4. 项目交付流程\n   1. 是否有明确的目标\n   2. 不要忽略额外投入\n   3. 项目交付能力：交付流程、人员职责安排、时间规范、烂尾风险\n\n#### 调研\n\n- 很多时候，学术结果难以复现。\n- 很多方法在某些数据上可能会非常好用，但是在另一些数据上则会失效。\n- 很多方法的成功取决于一些细节，而这些细节只有真正做过的人才会知道。\n- 很多时候人们会过于关注方法的效果，而忽略了整体的运行实效。\n- 在绝大多数的时候，人们都会低估整个项目的难度\n\n#### 数据标注\n\n- 前期一定要制定充分的标注规则\n- 数据的采集一定要具有代表性\n- 非常不建议采用自动标注的方式\n- 先训练一个初步模型，然后只让相关人员进行校对，可以保证标注效率并减少标注成本\n\n#### 算法开发\n\n- 不要采用规则的方式进行开发：规则越加越多只会导致成本爆炸\n- 初期就要引导客户使用和购买能够支持深度学习框架的硬件\n- 算法开发的过程中，一定要有量化的指标并记录下来\n- 开发的过程中，多分解问题\n- 前端对接的时候一定要去引导何为“智能”\n\n#### 效果优化\n\n- 初期要充分考虑到效果优化所需要的时间和成本\n- 客户并不知道通过什么标准来评估一个系统的好坏\n- 一定要从数据的角度出发进行优化\n- 学会止损\n- 除了准确性的优化，还要注重代码运算效率的优化\n- 算法开发和效果优化常常是需要反复进行的工作\n\n#### 部署\n\n- 如果客户的系统比较奇怪，或者难以满足一些要求，要提前让客户知晓这些风险。\n- 即使再小的项目，强烈建议用微服务架构、Docker、k8s进行部署。\n- 因为技术、环境等问题，不要把算法部署在本地，尽量采用云端部署\n\n## NLP基础\n\n基础性研究：网络架构、优化理论、对抗训练、数据增强、半监督学习、域迁移、Meta Learning、Auto ML、多任务模型、集成学习、图网络、知识图谱、多模态学习、机器推理\n\nNLP研究：\n\n- 预训练语言模型\n- 文本分类\n- 序列标注：给词打标\n- 关系提取\n- Dependency Parsing\n- Semantic Parsing\n- Seq2Seq\n- 文本生成：GPT\n- 文本推荐\n- 翻译\n- 指代消解\n\n## 深度学习框架\n\n包括什么：\n\n- 以 GPU 为基础的 Tensor 运算（矩阵运算）\n- 构建网络后自动求解梯度的方法\n- 模型训练体系，把数据从处理到优化再到评估\n- 模型推断体系，模型的使用、部署\n\n### 硬件\n\n#### CPU\n\n- 一般不用 CPU 训练深度学习模型。\n- 很多 if…else 出现时，CPU 会比 GPU 快。\n- 如果需要加速，可以通过 Cython 访问 C++，这在实际业务性应用时很有用。\n- 对于大部分硬件（GPU，TPU，FPGA），CPU会负责数据的读写 -> 在进行训练时，有时为了加速需要选择更多核的机器（cache miss 导致的速度不匹配）\n\n#### CPU\n\n英伟达的：\n\n- geforce系列\n- Pxxx系列\n\n特点：\n\n- SIMT\n\n注意事项：\n\n- GPU 训练效率可以被 DeepSpeed 显著提升。\n- 很少出现 GPU 多线程训练。\n- GPU 训练有时可能会被一些其他因素影响，如CPU，GPU 之间沟通速度（多\nGPU或多节点）。\n- 传统来说，NLP 的训练一般来说不会耗尽 GPU的资源，但是深度迁移模型出现后，GPU 常常出现算力不足或显存资源不足的情况。\n- GPU 可处理动态的网络。\n- 显存污染问题\n- 部署经常被内存与显存之间的带宽影响\n- 部署时需要对参数做详细调整，最重要参数为 Batch Size\n\n#### TPU\n\n- 用于训练神经网络的 TPU 只能通过 GCP 获得\n- TPU 本质上来说是矩阵/向量相乘的机器，构造远比 GPU 简单，所以：\n  - TPU 十分便宜\n  - TPU 很容易预测其表现\n- TPU 很擅长基于 Transformer 架构的训练和预测\n- TPU 不能处理动态网络\n- 原生 Tensorflow 对 TPU 支持最好，PyTorch 目前通过 XLA 的引进也部分支持 TPU。\n- TPU 的主要运算瓶颈在于 IO 支持。\n- TPU V3 多线程的方式性价比最高\n\n### 部署\n\n难点：\n\n- AI 项目整体结构复杂，模块繁多。\n- AI 很多时候需要大量的算力，需要使用 GPU，TPU 或者 FPGA。\n- 深度学习框架依赖十分复杂，配置环境困难\n\n原则：\n\n- 部署隔离、稳定\n- 采用合适硬件，注意 CPU 选型和 GPU 选型\n- 以 Profiler 为导向进行优化\n- 推断服务应该用同一个框架和一个线程，TPU 除外\n- 部署应该是在项目初期就考虑\n\n推断框架：\n\n- 读取模型，提供 HTTP 接口\n- 调用不同的硬件资源\n- 对推断过程做一定处理，其中最重要的是批处理\n\n**TF Serving**\n\n## 随机性\n\n- 随机性的来源\n  - 数据的噪声：无法消除\n  - 函数的拟合：可提升\n    - 过拟合与欠拟合是对神经网络设计和训练很重要的一点\n\n能否解决问题在很大程度上取决于数据是否有足够信息：需要引入更多的信息来预测 -> 增加更多的维度\n\n## 神经网络\n\n数学本质：复合函数，可以很方便地组合出很多复杂的函数\n\n由于复合函数求导法则，所以大部分神经网络的训练过程可以自动化（反向梯度传递）\n\n优化算法：\n\n- SGD\n- SGD with Momentum\n- Adagrad\n- Adam\n\n由于求解过程的复杂性，这使得神经网络的求解并不一定会收敛到最优解，设计与优化算法需要考虑解决梯度消失跟梯度爆炸\n\n### 基础构成\n\n损失函数：来评价模型的预测值和真实值不一样的程度\n\n$$\nL_2(\\hat{y},y) = (\\hat{y} - y)^2\\\\\nL_1(\\hat{y},y) = |\\hat{y} - y|\n$$\n\n激活函数：\n\nDropout：\n\nBatch Normalization：\n\n## Embedding\n\n- 编码文本特征\n\n## RNN\n\n- 特征提取\n\n","metadata":"","hasMoreCommit":false,"totalCommits":3,"commitList":[{"date":"2023-02-06T16:19:11+08:00","author":"cjiping","message":"✏️NLP","hash":"83258f19f7bf3a2ca0dde18c6cfe3ff05fa40cd8"},{"date":"2023-01-31T17:57:52+08:00","author":"cjiping","message":"✏️NLP","hash":"3cca42d47029bff5841aa1a255cf8f714f561465"},{"date":"2023-01-30T18:05:39+08:00","author":"cjiping","message":"➕NLP","hash":"7d2ecf0034633f2c2367563437f5d9dad166fe36"}],"createTime":"2023-01-30T18:05:39+08:00"}