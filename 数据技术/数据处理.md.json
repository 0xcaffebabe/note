{"name":"数据处理","id":"数据技术-数据处理","content":"# 数据处理\n\n在数据处理阶段，两大挑战分别是如何保证数据质量和如何降低运维成本\n\n- 满足 ETL 的幂等性：保证 ETL 任务执行即使多次结果仍是准确的，可以保证数据质量及降低运维成本\n- 日志分级分类解耦，一站式查看：数据处理涉及的组件很多，所以需要能在一个统一的地方查看日志方便运维管理\n- 主动识别数据质量问题并预警\n- ETL 可观测\n- 组件低成本替换：避免对于组件的过度依赖以防止 ETL 的部署迁移或组件替换时，由于组件的高度耦合导致成本变高\n- 可配置：尽可能减少对于常规数据处理的代码重复开发\n\n## 批处理\n\n- 输入数据是有界且不可变的\n- 除了输出 其他操作都没有副作用\n\n#### UNIX的管道\n\n#### [MapReduce](/数据技术/Hadoop.md#MapReduce)的不足\n\n- 抽象层次不足，太原始\n- 维护成本：每一步的 MapReduce 都有可能出错，为了这些异常处理，就需要协调系统，协调系统又是一个复杂度的来源\n- 时间性能：对 MapReduce 的配置细节不理解，难以发挥其高性能，每一步计算都要进行硬盘的读取和写入\n- 只支持批处理\n\n需要的：\n\n1. 一种技术抽象让多步骤数据处理变得易于维护\n2. 不要复杂的配置，需要能自动进行性能优化\n3. 要能把数据处理的描述语言，与背后的运行引擎解耦合开来\n4. 要统一批处理和流处理的编程模型\n5. 要在架构层面提供异常处理和数据监控的能力\n\n```mermaid\nstateDiagram-v2\n  state 前端/用户端 {\n    统一批处理/流处理的的描述语法 --> 有向图编译系统\n  }\n  有向图编译系统 --> 后端/计算引擎: DAG\n  state 后端/计算引擎 {\n    基于有向图的优化系统\n    计算资源自动分配系统\n    自动监控和错误追踪\n  }\n```\n\n#### 分布式批处理需要解决的问题\n\n1. 如何将输入数据分区\n2. 容错：任务可能随时会失败\n\n## 模式\n\n### Workflow\n\n```mermaid\n---\ntitle: 复制模式\n---\nflowchart\n  数据集 --> 复制器\n  复制器 --> 工作流1\n  复制器 --> 工作流2\n  复制器 --> 工作流3\n```\n\n```mermaid\n---\ntitle: 过滤模式\n---\nflowchart\n  数据集(1,2,3) --> 过滤器\n  过滤器 --> 工作流(1,2)\n```\n\n```mermaid\n---\ntitle: 分离模式\n---\nflowchart\n  数据集(1,2,3) --> 分离器\n  分离器 --> 工作流1(1,2)\n  分离器 --> 工作流2(2,3)\n```\n\n```mermaid\n---\ntitle: 合并模式\n---\nflowchart\n  数据集1(1,2) --> 合并器\n  数据集2(2,3) --> 合并器\n  合并器 --> 工作流(1,2,3)\n```\n\n### [发布订阅](/软件工程/设计模式/行为模式.md#观察者)\n\n### DAG\n\n```mermaid\n---\ntitle: 单任务 DAG\n---\nstateDiagram\n    [*] --> Stage1Task1\n    [*] --> Stage1Task2\n    [*] --> Stage1Task3\n    [*] --> Stage1Task4\n\n    Stage1Task1 --> Stage2Task1\n    Stage1Task1 --> Stage2Task2\n    Stage1Task2 --> Stage2Task1\n    Stage1Task2 --> Stage2Task2\n    Stage1Task3 --> Stage2Task3\n    Stage1Task3 --> Stage2Task4\n    Stage1Task4 --> Stage2Task3\n    Stage1Task4 --> Stage2Task4\n\n    Stage2Task1 --> Stage3Task1\n    Stage2Task2 --> Stage3Task1\n    Stage2Task3 --> Stage3Task1\n    Stage2Task4 --> Stage3Task1\n\n    state \"Stage 1\" as Stage1 {\n        Stage1Task1: Task 1.1\n        Stage1Task2: Task 1.2\n        Stage1Task3: Task 1.3\n        Stage1Task4: Task 1.4\n    }\n\n    state \"Stage 2\" as Stage2 {\n        Stage2Task1: Task 2.1\n        Stage2Task2: Task 2.2\n        Stage2Task3: Task 2.3\n        Stage2Task4: Task 2.4\n    }\n\n    state \"Stage 3\" as Stage3 {\n        Stage3Task1: Task 3.1\n    }\n\n    Stage1 --> Stage2\n    Stage2 --> Stage3\n```\n\n单任务 DAG 可以读取零个或多个数据源，并输出零个或多个结果数据源\n\n单任务DAG的操作和转换：\n\n1. 输入\n2. map only 处理：包括 map、flatMap、forEach 等不在处理系统的节点之间交换任何数据的操作\n3. shuffle：数据根据 key 在多个节点之间重新分发，是最耗费资源的\n4. 重分区：一种轻量的 shuffle，其将数据进行重新调整所属节点\n5. 分布式join：包括广播 join、远程缓存 join 等\n\n```mermaid\n---\ntitle: DAG 管道\n---\nstateDiagram-v2\n  direction LR\n  存储节点1 --> 处理节点1\n  存储节点1 --> 处理节点2\n  处理节点1 --> 存储节点2\n  处理节点1 --> 存储节点3\n  处理节点2 --> 存储节点4\n```\n\n不同于单任务 DAG，管道 DAG，涉及多个任务。管道 DAG 的每个处理节点都会读取持久存储节点的数据并写入持久存储节点\n\nDAG 管道的特点：\n\n1. 处理节点之间不会互相连接，而是会通过一个存储节点连接。处理节点负责从存储节点获取数据、处理数据，并将数据发送到一个或多个存储节点\n2. 管道总是以存储节点开始和结束，因为处理节点处理完数据后不会保留状态，只有存储节点保留了数据的值\n3. 存储节点可以被多个处理节点使用，以实现产出复杂、昂贵的数据重用及数据一致性保证\n\n## 架构\n\n### Lambda\n\n完整的数据集 = λ (实时数据) * λ (历史数据)\n\n```mermaid\ngraph TD\n    NewData[New data]\n    subgraph Speed Layer\n        RealtimeView1[Realtime view]\n        RealtimeView2[Realtime view]\n        RealtimeView3[Realtime view]\n    end\n    subgraph Batch Layer\n        MasterDataset[Master dataset]\n    end\n    subgraph Serving Layer\n        BatchView1[Batch view]\n        BatchView2[Batch view]\n        BatchView3[Batch view]\n    end\n    Query[\"Query: 'How many...?'\"]\n\n    NewData --> SpeedLayer\n    NewData --> BatchLayer\n    SpeedLayer --> RealtimeView1\n    SpeedLayer --> RealtimeView2\n    SpeedLayer --> RealtimeView3\n    BatchLayer --> MasterDataset\n    MasterDataset --> ServingLayer\n    ServingLayer --> BatchView1\n    ServingLayer --> BatchView2\n    ServingLayer --> BatchView3\n    RealtimeView1 --> Query\n    RealtimeView2 --> Query\n    RealtimeView3 --> Query\n    BatchView1 --> Query\n    BatchView2 --> Query\n    BatchView3 --> Query\n```\n\n- 批处理层通过处理所有的已有历史数据来实现数据的准确性，是基于完整的数据集来重新计算的\n- 速度层通过流处理，提供最新数据的实时视图来最小化延迟，一旦数据通过批处理层进入服务层，速度层中相应的结果就不再需要了\n- 应用在使用数据时，需要合并批处理层及速度层的数据，以此获得数据的完整视图\n\n![离线 + 实时数据视图](/assets/2024410151420.webp)\n\n这种架构的最大缺点在于需要维护两套代码分别进行批处理与流处理，由此可能会产生的两个层的数据不一致、没有足够的时间进行批处理等问题\n\n#### 批处理层\n\n更新算法：\n\n- 全量式算法（Recomputation Algorithm）：每次更新批处理视图时，都会重新计算整个主数据集的数据，生成新的批处理视图。实现简单，对于算法错误更新容错性好，但资源消耗大、速度慢\n- 增量式算法（Incremental Algorithm）：通过增量更新的方式，在新的数据到达时直接更新现有的视图，而不需要重新计算整个数据集。实现较复杂、增量式算法的视图可能会显著增大，因为需要存储更多的中间状态、算法错误更新容错性较差、但资源消耗少，速度快\n\n#### 服务层\n\n服务层的性能优化关键在于通过适当的索引结构设计来提高吞吐量和降低延迟。Lambda架构的优势在于可以根据查询需求定制服务层，使得系统能够在不同的负载和查询模式下保持高效运行\n\n范式化可以保持最小数据冗余，但同时查询慢。反范式化会增加数据冗余，但同时能提升查询性能。Lambda架构通过分离主数据集（master dataset）和服务层（serving layer），使批处理层照规范化方式存储，服务层为查询服务量身定制，可以根据需要进行优化，达到最佳性能\n\n服务层的数据库需求：\n\n1. 批量写入：需要支持批量生成视图。新版本的视图生成后，必须能够完全替换旧版本\n2. 可伸缩：必须能够处理任意大小的视图。这意味着需要分布在多台机器上，类似于分布式文件系统和批处理框架\n3. 随机读取：使用索引直接访问视图的小部分，以确保查询的低延迟\n4. 分布式容错\n\n服务层不需要随机写入功能。这是因为视图仅以批量方式生成，而不是逐个更新的。同时随机写入带来了更多的复杂性，没有随机写入，数据库可以优化读取路径。\n\n#### 速度层\n\n在速度层，使用与批处理类似的全量算法是不可行的，可行的解决方案都依赖于增量算法，思路是在数据进入时更新实时视图，从而重用之前用于生成视图的工作\n\n存储实时视图的数据库最主要的是要满足，这两点一般为 NoSQL 数据库所满足：\n\n- 随机读取：能够迅速响应查询。这意味着存储的数据必须是被索引的\n- 随机写入：为了支持增量算法，数据库还必须能够进行低延迟的随机写入，以便实时更新视图\n\n速度层的特点：\n\n- 最终准确性：某些函数在批处理计算中很容易计算，但在实时计算中却很难实现（例如唯一计数的确定），所以可以在速度层中采取近似的方法\n- 存储状态较少：由于速度层通常只存储自上次批处理层到现在这段时间内的数据，所以查询响应快、资源需求较低\n\n增量计算的挑战在于分布式系统带来的数据一致性问题，除了可以利用CRDTs（无冲突复制数据类型）来实现增量计算时的最终一致性，Lambda架构提供了天然的保护机制。即使实时视图由于某些边缘情况或合并算法的错误而损坏，批处理层和服务层的视图最终会自动修正这些错误\n\n#### 查询层\n\n负责利用批处理视图和实时视图来响应查询，确定从每个视图中使用哪些数据，并将它们合并以得到正确的结果\n\n查询层的视图必须被设计为可合并的，时间导向的查询自然适合这种结构\n\n### Kappa\n\n```mermaid\nstateDiagram-v2\n  state 速度层 {\n    任务N\n    任务N+1\n  }\n  数据 --> 速度层\n  state 服务层 {\n    数据N\n    数据N+1\n  }\n  任务N --> 数据N\n  任务N+1 --> 数据N+1\n  应用 --> 数据N+1\n```\n\n- 一个可以重跑历史数据的消息队列\n\nkappa 则是通过只支持流处理来避免 Lambda 架构的复杂性，当计算逻辑发生变更时，就可以将 offset 回拨，重新生成数据，当新的数据追赶上了旧数据，应用便可以切换到新数据读取数据\n\n这种架构的缺点在于很难对历史数据进行重新处理。\n\n### 事件溯源\n\n> 对对象记录事件状态产生的变化，需要对象时对这些事件进行重放\n\n为了避免重放事件带来的效率问题，常见的解决方案是定时保存对象快照\n\n![屏幕截图 2021-01-25 141426](/assets/屏幕截图%202021-01-25%20141426.png)\n\n好处：\n\n1. 可靠发布领域事件\n2. 保留了变更历史\n3. 最大程度避免阻抗失衡\n\n弊端：\n\n1. 有一定学习曲线\n2. 基于消息传递应用的复杂性\n3. 随着代码演进状态发生变更就必须兼容以前版本\n4. 删除与查询变得不再容易\n\n## Spark\n\n- Spark SQL 主要用于结构化数据的处理：支持以SQL语法查询各种数据源\n- Spark Streaming：微批处理 达到类流处理\n- MLlib：机器学习库\n- Graphx：用于图形计算和图形并行计算的新组件\n\nSpark 比 MapReduce 快的原因：更为简单的 RDD 编程模型减少了作业调度次数，以及优先使用内存\n\n![各组件](/assets/202338213435.webp)\n\n1. SparkContext 启动 DAGScheduler 构造执行的 DAG 图，拆分成计算任务\n2. Driver 向 Cluster Manager 请求计算资源，分配 Worker\n3. Worker 向 Driver 注册并下载代码执行\n\n### RDD\n\n- 弹性分布式数据集（Resilient Distributed Dataset）\n\n分区：同一个 RDD 包含的数据被存储在系统的不同节点中，需要读取时根据ID 和分区的 index 可以唯一确定对应数据块的编号，从而通过底层存储层的接口中提取到数据进行处理\n\n不可变：一个 RDD 都是只读的，只可以对现有的 RDD 进行转换（Transformation）操作，得到新的 RDD 作为中间计算的结果\n\n并行：由于上面两个特性，就可以并行对 RDD 进行操作\n\n#### 结构\n\nSparkContext：所有 Spark 功能的入口，它代表了与 Spark 节点的连接，一个线程只有一个 SparkContext\n\nSparkConf： 一些参数配置信息\n\nPartitions：数据的逻辑结构，每个 Partition 会映射到某个节点内存或硬盘的一个数据块\n\nPartitioner：定义了划分数据分片的分区规则，如按哈希取模或是按区间划分等\n\nDependencies：每一步产生的 RDD 里都会存储它的依赖关系，即它是通过哪个 RDD 经过哪个转换操作得到的\n\n```mermaid\n---\ntitle: 窄依赖，父 RDD 的分区可以一一对应到子 RDD 的分区\n---\nstateDiagram-v2\n  state RDD1 {\n    RDD1_分区1\n    RDD1_分区2\n    RDD1_分区3\n  }\n  state RDD2 {\n    RDD2_分区1\n    RDD2_分区2\n    RDD2_分区3\n  }\n  RDD1_分区1 --> RDD2_分区1: map\n  RDD1_分区2 --> RDD2_分区2: filter\n  RDD1_分区3 --> RDD2_分区3: union等\n```\n\n```mermaid\n---\ntitle: 宽依赖，父 RDD 的每个分区可以被多个子 RDD 的分区使用\n---\nstateDiagram-v2\n  state RDD1 {\n    RDD1_分区1\n    RDD1_分区2\n    RDD1_分区3\n  }\n  state RDD2 {\n    RDD2_分区1\n    RDD2_分区2\n  }\n  RDD1_分区1 --> RDD2_分区1: join\n  RDD1_分区2 --> RDD2_分区1: join\n  RDD1_分区3 --> RDD2_分区1: join\n  RDD1_分区1 --> RDD2_分区2: groupBy等\n  RDD1_分区2 --> RDD2_分区2: groupBy等\n  RDD1_分区3 --> RDD2_分区2: groupBy等\n\n```\n\n窄依赖允许子 RDD 的每个分区可以被并行处理产生，而宽依赖则必须等父 RDD 的所有分区都被计算好之后才能开始处理\n\nCheckpoint：对于一些计算过程比较耗时的 RDD，可以进行持久化，标记这个 RDD 有被检查点处理过，并且清空它的所有依赖关系，这样在进行崩溃恢复的时候就不用在向前向父 RDD 回溯\n\nStorage Level：记录 RDD 持久化时的存储级别，内存或内存硬盘 或在分区节点上内存、内存硬盘\n\nIterator：迭代函数，Compute：计算函数 都是用来表示 RDD 怎样通过父 RDD 计算得到的\n\npreferredLocations：本着计算向数据移动原则，会优先将task任务部署在其将要处理的数据所在的节点上\n\n#### 数据操作\n\n大部分操作跟[Stream](/编程语言/JAVA/高级/Stream流.md)差不多\n\n- 转换（Transformation）：把一个 RDD 转换成另一个 RDD map、filter、mapPartitions，groupByKey\n- 动作（Action）：通过计算返回一个结果 collect、reduce、count，countByKey\n\nSpark 的 Shuffle 操作跟 MapReduce 是一样的，其通过生产与消费 Shuffle 中间文件的方式，来完成集群范围内的数据交换\n\n![Shuffle 中间文件](/assets/2023829192817.webp)\n\nMap 阶段产生中间文件：\n\n- 对于分片中的数据记录，逐一计算其目标分区，并将其填充到 PartitionedPairBuffer\n- PartitionedPairBuffer 填满后，如果分片中还有未处理的数据记录，就对 Buffer 中的数据记录按（目标分区 ID，Key）进行排序，将所有数据溢出到临时文件，同时清空缓存\n- 重复以上步骤直到分片中所有的数据记录都被处理\n- 对所有临时文件和 PartitionedPairBuffer 归并排序，最终生成数据文件和索引文件\n\nReduce 阶段分发数据：\n\nReduce Task 将拉取到的数据块填充到读缓冲区，然后按照任务的计算逻辑不停地消费、处理缓冲区中的数据记录\n\n```mermaid\nstateDiagram-v2\n  direction LR\n  MapTask1 --> 数据文件1\n  MapTask1 --> 索引文件1\n  MapTask2 --> 数据文件2\n  MapTask2 --> 索引文件2\n  数据文件1 --> ReduceTask1\n  数据文件1 --> ReduceTask2\n  索引文件1 --> ReduceTask1\n  索引文件1 --> ReduceTask2\n  数据文件2 --> ReduceTask1\n  数据文件2 --> ReduceTask2\n  索引文件2 --> ReduceTask1\n  索引文件2 --> ReduceTask2\n```\n\nshuffle 的过程带来的开销不仅有 Map 阶段后的内存与磁盘方面，也有 Reduce 阶段数据分发的网络开销\n\n#### DAG\n\n在 Spark 开发下，对一个 RDD 上调用算子、封装计算逻辑，这个过程会衍生新的子 RDD，在子 RDD 之上，开发者还会继续调用其他算子，衍生出新的 RDD，如此往复便有了 DAG\n\n而 DAG 的多个节点，会根据 Action 操作，并以 shuffle 为边界，形成多个 Stage。在同一 Stage 内部，所有算子融合为一个函数，Stage 的输出结果由这个函数一次性作用在输入数据集而产生，同一 Stage 内部的计算，都是在内存中进行的，同时由于将多个算子或操作融合为一个函数，所以也不会产生多份内存数据\n\n### 调度系统\n\n```mermaid\nsequenceDiagram\n  DAGScheduler ->> TaskScheduler: TaskSets\n  SchedulerBackend ->> TaskScheduler: WorkerOffer\n  SchedulerBackend ->> SchedulerBackend: 任务调度\n  TaskScheduler ->> SchedulerBackend: LaunchTask\n  SchedulerBackend -->> ExecutorBackend: LaunchTask\n  SchedulerBackend --> ExecutorBackend: LaunchedExecutor/RemoveExecutor/StatusUpdate\n\n  ExecutorBackend ->> ExecutorBackend: 任务执行\n  ExecutorBackend -->> SchedulerBackend: StatusUpdate\n  SchedulerBackend ->> TaskScheduler: StatusUpdate\n  TaskScheduler ->> DAGScheduler: StatusUpdate\n```\n\n1. DAGScheduler 以 Shuffle 为边界，将开发者设计的计算图 DAG 拆分为多个执行阶段 Stages，然后为每个 Stage 创建任务集 TaskSet\n2. SchedulerBackend 通过与 Executors 中的 ExecutorBackend 的交互来实时地获取集群中可用的计算资源，并将这些信息记录到 ExecutorDataMap 数据结构\n3. 与此同时，SchedulerBackend 根据 ExecutorDataMap 中可用资源创建 WorkerOffer，以 WorkerOffer 为粒度提供计算资源\n4. 对于给定 WorkerOffer，TaskScheduler 结合 TaskSet 中任务的本地性倾向，按照 PROCESS_LOCAL、NODE_LOCAL、RACK_LOCAL 和 ANY 的顺序，依次对 TaskSet 中的任务进行遍历，优先调度本地性倾向要求苛刻的 Task，尽量使计算向数据移动\n5. 被选中的 Task 由 TaskScheduler 传递给 SchedulerBackend，再由 SchedulerBackend 分发到 Executors 中的 ExecutorBackend。Executors 接收到 Task 之后，即调用本地线程池来执行分布式任务。\n\n### 存储系统\n\n存储系统用于存储 RDD 缓存、Shuffle 中间文件、广播变量\n\n```mermaid\nstateDiagram-v2\n  BlockManagerMaster --> BlockManager\n  BlockManager --> BlockManagerMaster\n  BlockManager --> MemoryStore\n  MemoryStore --> BlockManager\n  BlockManager --> DiskStore\n  DiskStore --> BlockManager\n\n  note right of BlockManagerMaster\n    在Driver端统一管理、\n    维护存储系统全局元数据\n  end note\n  note right of MemoryStore\n    管理RDD Cache、\n    广播变量在内存中的存储\n  end note\n  note right of BlockManager\n    在Executori端搜集本地存储系统元数据，\n    及时向BlockManagerMasteri汇报，\n    同时从BlockManagerMaster获取全局信息\n  end note\n  note right of DiskStore\n    管理RDD Cache、Shuffle\n    中间文件在磁盘中的存储\n  end note\n```\n\nMemoryStore 同时支持存储对象值和字节数组这两种不同的数据形式，并且统一采用 MemoryEntry 数据抽象对它们进行封装，其内部会使用 LinkedHashMap 存储数据，以实现LRU算法\n\n而 DiskStor 也会记录逻辑数据块 Block 与磁盘文件系统中物理文件的对应关系，每个 Block 都对应一个磁盘文件\n\n### 内存管理\n\n内存的区域划分：\n\n- Execution Memory：用于执行分布式任务，如 Shuffle、Sort 和 Aggregate 等操作\n- Storage Memory：用于缓存 RDD 和广播变量等数据\n- User Memory：用于存储开发者自定义数据结构\n- Reserved Memory：用来存储各种 Spark 内部对象，例如存储系统中的 BlockManager、DiskBlockManager 等\n\n1.6 版本之后，Spark 推出了统一内存管理模式， Execution Memory 和 Storage Memory 之间可以相互转化\n\n1. 如果对方的内存空间有空闲，双方就都可以抢占\n2. 对于 RDD 缓存任务抢占的执行内存，当执行任务有内存需要时，RDD 缓存任务必须立即归还抢占的内存，涉及的 RDD 缓存数据要么落盘、要么清除\n3. 对于分布式计算任务抢占的 Storage Memory 内存空间，即便 RDD 缓存任务有收回内存的需要，也要等到任务执行完毕才能释放\n\n#### 广播变量\n\n一种分发机制，它一次性封装目标数据结构，以 Executors 为粒度去做数据分发\n\n当有两个数据集需要 join 时，可以利用广播变量将较小的数据集进行广播，避免 shuffle\n\n### SparkSQL\n\n![架构](/assets/20221218152657.webp)\n\n#### DataSet\n\nDataSet 所描述的数据都被组织到有名字的列中，就像关系型数据库中的表一样\n\n![20221218152850](/assets/20221218152850.webp)\n\n#### DataFrame\n\n可以被看作是一种特殊的 DataSet，但是它的每一列并不存储类型信息，所以在编译时并不能发现类型错误\n\n从数据表示（Data Representation）的角度来看，是否携带 Schema 是 DataFrame 与 RDD 唯一的区别，RDD 算子多是高阶函数，这些算子允许开发者灵活地实现业务逻辑，表达能力极强\n\n```mermaid\ngraph TD;\n    A[SQL Query] --> B[解析和分析];\n    B --> C[生成逻辑执行计划];\n    C --> D[逻辑执行计划优化];\n    D --> E[生成物理执行计划];\n    E --> F[物理执行计划优化];\n    F --> G[代码生成];\n    G --> H[执行查询];\n```\n\nCatalyst 执行过程优化\n\n- 逻辑执行计划生成（Logical Plan Generation）：在这一步骤中，Spark SQL 将解析和分析后的查询转换为逻辑执行计划。逻辑执行计划是一个抽象的表示，描述了查询的逻辑结构，但不包含具体的物理执行细节。\n- 逻辑执行计划优化（Logical Plan Optimization）：一旦生成了逻辑执行计划，Spark SQL 引擎会执行一系列的逻辑优化操作，以改进查询的执行效率。这些优化操作包括谓词下推、列裁剪、表达式简化等。核心思想都是用尽一切办法，减少需要扫描和处理的数据量，降低后续计算的负载\n- 物理执行计划生成（Physical Plan Generation）：在逻辑执行计划优化完成后，Spark SQL 引擎将逻辑执行计划转换为物理执行计划。物理执行计划描述了如何在分布式环境下执行查询，包括数据的分区、数据的分发和执行算子的顺序等。\n- 物理执行计划优化（Physical Plan Optimization）：一旦生成了物理执行计划，Spark SQL 引擎会执行一些物理优化操作，以进一步改进查询的执行性能。这些操作包括数据本地化、并行执行、流水线处理等\n\n### SparkStreaming\n\nSpark Streaming 用时间片拆分了无限的数据流，然后对每一个数据片用类似于批处理的方法进行处理，输出的数据也是一块一块的，通过提供了一个对于流数据的抽象 DStream 来描述数据流，底层 DStream 也是由很多个序列化的 RDD 构成，按时间片（比如一秒）切分成的每个数据单位都是一个 RDD\n\n![20221218153730](/assets/20221218153730.webp)\n\n主要缺点是实时计算延迟较高，这是由于 Spark Streaming 不支持太小的批处理的时间间隔\n\n### StructuredStreaming\n\n输入的数据流按照时间间隔（以一秒为例）划分成数据段。每一秒都会把新输入的数据添加到表中，Spark 也会每秒更新输出结果。输出结果也是表的形式，输出表可以写入硬盘或者 HDFS。\n\n![20221218154712](/assets/20221218154712.webp)\n\nStructured Streaming 提供一个 level 更高的 API，这样的数据抽象可以让开发者用一套统一的方案去处理批处理和流处理\n\n相比 SparkStreaming，StructuredStreaming可以支持更小的时间间隔，2.3 也引入了连续处理模式，同时也有对事件时间的支持\n\n### 优化\n\n#### Project Tungsten\n\n3.0 引入了 Project Tungsten：\n\n1. 在数据结构方面，Tungsten 自定义了紧凑的二进制格式，计算、内存效率比 Java 对象高\n2. 基于内存页的顺序内存访问加上堆外内存来管理数据结构规避了使用 Java HashMap 额外的内存开销、GC开销，及提升 CPU 缓存的命中率\n3. 用全阶段代码生成（Whol Stage Code Generation）取代火山迭代模型，不仅减少了虚函数调用和降低内存访问频率，由于每一条指令都是明确的，可以顺序加载到 CPU 寄存器，源数据也可以顺序地加载到 CPU 的各级缓存中\n\n#### AQE\n\nAQE（Adaptive Query Execution），可以在 Spark SQL 优化的过程中动态地调整执行计划：\n\n1. 自动 join 策略调整，根据数据的大小和分布等因素自动选择最佳的连接策略，以提高查询性能\n2. 自动分区合并：当 shuffle 的 reduce 阶段产生了小分区后，这些小分区会被合并成一个较大的分区，减少调度开销\n3. 自动数据倾斜处理：如果 AQE 发现某张表存在倾斜的数据分片，就会自动对它通过把数据重新分区\n4. ...\n\n#### shuffle 优化\n\n1. 减少 shuffle 的数据量\n2. 避免 shuffle，如果无法避免，则尽可能拖到最后再去 shuffle，这样也能减少 shuffle 的数据量\n\n#### DPP 动态分区裁剪\n\n分区表都会在文件系统中创建单独的子目录来存储相应的数据分片，如果过滤谓词中包含分区键，那么 Spark SQL 对分区表做扫描的时候，是完全可以跳过（剪掉）不满足谓词条件的分区目录，这就是分区剪裁\n\n动态分区剪裁这个功能主要用在星型模型数仓的数据关联场景中，动态分区裁剪把维度表中的过滤条件，通过关联关系传导到事实表，来减少事实表中数据的扫描量\n\n#### join 优化\n\nSpark 的分布式 join 衍生自单机的 Nested Loop Join、Hash Join、Sort Merge Join 等。\n\n在等值 join 下，Spark SQL优先选择BHJ,其次是SMJ,再次是SHJ。其中，SHJ执行效率更高，但是内表数据全部放入内存的要求过于苛刻，SMJ可以充分利用磁盘来完成排序与归并关联，因此，Spark SQL默认选择SMJ作为兜底策略，完成等值Join的关联计算\n\n在不等值 join 下，因为不等值Joi只能依赖NLJ算法，因此内表满足广播阈值条件的情况下，在这种关联场景中，Spark SQL会优先选择BNLJ。否则，Spark SQL只能使用笨重的CPJ来兜底去完成关联计算\n\n| Join策略                   | 策略缩写 | Join Hints               | 适用场景                                                        |\n| -------------------------- | -------- | ------------------------ | --------------------------------------------------------------- |\n| Broadcast Hash Join        | BHJ      | broadcast、broadcastjoin | 内表较小，通常在2GB左右为宜，最大不能超过8GB                    |\n| Shuffle Merge Sort Join    | SMJ      | merge、mergejoin         | 等值Join下，Spark SQL的兜底策略，不需要开发者特别设置           |\n| Shuffle Hash Join          | SHJ      | shuffle hash             | 当大表超过小表3倍以上，且小表数据分布比较均匀，SHJ比SMJ效率更佳 |\n| Broadcast Nested Loop Join | BNLJ     | 无                       | 没有提供Join Hints,开发者无能为力，只能靠Spark SQL自行判断      |\n| Cartesian Product Join     | CPJ      | shuffle_replicate_nl     | 不等值Join下，Spark SQL的兜底策略，不需要开发者特别设置         |\n\n对于两张大表的直接 join，是需要尽力避免的，一种优化方式是通过分区机制，将每张大表拆成较小的表，将匹配上的两张较小的表进行join，最后再进行合并\n\n## Flink\n\n### 架构\n\n![20221219144426](/assets/20221219144426.webp)\n\n### 核心模型\n\n最核心的数据结构是 Stream，它代表一个运行在多个分区上的并行流\n\n当一个 Flink 程序被执行的时候，它会被映射为 Streaming Dataflow：\n\n![2022121914389](/assets/2022121914389.webp)\n\n程序天生是并行和分布式的。一个 Stream 可以包含多个分区（Stream Partitions），一个操作符可以被分成多个操作符子任务，每一个子任务是在不同的线程或者不同的机器节点中独立执行的：\n\n![20221219144240](/assets/20221219144240.webp)\n\n## Beam\n\n一个适配流处理、批处理的中间层\n\n### 编程模型\n\n- 窗口：将无边界数据根据事件时间分成了一个个有限的数据集\n- 水位线：来表示与数据事件时间相关联的输入完整性的概念，用来测量数据进度\n- 触发器：指的是表示在具体什么时候，数据处理逻辑会真正地触发窗口中的数据被计算\n- 累加模式：如果在同一窗口中得到多个运算结果，我们应该如何处理这些运算结果\n\n### PCollection\n\n- Parallel Collection，意思是可并行计算的数据集\n\n特性：\n\n- 需要编码器：需要将你的数据序列化/反序列化以在网络上传输\n- 无序：以个 PCollection 被分配到不同的机器上执行，那么为了保证最大的处理输出，不同机器都是独立运行的，它的执行顺序就无从得知了\n- 无界：Beam 要统一表达有界数据和无界数据，所以没有限制它的容量\n- 不可变\n\n### Transform\n\n```mermaid\nstateDiagram-v2\n  数据1 --> Transform\n  数据2 --> Transform\n  Transform --> 数据3\n  Transform --> 数据4\n```\n\n常见的 Transform 接口：\n\n- ParDo：类似于flatMap\n- GroupByKey：把一个 Key/Value 的数据集按 Key 归并\n\n### Pipeline\n\n```mermaid\nstateDiagram-v2\n  输入 --> PCollection1: Transform1\n  PCollection1 --> PCollection2: Transform2\n  PCollection2 --> PCollection3: Transform3\n  PCollection3 --> 输出: Transform4\n```\n\n分布式环境下，整个数据流水线会启动 N 个 Workers 来同时处理 PCollection，在具体处理某一个特定 Transform 的时候，数据流水线会将这个 Transform 的输入数据集 PCollection 里面的元素分割成不同的 Bundle，将这些 Bundle 分发给不同的 Worker 来处理\n\n在单个 Transfrom中，如果某一个 Bundle 里面的元素因为任意原因导致处理失败了，则这整个 Bundle 里的元素都必须重新处理\n\n在多步骤的 Transform 上，如果处理的一个 Bundle 元素发生错误了，则这个元素所在的整个 Bundle 以及与这个 Bundle 有关联的所有 Bundle 都必须重新处理\n\n### IO\n\n- XXIO.read()\n- XXIO.write()\n\n## StreamingSQL\n\n```sql\n/* 窗口：最近10个温度的平均值 */\nSelect bid, avg(t) as T From BoilerStream WINDOW HOPPING (SIZE 10, ADVANCE BY 1);\n/* join */\nfrom TempStream[temp > 30.0]#window.time(1 min) as T\n  join RegulatorStream[isOn == false]#window.length(1) as R\n  on T.roomNo == R.roomNo\nselect T.roomNo, R.deviceID, 'start' as action\ninsert into RegulatorActionStream; // Siddhi Streaming SQL\n/*  某个模式有没有在特定的时间段内发生 */\nfrom every( e1=TempStream ) -> e2=TempStream[ e1.roomNo == roomNo and (e1.temp + 5) <= temp ]\n    within 10 min\nselect e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp\ninsert into AlertStream;\n```\n","metadata":"","hasMoreCommit":true,"totalCommits":29,"commitList":[{"date":"2024-11-26T19:44:48+08:00","author":"MY","message":"📦数据处理","hash":"7866da93cdb2fb828132103cd4d0a2c257f3b59f"},{"date":"2024-11-25T15:59:06+08:00","author":"MY","message":"📦微服务","hash":"c8734b29d256e1b4841404e738ec87270d58d3f2"},{"date":"2024-06-24T20:03:06+08:00","author":"MY","message":"✏数据处理","hash":"17d9a9cfd5f093762857622df2cac4cfd26bebd7"},{"date":"2024-06-21T14:29:36+08:00","author":"MY","message":"✏数据处理","hash":"9005876db7ed5b95e243b1bb24d0243c9cde8427"},{"date":"2024-06-20T20:06:54+08:00","author":"MY","message":"✏数据处理","hash":"1748c3a4f9b0745fff0ccbf06c6b99f76a5c6b19"},{"date":"2024-06-19T19:38:53+08:00","author":"MY","message":"📦特征工程","hash":"04cdae4a71b3208a6b6add72773b0b7e54348f5d"},{"date":"2024-06-13T20:20:37+08:00","author":"MY","message":"➕流处理","hash":"786031c785bbd78729c2d621ad4b0182dc9f24b7"},{"date":"2024-06-12T19:45:05+08:00","author":"MY","message":"✏数据工程","hash":"b5657fde32ae6d7136781e7a2d22a545350dedbb"},{"date":"2024-06-05T19:47:18+08:00","author":"MY","message":"📦数据管理","hash":"691dc7a05c78b6f64162f2138915ae6a41d45304"},{"date":"2024-05-09T19:28:09+08:00","author":"MY","message":"✏spark","hash":"aeddb92eb8c2e5304435e78d0c631b5b97f97bd2"}],"createTime":"2022-12-14T15:38:14+08:00"}