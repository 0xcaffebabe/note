{"name":"深度学习","id":"数据技术-深度学习","content":"# 深度学习\n\n> 1. 深度学习在本质上解决什么问题？\n> 2. 各类神经网络模型之间为什么不同、差异来自哪里？\n> 3. 模型为何不断演进，哪些思想是稳定的、可迁移的？\n\n---\n\n## 一、深度学习的第一性原理\n\n### 1.1 本质定义\n\n**深度学习的本质**是：\n\n> 在高维空间中，使用可组合、可微的参数化函数族，近似复杂数据分布或条件映射关系，并通过梯度优化寻找具有泛化能力的解。\n\n其核心不在于“层数深”，而在于：\n\n* **函数复合**（Representation Composition）\n* **结构化假设**（Inductive Bias）\n* **可优化性**（Differentiable Optimization）\n\n---\n\n### 1.2 三个根本矛盾\n\n1. **表达能力 vs 泛化能力**\n   模型越复杂，越容易过拟合\n2. **理论最优 vs 计算可行**\n   全局最优不可达，只能近似搜索\n3. **数据规模 vs 模型复杂度**\n   数据决定模型上限，结构决定效率\n\n所有模型设计，本质上都是在这三者之间取平衡。\n\n---\n\n## 二、统一认知框架：归纳偏置视角\n\n不同神经网络并非“功能不同的黑箱”，而是**对数据结构作出了不同假设**。\n\n### 2.1 归纳偏置的核心维度\n\n| 维度     | 说明                |\n| ------ | ----------------- |\n| 输入结构假设 | 向量 / 序列 / 网格      |\n| 参数共享方式 | 无共享 / 时间共享 / 空间共享 |\n| 依赖建模范围 | 局部 / 全局           |\n| 记忆机制   | 无 / 显式 / 门控       |\n| 学习范式   | 判别式 / 生成式         |\n\n后续所有模型，都可放入这一坐标系中理解。\n\n---\n\n## 三、无结构假设：感知机与前馈网络\n\n### 3.1 感知机模型\n\n**问题假设**：数据线性可分\n**结构设计**：线性加权 + 阈值判断\n\n[\nw_i(t+1)=w_i(t)+\\eta[d_j-y_j(t)]\\cdot x_{j,i}\n]\n\n感知机的历史意义不在于能力，而在于：\n\n* 首次将“学习”形式化为参数更新\n* 奠定了梯度优化思想基础\n\n![感知器的学习过程](/assets/2024117191239.webp)\n\n---\n\n### 3.2 深度前馈网络（MLP）\n\n**问题假设**：输入是独立同分布的向量\n**核心思想**：多层非线性函数复合\n\n* 信息单向流动（前馈）\n* 非线性来自激活函数\n* 表达能力随层数指数提升\n\n前馈网络解决的是：\n\n> **从线性模型到通用函数逼近器的跃迁**\n\n---\n\n## 四、表示学习：自编码器体系\n\n### 4.1 自编码器（AutoEncoder）\n\n自编码器是一类**无监督表示学习模型**。\n\n[\n\\phi,\\psi=\\arg\\min_{\\phi,\\psi}||\\mathbf{X}-(\\phi\\circ\\psi)(\\mathbf{X})||^2\n]\n\n**信息论视角**：\n\n* 编码：有损压缩\n* 解码：重构信号\n* 目标：在压缩与重构误差之间取得最优平衡\n\n自编码器的本质贡献是：\n\n> **将“特征工程”内生化为模型结构本身**\n\n---\n\n## 五、优化的本质与训练动力学\n\n### 5.1 深度模型中的优化难题\n\n* 病态矩阵：数值不稳定\n* 局部极值：非凸目标\n* 鞍点：高维空间的主要障碍\n\n这些问题并非“实现缺陷”，而是**高维非凸优化的必然结果**。\n\n---\n\n### 5.2 随机梯度下降（SGD）\n\n**核心思想**：\n\n> 用噪声换取可扩展性\n\n* 小批量近似真实梯度\n* 噪声既是问题，也是正则化手段\n\n常见改进：\n\n* Momentum\n* Accelerated Gradient\n* Adam（自适应学习率）\n\n---\n\n## 六、空间归纳偏置：卷积神经网络（CNN）\n\n### 6.1 卷积层\n\n* 局部感受野\n* 参数共享\n* 平移不变性假设\n\n### 6.2 卷积神经网络\n\nCNN 的核心不是“卷积运算”，而是：\n\n> **对空间局部性和层级结构的强假设**\n\n![卷积网络的结构图](/assets/2024117155242.webp)\n\nCNN 的成功来源于：\n\n* 极强的结构约束\n* 极低的有效参数维度\n\n---\n\n## 七、时间归纳偏置：序列与递归结构\n\n### 7.1 循环神经网络（RNN）\n\n[\n\\mathbf{h}_t=f(\\mathbf{W}\\mathbf{x}*t+\\mathbf{U}\\mathbf{h}*{t-1})\n]\n\n**本质**：时间维度上的参数共享\n\n问题：\n\n* 梯度消失 / 爆炸\n\n---\n\n### 7.2 长短期记忆网络（LSTM）\n\nLSTM 通过门控机制：\n\n* 控制信息流\n* 显式管理长期记忆\n\n其意义在于：\n\n> **将“记忆”从隐式状态提升为可控结构**\n\n---\n\n### 7.3 seq2seq 与注意力\n\n* 编码器：压缩序列语义\n* 解码器：生成目标序列\n* 注意力：打破固定长度瓶颈\n\n![20240804150736](/assets/20240804150736.gif)\n\n---\n\n## 八、从序列到全局：Transformer\n\nTransformer 的革命性在于：\n\n* 放弃递归\n* 通过 Attention 建模全局依赖\n\n![Transformer 整体架构](/assets/20240804152626.webp)\n\n其本质优势是：\n\n> **计算路径最短化 + 并行化最大化**\n\n---\n\n## 九、生成式与概率视角模型\n\n### 9.1 深度信念网络（DBN）\n\n* 基于概率图模型\n* 核心单元：受限玻尔兹曼机\n\n历史意义大于现实应用。\n\n---\n\n### 9.2 生成式对抗网络（GAN）\n\n* 生成器：拟合数据分布\n* 判别器：区分真伪\n\n[\n\\arg\\min_g\\max_D-\\dfrac12\\int_x[p_{data}(x)\\log(D(x))+p_g(x)\\log(1-D(x))],dx\n]\n\nGAN 的思想价值在于：\n\n> **将学习转化为博弈过程**\n\n---\n\n## 十、补充模型：特定归纳假设的探索\n\n* 径向基神经网络：局部响应假设\n* 自组织映射（SOM）：拓扑保持映射\n* 模糊神经网络：不确定性建模\n\n这些模型逐渐淡出主流，但其思想被继承。\n\n---\n\n## 十一、表示学习总结\n\n表示学习的统一目标：\n\n> **学习对任务有用、对噪声不敏感、可迁移的中间表示**\n\n深度学习的未来，不在于更多模型，而在于：\n\n* 更合理的结构假设\n* 更稳定的优化动力学\n* 更可解释的表示空间\n\n## 关联内容（自动生成）\n\n- [/数据技术/机器学习.md](/数据技术/机器学习.md) 深度学习是机器学习的一个重要分支，两者在模型优化、泛化能力等方面有密切关系\n- [/数据技术/特征工程.md](/数据技术/特征工程.md) 深度学习中的表示学习与特征工程中的自动特征提取技术密切相关\n- [/数据技术/监督学习.md](/数据技术/监督学习.md) 深度学习模型通常用于解决监督学习任务，如分类和回归问题\n- [/数据技术/推荐系统.md](/数据技术/推荐系统.md) 深度学习在推荐系统中用于学习用户和物品的复杂表示\n- [/数学/线性代数.md](/数学/线性代数.md) 线性代数是深度学习的数学基础，特别是矩阵与向量运算在神经网络中的应用\n- [/数学/概率论与数理统计.md](/数学/概率论与数理统计.md) 概率论为深度学习中的不确定性建模和贝叶斯方法提供理论基础\n- [/数据技术/非监督学习.md](/数据技术/非监督学习.md) 深度学习在无监督学习领域有重要应用，如自编码器和生成模型\n- [/数据技术/数据处理.md](/数据技术/数据处理.md) 深度学习模型的训练和推理需要高效的数据处理框架支持\n- [/编程语言/python.md](/编程语言/python.md) Python是深度学习领域的主要编程语言，拥有丰富的深度学习框架和库\n- [/计算机系统/数字逻辑电路.md](/计算机系统/数字逻辑电路.md) 神经网络加速器（NPU）等硬件为深度学习模型提供计算支持\n","metadata":"tags: ['数据技术']","hasMoreCommit":false,"totalCommits":5,"commitList":[{"date":"2026-02-12T14:07:03+08:00","author":"MY","message":"doc: 整理标签","hash":"290b3e8ad18f48832ac282290238d020fc030a88"},{"date":"2025-12-30T17:19:45+08:00","author":"MY","message":"feat(doc): 深度学习文档重构与知识体系完善","hash":"29626f8158968e6fc336505b9abeeb2b44640fa4"},{"date":"2024-08-04T17:33:29+08:00","author":"MY","message":"✏️机器学习","hash":"85c867fa1bff5caa612b8d80349e6a64ce8062d0"},{"date":"2024-01-17T20:03:46+08:00","author":"MY","message":"✏机器学习","hash":"a64bdf732c7a380407a3f494f728a61f8632c16d"},{"date":"2024-01-16T13:39:44+08:00","author":"MY","message":"📦机器学习","hash":"cd5c15fdc0352c79322085de552b50fdc39a8c58"}],"createTime":"2024-01-16T13:39:44+08:00"}