{"name":"特征工程","id":"数据技术-特征工程","content":"\n# 特征工程\n\n利用工程手段从“用户信息”“物品信息”“场景信息”中提取特征的过程，在已有的、可获得的数据基础上，“尽量”保留有用信息是现实中构建特征工程的原则\n\n## 常用特征\n\n1. 用户行为数据：区分隐式反馈与显式反馈，对用户行为数据的采集与使用与业务强相关\n2. 用户关系数据：人与人之间连接的记录，区分强关系（主动建立连接）与弱关系（间接的关系导致的连接）\n3. 属性、标签类数据：物品属性、人口属性、主动打的标签等\n4. 内容类数据：描述型文字、图片，甚至视频，需要进一步通过NLP、图像识别等转为结构化信息才能作为特征使用\n5. 场景信息：描述的是用户所处的客观的推荐环境，常见的有所处于什么时空\n\n## 特征处理\n\n进行特征处理的目的，是把所有的特征全部转换成一个数值型的特征向量\n\n类别特征处理：\n\nOne-hot 编码（也被称为独热编码），它是将类别、ID 型特征转换成数值向量的一种最典型的编码方式。它通过把所有其他维度置为 0，单独将当前类别或者 ID 对应的维度置为 1 的方式生成特征向量\n\n```\n周二 => [0,1,0,0,0,0,0] -- 将一周7天视为7个维度，将周二所在的维度设为1\n```\n\n数值类特征处理：\n\n1. [归一化](/数学/概率论与数理统计.md#特征变化)\n2. 分桶：将样本按照某特征的值从高到低排序，然后按照桶的数量找到分位数，将样本分到各自的桶中，再用桶 ID 作为特征值\n\n```\n分桶：\n[1,1,1,1,1,1,5,8,10] => [(1,5),(5,10)]\n```\n\n## Embedding\n\n用一个数值向量“表示”一个对象（Object）的方法\n\n### 词 Embedding\n\n![生成 Skip-gram 模型结构的训练数据](/assets/202391820926.webp)\n\n在通过神经网络训练得到模型，一个词就可以通过模型推断，转为向量\n\n连续词袋（CBOW）：一种通过上下文预测目标词的神经网络架构，上下文由目标词周围的一个或多个词组成，这个数目由窗口大小决定。窗口是指上下文词语的范围，如果窗口为 10，那么模型将使用目标词前后各 10 个词\n\n### 图 Embedding\n\n1. Deep Walk：在由物品组成的图结构上进行随机游走，产生大量物品序列，然后将这些物品序列作为训练样本输入 Word2vec 进行训练，最终得到物品的 Embedding\n2. Node2vec：通过调整随机游走跳转概率的方法，让 Graph Embedding 的结果在网络的同质性（Homophily）和结构性（Structural Equivalence）中进行权衡。同质性指的是距离相近节点的 Embedding 应该尽量近似，结构性指的是结构上相似的节点的 Embedding 应该尽量接近\n   1. 为了使 Graph Embedding 的结果能够表达网络的“结构性”，在随机游走的过程中，需要让游走的过程更倾向于 BFS（Breadth First Search，广度优先搜索），因为 BFS 会更多地在当前节点的邻域中进行游走遍历，相当于对当前节点周边的网络结构进行一次“微观扫描”。当前节点是“局部中心节点”，还是“边缘节点”，亦或是“连接性节点”，其生成的序列包含的节点数量和顺序必然是不同的，从而让最终的 Embedding 抓取到更多结构性信息\n   2. 而为了表达“同质性”，随机游走要更倾向于 DFS（Depth First Search，深度优先搜索）才行，因为 DFS 更有可能通过多次跳转，游走到远方的节点上。但无论怎样，DFS 的游走更大概率会在一个大的集团内部进行，这就使得一个集团或者社区内部节点的 Embedding 更为相似，从而更多地表达网络的“同质性”\n\nEmbedding 可以直接使用，在到 Embedding 向量之后，直接利用 Embedding 向量的相似性实现某些推荐系统的功能。也可以预先训练好物品和用户的 Embedding 之后，不直接应用，而是把这些 Embedding 向量作为特征向量的一部分，跟其余的特征向量拼接起来，作为推荐模型的输入参与训练。最后是一种 E2E 的应用，即不预先训练 Embedding，而是把 Embedding 的训练与深度学习推荐模型结合起来，采用统一的、端到端的方式一起训练，直接得到包含 Embedding 层的推荐模型。\n\n### 非负矩阵因式分解\n\n输入多个样本数据，每个样本数据都是一个m维数值向量，首先把我们的数据集用矩阵的形式写出来，每一列是一个数据，而每一行是这些数据对应维度的数值。于是我们就有了一个大小为m*n的输入矩阵。而算法的目标就是将这个矩阵分解为另外两个非负矩阵的积\n","metadata":"","hasMoreCommit":false,"totalCommits":2,"commitList":[{"date":"2024-08-04T17:33:29+08:00","author":"MY","message":"✏️机器学习","hash":"85c867fa1bff5caa612b8d80349e6a64ce8062d0"},{"date":"2024-06-19T19:38:53+08:00","author":"MY","message":"📦特征工程","hash":"04cdae4a71b3208a6b6add72773b0b7e54348f5d"}],"createTime":"2024-06-19T19:38:53+08:00"}