{"name":"监督学习","id":"数据技术-监督学习","content":"# 监督学习\n\n## 一、监督学习的第一性原理\n\n### 1. 本质定义\n\n**监督学习的本质问题是：**\n\n> 在给定输入—输出样本对 $(x, y)$ 的条件下，\n> 在某个假设空间 $H$ 中，寻找一个函数 $f$，\n> 使其在未知数据上的**期望风险最小**。\n\n形式化表达：\n\n$$\n\\min_{f \\in H} ; \\mathbb{E}_{(x,y) \\sim D}[L(f(x), y)]\n$$\n\n这一定义揭示了监督学习的三大不变量：\n\n| 维度       | 含义         |\n| -------- | ---------- |\n| 假设空间 $H$ | 模型对世界的简化方式 |\n| 损失函数 $L$ | 对“错误”的价值判断 |\n| 泛化能力     | 对未知世界的适应能力 |\n\n所有监督学习算法，本质上只是对这三者的**不同取舍组合**。\n\n---\n\n### 2. 监督学习的基本问题类型\n\n* **回归问题**：输出为连续值（预测“多少”）\n* **分类问题**：输出为离散类别（判断“是什么”）\n\n这不是算法差异，而是**输出空间结构的差异**。\n\n---\n\n## 二、监督学习的核心认知框架\n\n### 1. 假设空间视角（核心升维）\n\n监督学习算法的根本差异，不在于公式细节，而在于：\n\n> **它们假设世界“长什么样”**\n\n| 假设空间类型 | 核心思想       | 代表模型              |\n| ------ | ---------- | ----------------- |\n| 线性假设   | 世界可被线性关系近似 | 线性回归、逻辑回归、GLM     |\n| 距离假设   | 相似样本有相似输出  | KNN               |\n| 树结构假设  | 世界可由规则切分   | 决策树、随机森林、GBDT     |\n| 最大间隔假设 | 最安全的边界最可靠  | SVM               |\n| 概率判别假设 | 输出是条件概率    | Softmax、Logistic  |\n| 时间依赖假设 | 当前依赖历史     | AR / ARMA / ARIMA |\n\n---\n\n### 2. 学习原则（跨模型不变量）\n\n所有模型都遵循以下原则：\n\n* **经验风险最小化**：拟合已知数据\n* **结构风险控制**：防止过拟合（正则化、剪枝）\n* **偏差—方差权衡**：简单 vs 表达能力\n\n---\n\n## 三、线性世界观：线性模型家族\n\n### 1. 线性回归：可解释性的极致\n\n**核心假设**：\n\n> 输出是输入特征的线性加权组合\n\n$$\nf(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b\n$$\n\n**学习目标**：最小化平方误差\n\n$$\nJ(w,b) = \\frac{1}{2m}\\sum_{i=1}^m (f(x^{(i)}) - y^{(i)})^2\n$$\n\n![w b 不同取值对应的平面](/assets/20231021210901.png)\n\n**哲学含义**：\n\n* 世界是连续、可分解、可解释的\n* 是工业界长期使用的“基准理性模型”\n\n---\n\n### 2. 多项式回归：线性形式下的非线性表达\n\n本质不是“更复杂模型”，而是：\n\n> **通过特征映射扩展假设空间**\n\n线性模型 + 非线性特征 = 表达能力提升\n\n---\n\n### 3. 广义线性模型（GLM）\n\n**核心突破**：\n\n* 放松“正态分布 + 恒等映射”的限制\n\n统一形式：\n\n$$\ny = g^{-1}(\\mathbf{w}^T\\mathbf{x} + b)\n$$\n\n* $g$：联系函数\n* $g^{-1}$：激活函数\n\nGLM 是 Logistic、Poisson 回归的理论母体。\n\n---\n\n### 4. 广义可加模型（GAM）\n\n**思想升级**：\n\n> 保留可解释性，引入受控非线性\n\n$$\ny_i = \\beta_0 + f_1(x_{i1}) + \\cdots + f_p(x_{ip})\n$$\n\n是“线性理性”向“复杂现实”的一次温和妥协。\n\n---\n\n## 四、概率判别世界观\n\n### 1. 逻辑回归：概率化的线性分类\n\n通过 Sigmoid 将线性输出映射为概率：\n\n$$\nf(\\mathbf{x}) = \\frac{1}{1+e^{-(\\mathbf{w}^T\\mathbf{x}+b)}}\n$$\n\n决策边界由 $\\mathbf{w}^T\\mathbf{x}+b=0$ 定义。\n\n![线性决策边界](/assets/20231022154106.png)\n![非线性决策边界](/assets/20231022154322.png)\n\n**损失函数来源**：最大似然估计（交叉熵）\n\n---\n\n### 2. Softmax 回归：多分类的自然推广\n\n输出的是条件概率分布：\n\n$$\nP(y=k|\\mathbf{x}) = \\frac{e^{z_k}}{\\sum_i e^{z_i}}\n$$\n\nSoftmax 揭示了：\n\n> 分类不是“判断”，而是“概率分配”。\n\n---\n\n## 五、最大间隔世界观：支持向量机\n\n### 1. SVM 的核心思想\n\n> 在所有可分边界中，选择**最安全的那一条**\n\n即：最大化分类间隔。\n\n![SVM的几何意义](/assets/202411519416.webp)\n\n* 硬间隔：理想世界\n* 软间隔：现实世界的容错机制\n\n---\n\n### 2. 核技巧：隐式特征映射\n\n$$\nk(\\mathbf{x}, \\mathbf{x}') = \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}')\n$$\n\n**本质**：\n\n* 不显式升维\n* 直接计算高维内积\n\n这是计算理性对表达能力的折中。\n\n---\n\n## 六、规则切分世界观：树模型家族\n\n### 1. 决策树的本质\n\n> 用一组 if-else 规则逼近真实函数\n\n![决策树结构示意](/assets/decision_tree_placeholder.png)\n\n**核心问题**：\n\n* 如何选择划分？\n* 何时停止？\n\n---\n\n### 2. 不纯度度量\n\n| 指标   | 本质      |\n| ---- | ------- |\n| 熵    | 不确定性    |\n| 信息增益 | 不确定性减少  |\n| 基尼系数 | 随机不一致概率 |\n\n---\n\n### 3. 决策树算法谱系\n\n```\n熵 → ID3\n熵 / 属性熵 → C4.5\n基尼 / 二叉划分 → CART\n```\n\n---\n\n### 4. 剪枝：结构风险控制\n\n* 预剪枝：生长时约束\n* 后剪枝：整体简化\n\n本质目标一致：\n\n> 用更简单的树，换取更强的泛化能力\n\n---\n\n## 七、集成思想：从单模型到系统理性\n\n### 1. 随机森林：去相关化的并行集成\n\n* 数据随机化（Bootstrap）\n* 特征随机化\n\n**哲学本质**：\n\n> 多个“有偏但不相关”的模型胜过一个完美模型\n\n---\n\n### 2. 极端随机森林\n\n进一步牺牲单棵树质量，换取整体多样性。\n\n---\n\n### 3. 梯度提升树（GBDT）\n\n> 串行纠错：每一棵树都在修正过去的错误\n\n这是**函数空间上的梯度下降**。\n\n---\n\n## 八、距离世界观：KNN\n\n**核心假设**：\n\n> 世界是连续的，相近即相似\n\n* 无训练过程\n* 计算负担在预测阶段\n\n权衡核心：\n\n* 小 $k$：低偏差，高方差\n* 大 $k$：高偏差，低方差\n\n---\n\n## 九、时间依赖世界观\n\n### 时间序列模型的特殊性\n\n* 打破 IID 假设\n* 显式建模历史依赖\n\n| 模型    | 核心思想     |\n| ----- | -------- |\n| AR    | 历史值决定现在  |\n| MA    | 历史噪声影响现在 |\n| ARMA  | 二者结合     |\n| ARIMA | 差分平稳化    |\n\n---\n\n## 十、监督学习的演进与选型方法论\n\n### 1. 模型演进路径\n\n```\n线性模型\n → 决策树\n → 随机森林\n → GBDT\n → 深度学习（预留）\n```\n\n### 2. 模型选择的四个核心约束\n\n* 数据规模\n* 特征复杂度\n* 可解释性要求\n* 工程与计算成本\n\n## 关联内容（自动生成）\n\n- [/数据技术/机器学习.md](/数据技术/机器学习.md) 机器学习是监督学习的上层概念，监督学习是机器学习的重要范式之一，两者在模型优化、泛化能力等方面有密切关系\n- [/数据技术/深度学习.md](/数据技术/深度学习.md) 深度学习是监督学习的一种高级形式，体现了监督学习在复杂模型中的应用和发展\n- [/数据技术/非监督学习.md](/数据技术/非监督学习.md) 非监督学习与监督学习是机器学习的两大范式，对比理解有助于深入掌握监督学习的特点\n- [/数据技术/特征工程.md](/数据技术/特征工程.md) 特征工程是监督学习的重要前置步骤，直接影响监督学习模型的效果和性能\n- [/数据技术/推荐系统.md](/数据技术/推荐系统.md) 推荐系统大量使用监督学习算法，如逻辑回归、GBDT等，是监督学习的重要应用领域\n- [/数学/线性代数.md](/数学/线性代数.md) 线性代数是监督学习中线性模型的数学基础，特别是矩阵与向量运算在数据建模中的应用\n- [/数学/概率论与数理统计.md](/数学/概率论与数理统计.md) 概率论为监督学习中的贝叶斯方法、最大似然估计等提供理论基础\n- [/数据技术/数据分析.md](/数据技术/数据分析.md) 数据分析与监督学习密切相关，监督学习往往作为数据分析的高级手段用于预测和分类任务\n","metadata":"tags: ['数据技术']","hasMoreCommit":false,"totalCommits":7,"commitList":[{"date":"2026-02-12T14:07:03+08:00","author":"MY","message":"doc: 整理标签","hash":"290b3e8ad18f48832ac282290238d020fc030a88"},{"date":"2026-01-13T17:04:05+08:00","author":"MY","message":"feat(doc): 完善监督学习文档内容","hash":"73544ac0bfa55cf948e93efb391a9e7db4bb2e24"},{"date":"2024-12-17T16:37:03+08:00","author":"MY","message":"📦机器学习","hash":"61b90f666ece70535bb2cb2331de3aeb843482ee"},{"date":"2024-01-22T18:46:02+08:00","author":"MY","message":"✏机器学习","hash":"94299c37debe9123be5db9a862adf191b1017c2b"},{"date":"2024-01-17T20:03:46+08:00","author":"MY","message":"✏机器学习","hash":"a64bdf732c7a380407a3f494f728a61f8632c16d"},{"date":"2024-01-16T17:09:34+08:00","author":"MY","message":"✏机器学习","hash":"b714702cc196f2cd811d78d0c59eeb6e21685768"},{"date":"2024-01-16T13:39:44+08:00","author":"MY","message":"📦机器学习","hash":"cd5c15fdc0352c79322085de552b50fdc39a8c58"}],"createTime":"2024-01-16T13:39:44+08:00"}