{"name":"非监督学习","id":"数据技术-非监督学习","content":"# 非监督学习\n\n> **从无标签数据中恢复世界结构的学习范式**\n\n---\n\n## 一、第一性原理：非监督学习到底在学什么？\n\n**非监督学习的本质不是“没有标签的监督学习”**，而是：\n\n> **在缺乏外部语义标注的前提下，假设数据本身蕴含某种内在结构，并试图恢复这种结构。**\n\n这些“结构”不是算法决定的，而是**建模假设**决定的。\n\n从稳定、跨算法的视角，可以将非监督学习统一抽象为三类**结构学习问题**：\n\n| 结构类型       | 学习对象       | 核心问题        |\n| ---------- | ---------- | ----------- |\n| 共现 / 频率结构  | 事件是否经常同时出现 | 世界中哪些因素彼此相关 |\n| 几何 / 拓扑结构  | 数据在空间中如何聚集 | 世界是如何被自然分组的 |\n| 概率 / 支持域结构 | 什么是“正常范围”  | 世界的常态边界在哪里  |\n\n对应形成三大方法族：\n\n* **关联规则挖掘** → 频率结构\n* **聚类** → 几何 / 分布结构\n* **异常检测** → 概率支持域结构\n\n> 后续所有算法，都是对这些结构假设的不同工程实现。\n\n---\n\n## 二、关联规则挖掘：学习“共现结构”\n\n### 1. 建模哲学\n\n关联规则挖掘假设：\n\n> **如果两个事件在数据中经常同时出现，那么它们之间可能存在某种潜在关系。**\n\n它不关心因果，只关心**统计共现**。\n\n---\n\n### 2. 核心度量：频率与条件关系\n\n#### 支持度（Support）\n\n> 描述某个事件组合在整体中的**出现频率**\n\n[\nSupport(X,Y)=P(X,Y)=\\frac{count(X,Y)}{N}\n]\n\n* 是**全局频率指标**\n* 决定“是否值得关注”\n\n---\n\n#### 置信度（Confidence）\n\n> 描述在 Y 已发生的条件下，X 发生的可能性\n\n[\nConfidence(X\\Leftarrow Y)=P(X|Y)=\\frac{P(X,Y)}{P(Y)}\n]\n\n* 是**条件概率**\n* 容易被高频项误导\n\n---\n\n#### 提升度（Lift）\n\n> 衡量条件关系是否“超出随机独立假设”\n\n[\nLift(X\\rightarrow Y)=\\frac{P(Y|X)}{P(Y)}\n]\n\n* Lift > 1：正相关\n* Lift = 1：独立\n* Lift < 1：负相关\n\n👉 **提升度才是真正的“相关性校正指标”**\n\n---\n\n### 3. 频繁项集挖掘的核心不变量\n\n#### Apriori 原理（反单调性）\n\n* 若项集是频繁的，其所有子集必然频繁\n* 若项集非频繁，其所有超集必然非频繁\n\n这是：\n\n> **搜索空间剪枝的数学基础**\n\n---\n\n### 4. 算法演进逻辑\n\n| 算法        | 核心思想         | 演进动因    |\n| --------- | ------------ | ------- |\n| Apriori   | 广度优先 + 候选剪枝  | 频繁扫描数据集 |\n| FP-Growth | 压缩表示 + 条件模式基 | 减少扫描次数  |\n\nFP-Growth 的本质不是“更聪明”，而是：\n\n> **用结构换时间，用内存换扫描成本**\n\n---\n\n## 三、聚类：学习“空间与分布结构”\n\n### 1. 聚类的统一建模假设\n\n所有聚类算法都隐含一个前提：\n\n> **相似的样本在某种空间或分布中应当彼此接近。**\n\n不同算法的差异在于：\n\n* “相似”如何定义？\n* “簇”被假设成什么形态？\n\n---\n\n### 2. 稳定的聚类分类维度（而非算法记忆）\n\n| 维度       | 关键问题      |\n| -------- | --------- |\n| 距离 vs 分布 | 是否显式建模概率？ |\n| 参数依赖     | 是否需要预设 K？ |\n| 簇形状假设    | 是否只支持凸簇？  |\n| 噪声鲁棒性    | 是否能识别离群点？ |\n\n---\n\n### 3. 聚类方法族\n\n#### （1）层次聚类 —— 结构优先\n\n* 通过逐步合并 / 分裂构建树状结构\n* 不需要预设簇数\n* 适合探索性分析\n\n👉 输出的是**层次关系本身**\n\n---\n\n#### （2）原型聚类（K-means）—— 几何均值假设\n\n**建模假设**：\n\n* 簇是凸的\n* 簇中心可用均值表示\n\n**目标函数**：\n[\nJ=\\frac1m\\sum_{i=1}^{m}|x^{(i)}-\\mu_{c^{(i)}}|^2\n]\n\n**算法不变量**：\n\n* 每次迭代目标函数**单调不增**\n* 若上升 → 实现或数值错误\n\n**工程代价**：\n\n* 依赖初始点\n* 易陷入局部最优\n\n---\n\n#### （3）分布聚类（EM / GMM）—— 概率生成模型\n\n**核心思想**：\n\n> 数据来自多个隐含分布的混合\n\nEM 是一个**优化框架**：\n\n* E-step：估计隐变量\n* M-step：最大化参数\n\n👉 聚类结果是**概率归属，而非硬划分**\n\n---\n\n#### （4）密度聚类（DBSCAN）—— 支持域假设\n\n* 高密度区域是簇\n* 低密度区域是分隔或噪声\n\n优点：\n\n* 无需 K\n* 可识别任意形状簇\n\n代价：\n\n* 对参数和尺度敏感\n\n---\n\n## 四、异常检测：学习“正常性的边界”\n\n### 1. 异常检测的核心哲学\n\n异常检测的关键假设是：\n\n> **正常数据有稳定结构，异常是对该结构的偏离。**\n\n因此：\n\n* 不直接学习异常\n* 而是**建模正常性**\n\n---\n\n### 2. 密度估计范式\n\n#### 判定规则\n\n[\np(x)\\begin{cases}\n<\\varepsilon & anomaly\\\n\\ge\\varepsilon & normal\n\\end{cases}\n]\n\n这是一个**支持域判定问题**。\n\n---\n\n### 3. 高斯异常检测模型\n\n假设各特征独立且服从高斯分布：\n\n[\n\\mu_j=\\frac{1}{m}\\sum x_j^{(i)},\\quad\n\\sigma_j^2=\\frac{1}{m}\\sum(x_j^{(i)}-\\mu_j)^2\n]\n\n联合概率：\n[\np(\\vec{x})=\\prod_{j=1}^np(x_j;\\mu_j,\\sigma_j^2)\n]\n\n---\n\n### 4. 阈值选择与数据现实\n\n* 阈值 ε 通过验证集选择\n* 异常样本稀少、分布多样\n* 本质上是**开放世界问题**\n\n---\n\n### 5. 异常检测 vs 监督学习（本质对比）\n\n| 维度    | 异常检测    | 监督分类  |\n| ----- | ------- | ----- |\n| 建模对象  | 正常分布    | 类边界   |\n| 对异常假设 | 未知、多样   | 稳定、可见 |\n| 适用场景  | 欺诈、设备监控 | 分类、识别 |\n\n---\n\n## 五、方法演进与稳定认知\n\n非监督学习的演进不是“算法更新”，而是：\n\n> **数据规模、维度与分布复杂度的变化**\n\n* 频率结构 → 从扫描到压缩\n* 几何结构 → 从均值到密度\n* 正常性模型 → 从参数分布到表征学习\n\n---\n\n## 六、最终总结：稳定知识视角\n\n* 非监督学习 ≠ 算法集合\n* 非监督学习 = **结构学习范式**\n* 算法只是：\n\n  > 假设 × 数据 × 计算资源 的交点\n\n## 关联内容（自动生成）\n\n- [/数据技术/机器学习.md](/数据技术/机器学习.md) 机器学习是监督学习和非监督学习的上层概念，非监督学习是机器学习的重要范式之一，两者在模型优化、泛化能力等方面有密切关系\n- [/数据技术/监督学习.md](/数据技术/监督学习.md) 监督学习与非监督学习是机器学习的两大范式，对比理解有助于深入掌握非监督学习的特点\n- [/数据技术/深度学习.md](/数据技术/深度学习.md) 深度学习在非监督学习领域有重要应用，如自编码器和生成模型\n- [/数据技术/特征工程.md](/数据技术/特征工程.md) 非监督学习中的聚类、降维等方法常用于特征工程中的特征变换和降维处理，提升特征质量\n- [/数学/线性代数.md](/数学/线性代数.md) 线性代数是机器学习的数学基础，特别是矩阵与向量运算在数据建模中的应用\n- [/数学/概率论与数理统计.md](/数学/概率论与数理统计.md) 概率论为非监督学习中的概率图模型、贝叶斯方法等提供理论基础\n","metadata":"tags: ['数据技术']","hasMoreCommit":false,"totalCommits":6,"commitList":[{"date":"2026-02-12T14:07:03+08:00","author":"MY","message":"doc: 整理标签","hash":"290b3e8ad18f48832ac282290238d020fc030a88"},{"date":"2026-01-13T21:06:54+08:00","author":"MY","message":"docs(数据技术): 更新非监督学习文档，添加结构化学习范式分析","hash":"ccf444090229bd99c1eb5d738da8fb007d28b524"},{"date":"2024-12-17T16:37:03+08:00","author":"MY","message":"📦机器学习","hash":"61b90f666ece70535bb2cb2331de3aeb843482ee"},{"date":"2024-01-17T20:03:46+08:00","author":"MY","message":"✏机器学习","hash":"a64bdf732c7a380407a3f494f728a61f8632c16d"},{"date":"2024-01-16T17:09:34+08:00","author":"MY","message":"✏机器学习","hash":"b714702cc196f2cd811d78d0c59eeb6e21685768"},{"date":"2024-01-16T13:39:44+08:00","author":"MY","message":"📦机器学习","hash":"cd5c15fdc0352c79322085de552b50fdc39a8c58"}],"createTime":"2024-01-16T13:39:44+08:00"}