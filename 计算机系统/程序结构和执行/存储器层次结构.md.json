{"name":"存储器层次结构","id":"计算机系统-程序结构和执行-存储器层次结构","content":"# 存储器层次结构\n\n## 物理介质\n\n- 高速缓冲存储器\n- 主存储器\n- 快闪存储器\n- 磁盘存储器\n- 光学存储器\n- 磁带存储器\n\n## 存储技术\n\n### 随机访问存储器\n\n- 静态RAM 其可以无限期保存两个电路稳态 主要用在高速缓存\n- 动态RAM 其可以在一段时间内保存稳态 主要用在主存\n- 非易失性存储器 \n\n## 基本存储体系\n\n1)输入设备将程序与数据写入主存； 2) CPU取指令； 3) CPU执行指令期间读数据； 4) CPU写回运算结果； 5) 输出设备输出结果；\n\n### 主存速度慢的原因\n\n- 主存增速与CPU增速不同步；\n- 指令执行期间多次访问存储器；\n\n### 主存容量不足的原因\n\n- 存在制约主存容量的技术因素\n\n  - CPU、主板等相关技术指标确定\n\n- 应用对主存的需求不断扩大\n- 价格原因\n\n### 存储体系的层次结构\n\n![2021927173034](/assets/2021927173034.png)\n\n- L1 Cache集成在CPU中，分数据Cache(D-Cache)和指令Cache(I-Cache）\n- 早期L2 Cache在主板上或与CPU集成在同一电路板上。随着工艺的提高，L2Cache被集成在CPU内核中，不分D-Cache和I-Cache\n\n存储器      | 硬件介质      | 单位成本(美元/MB) | 随机访问延时 | 说明\n-------- | --------- | ----------- | ------ | --------------------------\nL1 Cache | SRAM      | 7           | 1ns    |\nL2 Cache | SRAM      | 7           | 4ns    | 访问延时15x L1 Cache\nMemory   | DRAM      | 0.015       | 100ns  | 访问延时15X SRAM,价格1/40 SRAM\nDisk     | SSD(NAND) | 0.0004      | 150μs  | 访问延时1500X DRAM,价格1/40 DRAM\nDisk     | HDD       | 0.00004     | 10ms   | 访问延时70X SSD,价格1/10 SSD\n\n### 局部性\n\n- [缓存](/软件工程/架构/系统设计/缓存.md)\n\n#### 时间局部性\n\n- 现在被访问的信息在不久的将来还将再次被访问\n- 时间局部性的程序结构体现： 循环结构\n\n#### 空间局部性\n\n- 现在访问的信息，下一次访问的很有可能是附近的信息\n- 空间局部性的程序结构体现：顺序结构\n\n## 主存中的数据组织\n\n- 主存的一个存储单元所包含的二进制位数\n- 目前大多数计算机的主存按字节编址，存储字长也不断加大,如16位字长、32位字长和64位字长\n\n**ISA设计时要考虑的两个问题**：\n\n- 字的存放问题\n- 字的边界对齐问题\n\n### 数据存储与边界\n\n按边界对齐的数据存储：浪费一些空间\n\n![批注 2020-01-30 161009](/assets/批注%202020-01-30%20161009.png)\n\n未按边界对齐存放：虽节省了空间，但增加了访存次数\n\n![批注 2020-01-30 161136](/assets/批注%202020-01-30%20161136.png)\n\n需要在性能与容量间权衡\n\n- 双字长数据边界对齐的起始地址的最末三位为000(8字节整数倍；\n- 单字长边界对齐的起始地址的末二位为00(4字节整数倍)；\n- 半字长边界对齐的起始地址的最末一位为0(２字节整数倍)。\n\n#### 大端与小端存储\n\n- 小端存储\n\n  - 就是低位字节排放在内存的低地址端，高位字节排放在内存的高地址端\n\n- 大端存储\n\n  - 就是高位字节排放在内存的低地址端，低位字节排放在内存的高地址端\n\n无论是大端还是小端，每个系统内部是一致的，但在系统间通信时可能会发生问题！因为顺序不同，需要进行顺序转换\n\n## 存储技术\n\n## 随机访问存储器\n\n### 静态RAM(SRAM)\n\n![批注 2020-01-30 162459](/assets/批注%202020-01-30%20162459.png)\n\n**工作原理**\n\n- 读\n- 写\n- 保持\n\n#### 结构\n\n![批注 2020-01-15 092634](/assets/批注%202020-01-15%20092634.png)\n\n#### 静态存储器的不足\n\n- 晶体管过多\n- 存储密度低\n- 功耗大\n\n### 动态RAM(DRAM)\n\n![202274213610](/assets/202274213610.jpg)\n\n![批注 2020-01-30 164801](/assets/批注%202020-01-30%20164801.png)\n\nDRAM与SRAM不同的是，需要靠不断地“刷新”，才能保持数据被存储起来\n\n- 读写\n- 保持\n\n#### 刷新\n\n![集中刷新](/assets/批注%202020-01-30%20165930.png)\n\n![分散刷新](/assets/批注%202020-01-30%20170041.png)\n\n![异步刷新](/assets/批注%202020-01-30%20190013.png)\n\n#### 其它结构的DRAM存储单元\n\n- 单管\n\n![批注 2020-01-30 190355](/assets/批注%202020-01-30%20190355.png)\n\n### 传统DRAM\n\n- 内存模块\n- 增强DRAM\n- 非易失性存储器\n- 访问主存\n\n## 存储扩展\n\n- 位扩展\n\n用16K X 8 的存储芯片构建16K X 32的存储器\n\n- 字扩展\n\n用16K X 8 的存储芯片构建128k X 8的存储器\n\n- 字位扩展\n\n用16K X 8 的存储芯片构建128K X 32的存储器\n\n![批注 2020-01-30 190639](/assets/批注%202020-01-30%20190639.png)\n\n无论哪种类型的存储扩展都要完成CPU与主存间地址线、数据线、控制线的连接\n\n## 磁盘存储\n\n![磁盘构造](/assets/202192717310.jpg)\n\n磁盘驱动器本身就包含一个微控制器，这允许磁盘驱动器发出一些诸如高速缓存、坏块重映射等高级命令\n\n- 主轴（Spindle）：使整个盘面转动\n- 盘面（Platter）：一个磁盘有多个盘面\n- 磁道：盘片上划分出来的一个区域，读写过程中，磁头会沿着磁道移动，以读取或写入磁道上的数据\n- 扇区：磁道被划分为若干个扇区，每个扇区包含一个固定大小的数据块\n- 读写头：读取或写入磁盘数据的设备\n- 磁盘臂：负责控制读写头在磁盘表面上的位置\n- 柱面：是一组同心圆上对应的扇区的集合，它们处于磁盘的相同半径位置\n- 磁盘控制器：控制磁盘的运行和数据传输的主控，通常包括一个处理器和一些固件\n\n### 性能度量\n\n- 访问时间：发出一个读/写请求到收到响应数据的时间\n- 平均寻道时间：读写头移动到磁盘上任意一个磁道的平均时间\n- 旋转等待时间：读写头等待目标扇区旋转到磁头下方的时间\n- 平均旋转等待时间\n- 数据传输率：指从磁盘读取或写入数据的速率。它与磁盘的接口类型、磁盘旋转速度和数据密度有关\n- 平均故障时间：给定时间段内，磁盘发生故障的平均时间\n\n### 访问优化\n\n物理结构决定了磁盘更擅长顺序访问，为了优化随机读写的低效率，有一些手段：\n\n- 缓冲：将读取的块先存储在内存级缓冲区中，以满足未来的请求\n- 预读：利用空间局部性原理，预先读取周围的块数据\n- 调度：使用电梯算法\n- 文件组织：将会一起访问的文件组织到相邻的柱面上，现代的操作系统都已经对应用隐藏了低层的存放方式了，都由操作系统统一管理了，这意味着文件碎片会越来越多\n- 非易失性写缓冲区：在磁盘前再加一块不会断电丢失的快速缓存，先写到缓存，后续再慢慢刷到磁盘里\n- 日志磁盘：由于日志都是顺序写，所以速度可以比较快，但是读需要做特殊处理\n\n### 磁盘容量度量\n\n- 记录密度：磁盘表面上每英寸线性长度上存储的磁性转换数目\n- 磁道密度：磁盘表面上每英寸线性长度上的磁道数目\n- 面密度：每个磁头（读写头）上可用的记录密度\n\n### 连接到磁盘\n\n- 通用串行总线（USB）\n- 图形卡\n- 主机总线适配器：包括IDE、SATA、SCSI等\n\n### 访问磁盘\n\n**内存映射**：将磁盘文件映射到进程的虚拟地址空间中的技术，这样就可以像访问内存一样访问磁盘文件，从而方便了文件的读写操作\n\n### 磁盘臂调度算法\n\n读写磁盘块所需要的时间多少由以下三个因素决定：\n\n- 旋转时间：主轴转动盘面，使得磁头移动到适当的扇区上需要的时间\n- 寻道时间：制动手臂移动，使得磁头移动到适当的磁道上需要的时间\n- 数据传输时间\n\n调度算法：\n\n- 先来先服务算法\n  - 按照磁盘请求的顺序进行调度\n  - 公平简单，但是没有对寻道做任何优化，平均寻道时间较长\n- 最短寻道优先\n  - 优先调度距距离磁头最近的磁道\n  - 不够公平，如果一个请求距离当前磁头比较远，会出现饥饿现象\n![屏幕截图 2021-01-18 151803](/assets/屏幕截图%202021-01-18%20151803.png)\n- 电梯算法\n  - 总是保持一个方向运行，直到该方向没有请求为止，然后改变运行方向\n![20203219747](/assets/20203219747.png)\n\n旋转时间与寻道时间十分影响性能，所以一次只读取一个或者两个扇区效率很低。现代的磁盘控制器都拥有高速缓存，每次读取多个扇区，并将其缓存。\n\n### 错误处理\n\n对于磁盘坏块的处理，可以在控制器或者在操作系统对他们进行处理。\n\n在控制器中，处理的思想都是一样的，都是使用备用块来替代坏块。\n\n而在操作系统级别进行处理，操作系统必须获的所有坏块列表，并将其进行重映射。\n\n### 磁盘格式化\n\n低级格式化：对每个扇区设置前导码，ECC，由于前导码与ECC需要占用一定空间，所以可用磁盘容量总比宣传的小\n\n↓\n\n磁盘分区：0扇区包含主引导记录（MBR），MBR包含一些引导代码及分区表\n\n↓\n\n高级格式化：设置引导块、空闲存储管理、根目录、文件系统\n\n_系统的启动流程：BIOS运行->读入MBR并跳转->执行引导程序->找到操作系统内核载入内存执行_\n\n### 稳定存储器\n\n保持磁盘的数据一致性。\n\n- 稳定写：对每个磁盘驱动器轮流进行重复写操作\n- 稳定读：如果有稳定写总是成功，并且一段时间内数据不会自己变坏的前提下，稳定写就总是成功的\n- 崩溃恢复：当写的过程中发生崩溃，后续恢复的原则\n  - 两块都是好的且一样，什么也不做\n  - 一块有ECC错误，则好的覆盖掉有ECC错误的\n  - 两块都是好的但不一样，用第一块覆盖第二块\n\n![屏幕截图 2021-01-18 155800](/assets/屏幕截图%202021-01-18%20155800.png)\n\n### Partial Stroking\n\n磁盘的两个时间：\n\n1. 平均延时：把盘面旋转，把几何扇区对准悬臂位置的时间\n2. 平均寻道时间：盘面旋转之后，悬臂定位到扇区的的时间\n\n磁盘转速越高，平均延时就更低，如果不进行寻道，就可以剩下寻道的时间，同时越在磁盘外圈的数据访问越快，相同的角速度下，越外圈线速度越快，同样的时间扫过的扇区就越多\n\n## 固态硬盘\n\n![不同的颗粒可以存储更多的数据，但同时也更慢](/assets/20227621357.webp)\n\nSLC 的芯片，可以擦除的次数大概在 10 万次，MLC 就在 1 万次左右，而 TLC 和 QLC 就只在几千次\n\n![SSD会进行磁盘整理](/assets/202276213632.webp)\n\n### 磨损均衡\n\n某些块写入比其他块更加频繁，这些块寿命会更短\n\n![如果一个物理块被擦写的次数多了，FTL 就可以将这个物理块，挪到一个擦写次数少的物理块上](/assets/202276213917.webp)\n\n### TRIM命令\n\n为了避免磨损均衡在搬运很多已经删除了的数据，现在的操作系统和 SSD 的主控芯片，都支持 TRIM 命令。这个命令可以在文件被删除的时候，让操作系统去通知 SSD 硬盘，对应的逻辑块已经标记成已删除了\n\n### 写入放大\n\n当 SSD 硬盘的存储空间被占用得越来越多，每一次写入新数据，可能不得不去进行垃圾回收，合并一些块里面的页，然后再擦除掉一些页\n\n解决写入放大，需要在后台定时进行垃圾回收，在硬盘比较空闲的时候，就把搬运数据、擦除数据、留出空白的块的工作做完\n\n### 最大化SSD使用效率\n\nAeroSpike 这个 KV 数据库利用了 SSD的一些特性来加快读写速度：\n\n1. 尽可能去写一个较大的数据块，而不是频繁地去写很多小的数据块\n2. 读取数据的时候就可以读取小数据，不用担心擦写寿命问题\n3. 持续地进行磁盘碎片整理，一旦一个物理块里面的数据碎片超过 50%，就把这个物理块搬运压缩，然后进行数据擦除，确保磁盘始终有足够的空间可以写入，保 SSD 硬盘的写放大效应尽可能小\n\n此外：\n\n1. 对于SSD而言，没了HDD的寻址代价，随机读写和顺序读写性能类似，就地更新不会获得任何 IOPS 优\n2. SSD对于空间局部性也没有优势，预取这种额外的数据访问，只会导致 I/O 带宽被浪费\n3. SSD可以执行并行小IO充分利用内部的并行性\n\n## raid\n\n- 将数据条带化后的存放在不同磁盘上，通过多磁盘的并行操作提高磁盘系统的读写速率\n- 使用基于异或运算为基础的校验技术恢复损坏的数据，提升可靠性\n- 通过组合多个硬盘来提高存储容量\n\nraid的数据拆分有两种：\n\n- 比特级拆分\n- 块级拆分\n\n比特级的粒度较细，所以读写效率相对较低能\n\n![批注 2020-02-08 204640](/assets/批注%202020-02-08%20204640.png)\n\nRAID级别说明 | 可靠性 | 读性能 | 写性能 | 最少硬盘数量 | 硬盘利用率\n-------- | --- | --- | --- | ------ | -------------\nRAID0    | 低   | 高   | 高   | 1      | 100%\nRAID1    | 高   | 低   | 低   | 2      | 1/N\nRAID5    | 较高  | 高   | 中   | 3      | (N-1)/N\nRAID6    | 较高  | 高   | 中   | 3      | (N-2)/N\nRAID1E   | 高   | 中   | 中   | 3      | M/N\nRAID10   | 高   | 中   | 中   | 4      | M/N\nRAID50   | 高   | 高   | 较高  | 6      | (N-M)/N\nRAID60   | 高   | 高   | 较高  | 6      | (N - M * 2)/N\n\nN为RAID组成员盘的个数，M为RAID组的子组数。\n\n### RAID0\n\n- 将磁盘划分为多个条带，若读取多个条带的数据，可以实现并行IO，但对于每次读取一个条带的操作系统，性能不仅同普通磁盘，而且还有故障率更高的风险\n\n![批注 2020-02-08 204953](/assets/批注%202020-02-08%20204953.png)\n\n### RAID1\n\n- 在RAID0的基础上整个数据复制一份，写性能比单个磁盘差，但读性能很高，并且拥有容错性，一旦一个磁盘挂掉，使用副本代替即可\n\n![批注 2020-02-08 205138](/assets/批注%202020-02-08%20205138.png)\n\n### RAID2\n\n在数据盘的基础上，增加了一个磁盘来存放ECC\n\n### RAID 3/4\n\n- RAID3:在2的基础上，使用奇偶校验码。如果磁盘挂掉，由于挂掉的磁盘位置已知，所以可以推算出丢失的数据位为0还是1 2和3的性能都不比单个磁盘好\n- RAID4:重新使用了条带，使用一个驱动器专门存放前面4个条带的奇偶校验。但这种设计对小更新的性能很差，因为每次更新都需要更新校验和。\n\n![批注 2020-02-08 205227](/assets/批注%202020-02-08%20205227.png)\n\n### RAID5\n\n- 在4的基础上将校验和平均分布到各个磁盘，但如果某个磁盘挂掉，那就不好恢复了\n\n![批注 2020-02-08 205428](/assets/批注%202020-02-08%20205428.png)\n\n### RAID6\n\n在5的基础上使用额外的校验块。为每 4 位数据存储2 位的冗余信息，这样系统可容忍两张磁盘 发生故障\n\n### RAID10\n\n- 结合RAID1和RAID0，先镜像，再条带化\n\n![批注 2020-02-08 205601](/assets/批注%202020-02-08%20205601.png)\n\n### RAID01\n\n- 结合RAID0和RAID1，先条带化, 再镜像\n\n![批注 2020-02-08 205654](/assets/批注%202020-02-08%20205654.png)\n\n只能容忍一个磁盘故障，如0号盘损坏，左边RAID0失效，只能使用右边的RAID0，不能再有盘损坏，故冗余度为1\n\n### 实现方式\n\n- 软件RAID\n  - 功能都依赖于主机CPU完成,没有第三方的控制处理器和I/O芯片\n- 硬件RAID\n  - 专门RAID控制处理器和I/O处理芯片处理RAID任务，不占用主机CPU资源\n\n在空闲时期，控制器会对每张磁盘的每一个扇区进行读取，如果发现某个扇区无法读取，会从其余磁盘中进行恢复\n\n一些硬件RAID实现允许热交换：在不切断电濒的情况下梅出错磁盘用新的磁盘替换\n\n### 比较\n\n![批注 2020-02-08 205826](/assets/批注%202020-02-08%20205826.png)\n\n### 选择考量\n\n- 所需的额外存储代价\n- 在IO方面的性能问题\n- 磁盘故障时的性能：例如，在RAID 5中，当一个硬盘故障时，RAID控制器需要对数据进行重建，这可能导致性能下降\n- 数据重建过程：当一个硬盘故障时，RAID控制器需要将数据从其他硬盘中重建。数据重建可能需要很长时间\n\n## 存储技术的趋势\n\n- 价格和性能折中\n- 不同存储技术的价格与属性以不同的速率变化\n\n## 多体交叉存储器\n\n其基本思想是在不提高存储器速率、不扩展数据通路位数的前提下，通过存储芯片的交叉组织，提高CPU单位时间内访问的数据量，从而缓解快速的CPU与慢速的主存之间的速度差异。\n\n### 高位多体交叉存储器\n\n![批注 2020-01-16 113946](/assets/批注%202020-01-16%20113946.png)\n\n### 低位多体交叉存储器\n\n![批注 2020-01-16 113919](/assets/批注%202020-01-16%20113919.png)\n\n![批注 2020-02-08 161132](/assets/批注%202020-02-08%20161132.png)\n\n## 高速缓存存储器\n\n现代的多核处理器大都采用混合式的方式将缓存集成到芯片上，一般情况下，L3 是所有处理器核共享的，L1 和 L2 是每个处理器核特有的\n\n内存中的指令、数据，会被加载到 L1-L3 Cache 中，而不是直接由 CPU 访问内存去拿\n\nCPU 从内存中读取数据到 CPU Cache 的过程中，是一小块 Cache Line 来读取数据的，而不是按照单个数组元素来读取数据的，大部分 Cache Line的大小通常是64个字节，[Disruptor](/编程语言/JAVA/JAVA并发编程/Disruptor.md#Disruptor) 利用了这点\n\n### cache的工作过程\n\n```mermaid\nsequenceDiagram\n  alt 命中\n    CPU ->> Cache: 读取数据\n    Cache ->> CPU: 返回数据\n  end\n  alt 缺失\n    CPU ->> Cache: 读取数据\n    Cache ->> 内存: 读取数据\n    内存 ->> Cache: 返回数据\n    Cache ->> CPU: 返回数据\n  end\n```\n\n### 缓存写策略\n\n修改什么时候传播到主存：\n\n- 写回（Write Back）：对缓存的修改不会立刻传播到主存，只有当缓存块被替换时，这些被修改的缓存块，才会写回并覆盖内存中过时的数据\n- 写直达（Write Through）：缓存中任何一个字节的修改，都会立刻传播到内存\n\n当某个 CPU 的缓存发生变化，其他 CPU 缓存所保有该数据副本的更新策略：\n\n- 写更新（Write Update）：每次缓存写入新的值，该 CPU 都必须发起一次总线请求，通知其他 CPU 将它们的缓存值更新为刚写入的值\n- 写无效（Write Invalidate）：一个 CPU 修改缓存时，将其他 CPU 中的缓存全部设置为无效，写无效只需要发起一次总线事件即可\n\n当前要写入的数据不在缓存中时，根据是否要先将数据加载到缓存：\n\n- 写分配（Write Allocate）：写入数据前将数据读入缓存\n- 写不分配（Not Write Allocate）\n\n### cache地址映射机制\n\n对于一个地址，可以通过地址得到其应该在哪个组，确定在哪个组后，再将内存地址与组中的每一路缓存块 tag 进行匹配，如果相等，就说明该内存块已经载入到缓存中；如果没有匹配的 tag，就说明缓存缺失，需要将内存块放到该组的一个空闲缓存块上；如果所有路的缓存块都正在被使用，那么需要选择一个缓存块，将其移出缓存，把新的内存块载入\n\n![批注 2020-02-08 162555](/assets/批注%202020-02-08%20162555.png)\n\n![内存地址到 Cache Line](/assets/202275204524.webp)\n\n### cache的结构\n\n![缓存块的组织形式](/assets/202435142042.webp)\n\n- Cache被分成若干行，每行的大小与主存块相同\n- Cache每行包含四部分，是Cache要保存的信息。Tag从CPU访问主存的地址中剥离得到、Data是与主存交换的数据块、Valid表示Cache中的数据是否有效、 Dirty表示主存中的数据是最新\n\n![批注 2020-02-08 162726](/assets/批注%202020-02-08%20162726.png)\n\n有效位V|脏位M|是否有tag匹配|缓存操作|说明|状态转换\n-|-|-|-|-|-\n0|||读/写|缓存缺失，将内存数据载入缓存|tag设置成地址高21位，有效位V置1\n1|0|是|读|缓存命中|状态不变\n1||否|读/写|同组缓存块已满，选择一个缓存块替换|被替换的缓存块有效位置位V置0，回到第一行状态\n1|0|是|写|缓存命中|脏位M置1\n1|1|是|读|缓存命中，但缓存和内存数据不一致|缓存状态保持不变\n1|1|是|写|缓存命中，继续写|缓存状态保持不变\n\n### 相联存储器\n\n- 如何快速地查找\n  - 如何快速地判断数据是否存在\n\n### 缓存缺失\n\n- 强制缺失：第一次将数据块读入到缓存所产生的缺失，也被称为冷缺失（cold miss），因为当发生缓存缺失时，缓存是空的（冷的）\n- 冲突缺失：由于缓存的相连度有限导致的缺失，即不同的内存被映射到同一块缓存中\n- 容量缺失：由于缓存大小有限导致的缺失\n\n### Cache地址映射与变换方法\n\n- 主存数据如何迁至Cache才能实现快速查找\n\n#### 全相联映射\n\n缓存只有一个组，所有的内存块都放在这一个组的不同路上\n\n![批注 2020-02-08 164919](/assets/批注%202020-02-08%20164919.png)\n\n- 主存分块，Cache行 （Line），两者大小相同\n- 设每块4个字，主存大小为1024个字，则第61个字的主存地址为：\n  - 00001111 01 （块号 块内地址）\n- 主存分块后地址就从一维变成二维\n- 映射算法：主存的数据块可映射到Cache任意行，同时将该数据块地址对应行的标记存储体中保存\n\n**特点**\n\n- Cache利用率高\n- 块冲突率低\n- 淘汰算法复杂\n\n所以应用在小容量cache\n\n#### 直接映射\n\n缓存只有一个路，一个内存块只能放置在特定的组上\n\n![批注 2020-02-08 165705](/assets/批注%202020-02-08%20165705.png)\n\n- 主存分块，Cache行 （Line），两者大小相同\n- 主存分块后还将以Cache行数为标准进行分区\n- 设每块4个字，主存大小为1024个字，Cache分为4行，第61个字的主存地址为\n  - 000011 11 01 （区号，区内块号，块内地址）\n  - 主存地址从一维变成三维\n- 映射算法：Cache共n行，主存第j块号映射到Cache 的行号为 i=j mod n\n  - 即主存的数据块映射到Cache特定行\n\n**特点**\n\n- Cache利用率低\n- 块冲突率高\n- 淘汰算法简单\n\n应用在大容量cache\n\n#### 组相联映射\n\n缓存同时有多个组和多个路\n\n![批注 2020-02-08 185829](/assets/批注%202020-02-08%20185829.png)\n\n- 主存分块，Cache行 （Line），两者大小相同；\n- Cache分组（每组中包k行），本例假定K=4\n- 主存分块后还将以Cache组数为标准进行分组；\n- 设每块4个字，主存大小为1024个字，Cache分为4行，第61个字的主存地址为：\n  - 0000111 1 01 （组号，组内块号，块内地址）\n  - 主存地址从一维变成三维；\n- 映射算法：\n  - Cache共n组，主存第j块号映射到Cache 的组号为：i=j mod n\n  - 即主存的数据块映射到Cache特定组的任意行\n\n### 淘汰策略\n\n程序运行一段时间后，Cache存储空间被占满，当再有新数据要调入时，就需要通过某种机制决定替换的数据\n\n#### 先进先出法FIFO\n\n![批注 2020-02-08 191721](/assets/批注%202020-02-08%20191721.png)\n\n#### 最不经常使用法LFU\n\n![批注 2020-02-08 191910](/assets/批注%202020-02-08%20191910.png)\n\n#### 近期最少使用法LRU\n\n![批注 2020-02-08 192722](/assets/批注%202020-02-08%20192722.png)\n\n#### 替换算法的抖动\n\n- 刚刚淘汰的块在下一时刻又被访问...\n\n### 伪共享false sharing\n\n当两个线程同时各自修改两个相邻的变量，由于缓存是按缓存块来组织的，当一个线程对一个缓存块执行写操作时，必须使其他线程含有对应数据的缓存块无效。这样两个线程都会同时使对方的缓存块无效，导致性能下降\n\n经常会看到为了解决伪共享而进行的数据填充\n\n### VI协议\n\n- PrRd: 处理器请求从缓存块中读出\n- PrWr: 处理器请求向缓存块写入\n- BusRd: 总线侦听到一个来自另一个处理器的读出缓存请求\n- BusWr: 总线侦听到来自另一个处理器写入缓存的请求\n- V：缓存块有效\n- I：缓存块无效\n\n当前 CPU 发起的操作：\n\n```mermaid\nstateDiagram-v2\n  V --> V: PrRd/PrWr/BusWr\n  I --> I: PrWr/BusWr\n  I --> V: PrRd/BusRd\n```\n\n总线发起的请求：\n\n```mermaid\nstateDiagram-v2\n  V --> V: 除BusRd\n  I --> I: 除BusRd与BusWr\n  V --> I: 除BusWr\n```\n\n### MESI\n\n- 要解决缓存一致性问题，首先要解决的是多个 CPU 核心之间的数据传播问题\n\n是一种写失效协议：只有一个 CPU 核心负责写入数据，在这个 CPU 核心写入 Cache 之后，它会去广播一个“失效”请求告诉所有其他的 CPU 核心。其他的 CPU 核心，只是去判断自己是否也有一个“失效”版本的 Cache Block，然后把这个也标记成失效\n\n相对应的就是写广播协议：一个写入请求广播到所有的 CPU 核心，同时更新各个核心里的 Cache，写广播还需要把对应的数据传输给其他 CPU 核心\n\n- M：代表已修改（Modified）\n- E：代表独占（Exclusive），缓存块是干净有效且唯一的\n- S：代表共享（Shared），缓存块干净且被多个CPU共享\n- I：代表已失效（Invalidated）\n\n```mermaid\nstateDiagram-v2\n  M --> M: 本地读/写\n  M --> S: 总线读/发出写回信号\n  M --> I: 总线写/发出写回信号\n\n  E --> M: 本地写\n  E --> E: 本地读\n  E --> S: 总线读\n  E --> I: 总线写/发出写回信号\n\n  S --> S: 本地写/发出总线写信号\n  S --> S: 本地读\n  S --> I: 总线写/发出写回信号\n\n  I --> S: 本地读/发出总线读信号\n  I --> E: 本地读/发出总线读信号\n  I --> M: 本地写/发出总线写信号\n```\n\n### 内存屏障\n\n严格遵守 MESI 协议会导致某个核对缓存的占用比较长，从而影响性能。为此通过放宽 MESI 限制，引入 store buffer、invalid queue 的方式，提升了写缓存核间同步的速度\n\nstore buffer 是硬件实现的缓冲区，它的读写速度比缓存的速度更快，所有面向缓存的写操作都会先经过 store buffer，即先收集一些写操作，再批量写到缓存中，但它并不能保证变量写入缓存和主存的顺序\n\n当一个 CPU 向同伴发出 Invalid 消息的时候，它的同伴要先把自己的缓存置为 Invalid，然后再发出 acknowledgement。这个过程是比较慢的，所以引入了 invalid queue ，收到 Invalid 消息的 CPU，立刻回传确认消息，再把这个失效的消息放到一个队列中，等到空闲的时候再去处理失效消息，将缓存设置为 invalid\n\n这两个优化都可能导致变量没有写到缓存前，被其他核给读到过期值\n\n所以引入了内存屏障，屏障的作用是前边的读写操作未完成的情况下，后面的读写操作不能发生\n\n```c\n// CPU0\nvoid foo() {\n    a = 1;\n    smp_wmb(); // 写屏障\n    b = 1;\n}\n\n// CPU1\nvoid bar() {\n    while (b == 0) continue;\n    smp_rmb(); // 读屏障\n    assert(a == 1);\n}\n```\n\n除了使用读写对内存屏障进行分类外（alpha 结构），另外一种叫做单向屏障的不是以读写来区分的，而是像单行道一样，只允许单向通行：\n\n- LoadLoad屏障:对于这样的语句`Load1; LoadLoad; Load2`,在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕\n- StoreStore屏障:对于这样的语句`Store1; StoreStore; Store2`,在Store2及后续写入操作执行前，保证Store1的写 入操作对其它处理器可见。\n- LoadStore屏障:对于这样的语句`Load1; LoadStore; Store2`,在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。\n- StoreLoad屏障:对于这样的语句`Store1; StoreLoad; Load2`,在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。\n\n单向内存屏障有两个重要的语义：\n\n- release 语义：如果采用了带有 release 语义的写内存指令，那么这个屏障之前的所有读写都不能发生在这次写操作之后，但它并不能保证屏障之后的读写操作不会前移\n- acquire 语义：这个屏障之后的所有读写都不能发生在 barrier 之前，但它不管这个屏障之前的读写操作\n\n### TSO 模型\n\n处理器对于 store 操作（写操作）的行为有如下规定：\n\n1. Store Buffering（写缓冲）：每个处理器都拥有自己的写缓冲区，store 操作首先被存放在这个缓冲区中，而不是立即写入主存。\n2. Write Combining（写合并）：在写缓冲中，如果发现多个 store 操作针对同一个内存地址，那么这些操作可能会被合并成一个较大的写操作。\n3. Store Atomicity（写原子性）：store 操作对于其它处理器来说是原子的，即要么全部执行，要么全部不执行。但是，不同的 store 操作之间的顺序可能会被打乱。\n4. Store Ordering（写顺序）：每个处理器的 store 操作按照程序中的顺序执行，并且对于其它处理器来说，每个处理器所执行的 store 操作都是有序的\n\n写原子性跟写顺序性就解决了上述内存模型中的一致性问题\n\n## NUMA\n\n内存在物理上被分为了多个节点 node，CPU 可以访问所有节点，但是为了提升访问效率，CPU 可以有选择地优先访问离自己近的内存节点\n\n```mermaid\nstateDiagram-v2\n  state node1 {\n    cpu1 --> 内存1\n  }\n  cpu1 --> cpu2: cpu通信\n  cpu2 --> cpu1: cpu通信\n  cpu1 --> 内存2: 远程访问\n  cpu2 --> 内存1: 远程访问\n  state node2 {\n    cpu2 --> 内存2\n  }\n```\n\n内存策略|描述\n-|-\nMPOL_BIND|只在特定节点分配，如果空间不足则进行swap\nMPOL_INTERLEAVE|本地和远程节点均可分配\nMPOL_PREFERRED|指定节点分配，当内存不足时，优先选择离指定节点近的节点分配\nMPOL_LOCAL|优先在本地节点分配，当内存不足时，在其他节点分配\n","metadata":"tags: ['计算机系统']\nlevel: 1","hasMoreCommit":true,"totalCommits":28,"commitList":[{"date":"2024-12-03T19:50:41+08:00","author":"MY","message":"📦计算机系统","hash":"738fc2c827d503438ea6512a8d90b6435d5ca50b"},{"date":"2024-12-02T19:28:47+08:00","author":"MY","message":"📦计算机系统","hash":"120847197a4799fd4df9146d1579b956be153cae"},{"date":"2024-11-14T19:15:00+08:00","author":"MY","message":"📦计算机系统","hash":"5c5e65cc7dae2c91971a72f7a73dc3877be1a59c"},{"date":"2024-03-06T19:37:23+08:00","author":"MY","message":"✏内存","hash":"bdca4f9b5eda90cddd3fca3de7789732339e98db"},{"date":"2024-03-05T20:06:31+08:00","author":"MY","message":"✏内存","hash":"8f9e10b4889ef382f9556d7898c1a175490291df"},{"date":"2023-04-02T16:37:21+08:00","author":"MY","message":"📦存储器","hash":"199412f68c7ad380d7ddbec9793866f64ece4ca4"},{"date":"2023-03-09T09:41:00Z","author":"My","message":"🛠替换在线图片","hash":"0c8b08bc22fbe482ba02da2f1fcad211441d3c23"},{"date":"2022-07-05T21:20:38+08:00","author":"MY","message":"✏️更新 存储器层次结构","hash":"5ef7020f0bef151a54c5ef31450ff9b94be176bf"},{"date":"2022-07-04T21:43:46+08:00","author":"MY","message":"✏️更新 存储器层次结构","hash":"e60833a133cbcdb0d4de36cdc70fa2bfbc525594"},{"date":"2022-05-02T15:13:30+08:00","author":"MY","message":"✏️更新 存储器层次结构","hash":"8327b7b9f0f9cd1659bd30cea2407979f77f5f2d"}],"createTime":"2019-07-11T21:41:49+08:00"}