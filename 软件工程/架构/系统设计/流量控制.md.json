{"name":"流量控制","id":"软件工程-架构-系统设计-流量控制","content":"# 流量控制\n\n>当系统资源不够，不足以应对大量请求，即系统资源与访问量出现矛盾的时候，我们为了保证有限的资源能够正常服务，因此对系统按照预设的规则进行流量限制或功能限制的一种方法\n\n限流原因\n\n- 流量大\n- 保护服务\n\n限流形式：\n\n- 技术层面限流 限制并发 限制速率等\n    - 网关层限流\n    - 本地限流 分布式限流\n- 业务层面限流 类似于秒杀下场景的商品总量 抢完就完\n\n分层限流：制定每一层的限流策略时，都应该抱着不信任上层限流的思维，这样即便某一层限流机制发生问题，也不至于引发全局问题，最终形成的限流体系才是最健壮、最可靠的\n\n对于限流 我们需要考虑如下问题：\n\n- 限流指标\n- 限流手段\n- 超额流量处理\n\n**柔性限流**：不快速失败 进行排队处理\n\n**前端柔性**：随机延迟一段时间发起请求或在高峰期随机失效用户的部分频繁请求\n\n## 水位确定\n\n通过全链路压测确定系统承载的极限，水位确定的过高没有保护意义，定的过低又会导致资源浪费\n\n- 安全水位\n- 极限水位\n\n## 流量统计指标\n\n- 每秒事务数（Transactions per Second，TPS）：“事务”可以理解为一个逻辑上具备原子性的业务操作\n- 每秒请求数（Hits per Second，HPS）：指每秒从客户端发向服务端的请求数\n- 每秒查询数（Queries per Second，QPS）：一个请求的响应往往要由后台多个服务节点共同协作来完成 所以看似客户端发送了一条请求 但背后很可能涉及到多个查询\n\n当然具体业务相关的系统也有其他统计指标：如IO密集型的使用最高带宽 游戏服务采用在线用户数\n\n## 计算扩展\n\n- 上游业务扩容了 下游业务也要跟着扩展\n\n## 流量切换\n\n- DNS层切换\n- 虚拟IP\n- 应用层 Nginx 网关切换\n\n## 限流设计模式\n\n### 计数器算法\n\n系统维护一个计数器，来一个请求就加1，请求处理完成就减1，当计数器大于指定的阈值，就拒绝新的请求。\n\n基于这个简单的方法，可以再延伸出一些高级功能，比如阈值可以不是固定值，是动态调整的。另外，还可以有多组计数器分别管理不同的服务，以保证互不影响等。\n\n线程池大小，数据库连接池大小、nginx连接数等都属于计数器算法。\n\n全局或某段时间范围达到阈值则限流\n\n#### 时间窗计数\n\n- 固定时间窗 统计固定时间周期内的请求量或者资源消耗量\n\n![固定时间窗](/assets/屏幕截图%202022-05-09%20170123.png)\n\n这种问题是当时间处于两个时间窗的临界点时，会超过限额\n\n- 滑动窗口 滑动窗口原理是在每次有访问进来时，先判断前 N 个单位时间内的总访问量是否超过了设置的阈值，并对当前时间片上的请求数 +1\n\n![滑动窗口](/assets/202001271051.jpg)\n\n### [队列](/软件工程/性能工程.md#排队论)\n\n就是基于FIFO队列，所有请求都进入队列，后端程序从队列中取出待处理的请求依次处理。\n\n基于队列的方法，也可以延伸出更多的玩法来，比如可以设置多个队列以配置不同的优先级\n\n### 桶算法\n\n#### 漏桶算法\n\n漏桶(Leaky Bucket)算法思路很简单,水(请求)先进入到漏桶里,漏桶以一定的速度出水(接口有响应速率),当水流入速度过大会直接溢出(访问频率超过接口响应速率),然后就拒绝请求,可以看出漏桶算法能强行限制数据的传输速率\n\n![202001271537](/assets/202001271537.png)\n\n本质是总量控制，桶大小是设计关键\n\n漏桶算法主要适用于瞬时高并发流量的场景，在短短几分钟内涌入大量请求时，为了更好的业务效果和用户体验，即使处理慢一些，也要做到尽量不丢弃用户请求\n\n#### 令牌桶算法\n\n设置一个令牌桶，另外有一个脚本以持续恒定的速度往令牌桶里面放令牌，后端处理程序每处理一个请求就必须从桶里拿出一个令牌，如果令牌拿完了，那就不能处理请求了。我们可以控制脚本放令牌的速度来达到控制后端处理的速度，以实现动态流控\n\n![2020789425](/assets/20207894250)\n\n本质是速率控制，令牌产生的速率是设计关键，原本是用于网络设备控制传输速度的，而且它控制的目的是保证一段时间内的平均传输速度\n\n使用guava实现\n\n```java\n@RestController\npublic class Controller {\n\n    /**\n     * 一个每秒创建一个token的桶\n     */\n    RateLimiter limiter = RateLimiter.create(1);\n\n    @RequestMapping(\"index\")\n    public String index(){\n        // 500ms内无法获取令牌，返回错误，否则成功\n        if (limiter.tryAcquire(500, TimeUnit.MILLISECONDS)){\n            return \"success\";\n        }else {\n            return \"error\";\n        }\n\n    }\n}\n```\n\n#### 令牌桶与漏桶\n\n主要区别在于“漏桶算法”能够强行限制数据的传输速率，**而“令牌桶算法”在能够限制数据的平均传输速率外，还允许某种程度的突发传输**。在“令牌桶算法”中，只要令牌桶中存在令牌，那么就允许突发地传输数据直到达到用户配置的门限，因此它适合于具有突发特性的流量。\n\n#### 动态限流\n\n设计的典范是 TCP 协议的拥塞控制的算法。TCP 使用 RTT - Round Trip Time 来探测网络的延时和性能，从而设定相应的“滑动窗口”的大小，以让发送的速率和网络的性能相匹配\n\n记录下每次调用后端请求的响应时间，然后在一个时间区间内（比如，过去 10 秒）的请求计算一个响应时间的 P90 或 P99 值，如果这个 P90 或 P99 超过设定的阈值（响应太慢了），那么就可以以动态的方式限流，比如每次减去当前一半的QPS，再超就再剪\n\n## 分布式限流\n\n- 集中式存储统计信息 将原本在每个服务节点自己内存当中的统计数据给开放出来，让全局的限流服务可以访问到 这种方式每次服务调用额外增加了一次网络开销\n- 使用货币令牌 一个令牌会携带着一定额度贯穿整条链路 每次服务调用这个额度都会被减去点 当额度为0时 要不直接失败 要不就再向中心申请一定的额度 继续请求\n\n### 单机限流\n\n将集群总配额除以集群总的节点数，得到每个节点上可用的配额。在各个节点下发配额数据，然后在单机维度使用限流算法，实现单机维度的限流。可以实时监控每台节点的限流情况，动态修改每台节点的配额。通过判断，给流量较高的节点分配较多的配额，给流量较少的节点分配较少的配额，从而在流量倾斜的时候，也能够做到较为精准的限流\n\n### 全局限流\n\n选择一个集中式的限流服务器。该服务器用来记录限流配额。在生产和消费时，向该限流服务器记录配额信息，获取限流状态，判断是否进行限流。同时根据单机限流的方案，在本地缓存一份均分的配额数据，当限流服务器异常时，直接使用本地缓存的配额数据进行计算限流。同时提供开关，在某些情况下可以关闭限流\n\n## 限流阈值\n\n1. 观察服务性能数据： 通过观测业务高峰期的 QPS（每秒请求数）来确定集群或单机的限流阈值。例如，如果整个集群在业务高峰期的 QPS 没有超过 1000，可以将阈值设定为 1200，留出一定余量。这种方法依赖于现有的监控和性能数据。\n\n2. 压测： 通过压力测试来模拟不同流量情况，观察系统在不同负载下的表现，从而确定合适的限流阈值。压测可以帮助预测系统的负载承受能力，确保限流设置既不会过低浪费资源，也不会过高导致系统崩溃。\n\n3. 借鉴： 借鉴类似系统的限流设置或经验，尤其是行业标准或先前实施过的经验，来设定限流阈值。这种方法可以参考业界常用的实践或竞争对手的方案。\n\n4. 手动计算： 通过分析系统资源（如内存、CPU、网络带宽等）的容量，手动计算出系统能够承载的请求量。然后再结合业务需求，得出合适的限流阈值。这种方法适用于没有现成数据的情况下，更多依赖系统架构和资源评估。\n","metadata":"name: test\n# 测试\ntags: ['微服务', '服务治理']","hasMoreCommit":true,"totalCommits":11,"commitList":[{"date":"2024-12-13T17:26:53+08:00","author":"MY","message":"✏流控","hash":"ddde74525a5e1796cd5e8b5c95c61a609c927d94"},{"date":"2024-11-21T19:38:10+08:00","author":"MY","message":"📦流控 & 缓存","hash":"ec18717ffca6c3c8867ce0cf6bbbeff241c19ff8"},{"date":"2023-09-18T15:54:54+08:00","author":"MY","message":"✏流量控制","hash":"62c6662566295a5342aa45061bba8c7a807f449b"},{"date":"2023-01-16T16:58:46+08:00","author":"cjiping","message":"✏️限流","hash":"93acb4cc071ea230f180076cedd00d5c713e9d50"},{"date":"2022-07-15T17:06:40+08:00","author":"cjiping","message":"✏️更新 流量控制","hash":"4f0d5e724a36dfcf422a165896c9ec98010a3e98"},{"date":"2022-05-09T17:46:13+08:00","author":"cjiping","message":"✏️更新 可用性","hash":"f68313ea1ba47cd7a335d34d32466b17c04f15e9"},{"date":"2022-01-16T16:18:31+08:00","author":"MY","message":"✏️更新 分布式相关","hash":"0c267fb25c7fd1f2a3536cab5c979b0c3bf90b6d"},{"date":"2021-11-09T14:51:50+08:00","author":"cjiping","message":"✏流量控制添加标签","hash":"95ca11c44eada8b149788bed3affcc3661425921"},{"date":"2021-10-17T23:19:31+08:00","author":"My","message":"✏️更新 架构设计","hash":"b71ac55d132dbce81725cc8abc041718148d79dc"},{"date":"2021-09-22T22:34:25+08:00","author":"My","message":"✏️更新 高并发 可用性","hash":"e830ef3632dd33a03d378b77169f44a843e3440c"}],"createTime":"2020-11-20T15:17:06+08:00"}