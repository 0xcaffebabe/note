# 系统设计

## 性能

### 性能指标

- 响应时间
  - 某个请求从发出到接收到响应消耗的时间
- 吞吐量
  - 系统在单位时间内可以处理的请求数量，通常使用每秒的请求数来衡量
- 并发用户数
  - 系统能同时处理的并发用户请求数量

### 性能优化

- 集群
  - 将多台服务器组成集群，使用负载均衡将请求转发到集群中
- 缓存
  - 缓存对于性能的提升体现在响应时间上
- 异步
  - 将消息发送到消息队列之后立即返回，之后这个操作会被异步处理

## 伸缩性

不断向集群中添加服务器来缓解不断上升的用户并发访问压力和不断增长的数据存储需求

如果系统存在性能问题，那么单个用户的请求总是很慢的。
如果系统存在伸缩性问题，那么单个用户的请求可能会很快，但是在并发数很高的情况下系统会很慢

只要集群中的服务器是无状态的，那么往集群中添加服务器后进行负载均衡是很容易的

## 扩展性

添加新功能时对现有系统的其它应用无影响

- 使用消息队列对上下游应用解耦
- 使用分布式服务将业务与服务分离，服务都是一些可复用的服务，添加新功能时，只要调用已有的服务即可

## 可用性

### 冗余

保证高可用的主要手段是使用冗余

对于应用服务器来说，保证是无状态的，就可以实现冗余
而对于存储服务器，需要通过主从复制来实现冗余

### 监控

### 服务降级

## 安全性

## 高并发系统设计

单个数据库每秒两三千并发就基本扛不住了

![批注 2020-03-20 185429](/assets/批注%202020-03-20%20185429.png)

### 系统拆分

将单体系统拆分成多个系统或者多个服务，每个子系统使用自己的数据库，提高并发度

大部分的系统都是需要经过多轮拆分的，第一轮拆分将系统粒度划的小一点，可能随着业务的发展，单个系统会变得更复杂，所以需要进一步的拆分

### 缓存

大部分高并发的场景，都是写多读少，使用缓存，可以有效抗住高并发

### MQ

既然数据库每秒能撑住的请求是有限的，那么就可以使用MQ，大量的请求灌入MQ，利用MQ的削峰，让下游系统慢慢消费

### 分库分表

让每个表的数据少一点，提供SQL的执行速度

### 读写分离

大部分对数据库的请求都是读多写少，所以读写分离，分配多一些机器给读请求，能有效提高性能

### es

elasticsearch天生地支持分布式，可以通过增加机器来提高性能，并且es天生适合做搜索

## 负载均衡

- D-NAT模式

![批注 2020-06-04 155213](/assets/批注%202020-06-04%20155213.png)

- DR模式

![批注 2020-06-04 155344](/assets/批注%202020-06-04%20155344.png)

将RS的VIP配置在内核中

- TUN模式

![批注 2020-06-05 133425](/assets/批注%202020-06-05%20133425.png)

### LVS


调度算法:
- 静态
  - 轮询
  - 加权轮询
  - ...
- 动态
  - 最少连接
  - 加权最少连接
  - ...

流程：

![批注 2020-06-05 161431](/assets/批注%202020-06-05%20161431.png)

```
node01:
	ifconfig  eth0:8 192.168.150.100/24
node02~node03:
	1)修改内核：
		echo 1  >  /proc/sys/net/ipv4/conf/eth0/arp_ignore 
		echo 1  >  /proc/sys/net/ipv4/conf/all/arp_ignore 
		echo 2 > /proc/sys/net/ipv4/conf/eth0/arp_announce 
		echo 2 > /proc/sys/net/ipv4/conf/all/arp_announce 
	2）设置隐藏的vip：
		ifconfig  lo:3  192.168.150.100  netmask 255.255.255.255
		
RS中的服务：
node02~node03:
	yum install httpd -y
	service httpd start
	vi   /var/www/html/index.html
		from 192.168.150.1x

LVS服务配置
node01:
		yum install ipvsadm 
	ipvsadm -A  -t  192.168.150.100:80  -s rr
	ipvsadm -a  -t 192.168.150.100:80  -r  192.168.150.12 -g -w 1
	ipvsadm -a  -t 192.168.150.100:80  -r  192.168.150.13 -g -w 1
	ipvsadm -ln

验证：
	浏览器访问  192.168.150.100   看到负载  疯狂F5
	node01：
		netstat -natp   结论看不到socket连接
	node02~node03:
		netstat -natp   结论看到很多的socket连接
	node01:
		ipvsadm -lnc    查看偷窥记录本
		TCP 00:57  FIN_WAIT    192.168.150.1:51587 192.168.150.100:80 192.168.150.12:80
		FIN_WAIT： 连接过，偷窥了所有的包
		SYN_RECV： 基本上lvs都记录了，证明lvs没事，一定是后边网络层出问题
```

问题：
- LVS可能会发生单点故障
  - 主备
- RS挂的话，部分请求会失败

### keepalived

作为一个通用工具，解决高可用问题

配置

```
vrrp_instance VI_1 {
        state MASTER // 备服务器BACKUP
        interface eth0
        virtual_router_id 51
        priority 100
        advert_int 1
        authentication {
                auth_type PASS
                auth_pass 1111
        }
        virtual_ipaddress {
                172.17.0.100/16 dev eth0 label  eth0:3
        }
}
virtual_server 172.17.0.100 80 {
        delay_loop 6
        lb_algo rr
        lb_kind DR
        nat_mask 255.255.255.0
        persistence_timeout 0
        protocol TCP
        real_server 172.17.0.4 80 {
                weight 1
                HTTP_GET {
                        url {
                                path /
                                status_code 200
                        }
                        connect_timeout 3
                        nb_get_retry 3
												delay_before_retry 3
                }
        }
        real_server 172.17.0.6 80 {
                weight 1
                HTTP_GET {
                        url {
                                path /
                                status_code 200
                        }
                        connect_timeout 3
                        nb_get_retry 3
                        delay_before_retry 3
                }
        }
}
```